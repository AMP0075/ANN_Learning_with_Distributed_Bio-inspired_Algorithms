{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a71460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2364f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn import preprocessing\n",
    "from scipy.special import expit\n",
    "\n",
    "from numpy.random import default_rng\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import time\n",
    "\n",
    "from gaAnn_V_I import *\n",
    "\n",
    "\n",
    "\n",
    "class MultiLayerPerceptron():\n",
    "    # ================== Activation Functions ================ #\n",
    "\n",
    "    # accepts a vector or list and returns a list after performing corresponding function on all elements\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(vectorSig):\n",
    "        \"\"\"returns 1/(1+exp(-x)), where the output values lies between zero and one\"\"\"\n",
    "        sig = expit(vectorSig)\n",
    "        return sig\n",
    "\n",
    "    @staticmethod\n",
    "    def binaryStep(x):\n",
    "        \"\"\" It returns '0' is the input is less then zero otherwise it returns one \"\"\"\n",
    "        return np.heaviside(x, 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def linear(x):\n",
    "        \"\"\" y = f(x) It returns the input as it is\"\"\"\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        \"\"\" It returns the value (1-exp(-2x))/(1+exp(-2x)) and the value returned will be lies in between -1 to 1\"\"\"\n",
    "        return np.tanh(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(x):  # Rectified Linear Unit\n",
    "        \"\"\" It returns zero if the input is less than zero otherwise it returns the given input\"\"\"\n",
    "        x1 = []\n",
    "        for i in x:\n",
    "            if i < 0:\n",
    "                x1.append(0)\n",
    "            else:\n",
    "                x1.append(i)\n",
    "\n",
    "        return x1\n",
    "\n",
    "    @staticmethod\n",
    "    def leakyRelu(x):\n",
    "        \"\"\" It returns zero if the input is less than zero otherwise it returns the given input\"\"\"\n",
    "        x1 = []\n",
    "        for i in x:\n",
    "            if i < 0:\n",
    "                x1.append((0.01 * i))\n",
    "            else:\n",
    "                x1.append(i)\n",
    "\n",
    "        return x1\n",
    "\n",
    "    @staticmethod\n",
    "    def parametricRelu(self, a, x):\n",
    "        \"\"\" It returns zero if the input is less than zero otherwise it returns the given input\"\"\"\n",
    "        x1 = []\n",
    "        for i in x:\n",
    "            if i < 0:\n",
    "                x1.append((a * i))\n",
    "            else:\n",
    "                x1.append(i)\n",
    "\n",
    "        return x1\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(self, x):\n",
    "        \"\"\" Compute softmax values for each sets of scores in x\"\"\"\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "    # ============ Activation Functions Part Ends ============= #\n",
    "\n",
    "    # ================= Distance Calculation ================== #\n",
    "\n",
    "    @staticmethod\n",
    "    def chebishev(self, cord1, cord2, exponent_h):\n",
    "        dist = 0.0\n",
    "        if ((type(cord1) == int and type(cord2) == int) or ((type(cord1) == float and type(cord2) == float))):\n",
    "            dist = math.pow((cord1 - cord2), exponent_h)\n",
    "        else:\n",
    "            for i, j in zip(cord1, cord2):\n",
    "                dist += math.pow((i - j), exponent_h)\n",
    "        dist = math.pow(dist, (1.0 / exponent_h))\n",
    "        return dist\n",
    "\n",
    "    @staticmethod\n",
    "    def minimum_distance(self, cord1, cord2):\n",
    "        # min(|x1-y1|, |x2-y2|, |x3-y3|, ...)\n",
    "        dist = float('inf')\n",
    "        if ((type(cord1) == int and type(cord2) == int) or ((type(cord1) == float and type(cord2) == float))):\n",
    "            dist = math.fabs(cord1 - cord2)\n",
    "        else:\n",
    "            for i, j in zip(cord1, cord2):\n",
    "                temp_dist = math.fabs(i - j)\n",
    "                if (temp_dist < dist):\n",
    "                    dist = temp_dist\n",
    "        return dist\n",
    "\n",
    "    @staticmethod\n",
    "    def maximum_distance(self, cord1, cord2):\n",
    "        # max(|x1-y1|, |x2-y2|, |x3-y3|, ...)\n",
    "        dist = float('-inf')\n",
    "        if ((type(cord1) == int and type(cord2) == int) or ((type(cord1) == float and type(cord2) == float))):\n",
    "            dist = math.fabs(cord1 - cord2)\n",
    "        else:\n",
    "            for i, j in zip(cord1, cord2):\n",
    "                temp_dist = math.fabs(i - j)\n",
    "                if (temp_dist > dist):\n",
    "                    dist = temp_dist\n",
    "        return dist\n",
    "\n",
    "    @staticmethod\n",
    "    def manhattan(self, cord1, cord2):\n",
    "        # |x1-y1| + |x2-y2| + |x3-y3| + ...\n",
    "        dist = 0.0\n",
    "        if ((type(cord1) == int and type(cord2) == int) or ((type(cord1) == float and type(cord2) == float))):\n",
    "            dist = math.fabs(cord1 - cord2)\n",
    "        else:\n",
    "            for i, j in zip(cord1, cord2):\n",
    "                dist += math.fabs(i - j)\n",
    "        return dist\n",
    "\n",
    "    @staticmethod\n",
    "    def eucledian(self, cord1, cord2):\n",
    "        dist = 0.0\n",
    "        if ((type(cord1) == int and type(cord2) == int) or ((type(cord1) == float and type(cord2) == float))):\n",
    "            dist = math.pow((cord1 - cord2), 2)\n",
    "        else:\n",
    "            for i, j in zip(cord1, cord2):\n",
    "                dist += math.pow((i - j), 2)\n",
    "        return math.pow(dist, 0.5)\n",
    "\n",
    "    # =========== Distance Calculation Ends ============== #\n",
    "\n",
    "    def __init__(self, dimensions=(8, 5), all_weights=(0.1, 0.2), fileName=\"iris\"):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dimensions : dimension of the neural network\n",
    "            all_weights : the optimal weights we get from the bio-algoANN models\n",
    "        \"\"\"\n",
    "\n",
    "        self.allPop_Weights = []\n",
    "        self.allPopl_Chromosomes = []\n",
    "        self.allPop_ReceivedOut = []\n",
    "        self.allPop_ErrorVal = []\n",
    "\n",
    "        self.all_weights = all_weights\n",
    "\n",
    "        self.fitness = []\n",
    "\n",
    "        # ================== Input dataset and corresponding output ========================= #\n",
    "\n",
    "        self.fileName = fileName\n",
    "        self.fileName += \".csv\"\n",
    "        data = pd.read_csv(self.fileName)\n",
    "\n",
    "        classes = []\n",
    "        output_values_expected = []\n",
    "        input_values = []\n",
    "\n",
    "        # ~~~~ encoding ~~~~#\n",
    "\n",
    "        # labelencoder = LabelEncoder()\n",
    "        # data[data.columns[-1]] = labelencoder.fit_transform(data[data.columns[-1]])\n",
    "\n",
    "        # one hot encoding - for multi-column\n",
    "        # enc = OneHotEncoder(handle_unknown='ignore')\n",
    "        # combinedData = np.vstack((data[data.columns[-2]], data[data.columns[-1]])).T\n",
    "        # print(combinedData)\n",
    "        # y = enc.fit_transform(combinedData).toarray()\n",
    "        # y = OneHotEncoder().fit_transform(combinedData).toarray()\n",
    "\n",
    "        #\n",
    "        y = LabelBinarizer().fit_transform(data[data.columns[-1]])\n",
    "        # print(y)\n",
    "\n",
    "        # ~~~~ encoding ends~~~~#\n",
    "\n",
    "        for j in range(len(data)):\n",
    "            output_values_expected.append(y[j])\n",
    "\n",
    "        # print(output_values_expected)\n",
    "\n",
    "        input_values = []\n",
    "        for j in range(len(data)):\n",
    "            b = []\n",
    "            for i in range(1, len(data.columns) - 1):\n",
    "                b.append(data[data.columns[i]][j])\n",
    "            input_values.append(b)\n",
    "\n",
    "        self.X = input_values[:]\n",
    "        self.Y = output_values_expected[:]\n",
    "\n",
    "        # input and output\n",
    "        self.X = input_values[:]\n",
    "        self.Y = output_values_expected[:]\n",
    "\n",
    "        self.dimension = dimensions\n",
    "        # print(self.dimension)\n",
    "\n",
    "        # ================ Finding Initial Weights ================ #\n",
    "\n",
    "        self.pop = []  # weights\n",
    "        reshaped_all_weights = []\n",
    "        start = 0\n",
    "        for i in range(len(self.dimension) - 1):\n",
    "            end = start + self.dimension[i + 1] * self.dimension[i]\n",
    "            temp_arr = self.all_weights[start:end]\n",
    "            w = np.reshape(temp_arr[:], (self.dimension[i + 1], self.dimension[i]))\n",
    "            reshaped_all_weights.append(w)\n",
    "            start = end\n",
    "        self.pop.append(reshaped_all_weights)\n",
    "\n",
    "        self.init_pop = self.all_weights\n",
    "\n",
    "    # ================ Initial Weights Part Ends ================ #\n",
    "\n",
    "\n",
    "    def Predict(self, chromo):\n",
    "        # X, Y and pop are used\n",
    "        self.fitness = []\n",
    "        total_error = 0\n",
    "        m_arr = []\n",
    "        k1 = 0\n",
    "        for i in range(len(self.dimension) - 1):\n",
    "            p = self.dimension[i]\n",
    "            q = self.dimension[i + 1]\n",
    "            k2 = k1 + p * q\n",
    "            m_temp = chromo[k1:k2]\n",
    "            m_arr.append(np.reshape(m_temp, (p, q)))\n",
    "            k1 = k2\n",
    "\n",
    "        y_predicted = []\n",
    "        for x, y in zip(self.X, self.Y):\n",
    "\n",
    "            yo = x\n",
    "\n",
    "            for mCount in range(len(m_arr)):\n",
    "                yo = np.dot(yo, m_arr[mCount])\n",
    "                yo = self.sigmoid(yo)\n",
    "            \n",
    "            # converting to sklearn acceptable form\n",
    "            max_yo = max(yo)\n",
    "            for y_vals in range(len(yo)):\n",
    "                if(yo[y_vals] == max_yo):\n",
    "                    yo[y_vals] = 1\n",
    "                else:\n",
    "                    yo[y_vals] = 0\n",
    "            y_predicted.append(yo)\n",
    "        return (y_predicted, self.Y)\n",
    "\n",
    "    def main(self):\n",
    "        Y_PREDICT, Y_ACTUAL = self.Predict(self.init_pop)\n",
    "        Y_PREDICT = np.array(Y_PREDICT)\n",
    "        Y_ACTUAL = np.array(Y_ACTUAL)\n",
    "        \n",
    "        n_classes = 3\n",
    "        \n",
    "        label_binarizer = LabelBinarizer()\n",
    "        label_binarizer.fit(range(n_classes))\n",
    "        Y_PREDICT = label_binarizer.inverse_transform(np.array(Y_PREDICT))\n",
    "        Y_ACTUAL = label_binarizer.inverse_transform(np.array(Y_ACTUAL))\n",
    "        \n",
    "        # find error\n",
    "        \n",
    "        print(\"\\n Actual / Expected\", Y_ACTUAL)\n",
    "        print(\"\\n Predictions\", Y_PREDICT)\n",
    "        print(\"\\n\\nConfusion Matrix\")\n",
    "        print(confusion_matrix(Y_ACTUAL, Y_PREDICT))\n",
    "        \n",
    "        print(\"\\n\\nClassification Report\")\n",
    "        target_names = ['class 0', 'class 1', 'class 2']\n",
    "        print(classification_report(Y_ACTUAL, Y_PREDICT, target_names=target_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e6305ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inputting data :  0.007977962493896484\n",
      "============ Calling GA to get best weights ===============\n",
      "--------------GENERATION 0-----------\n",
      "--------------GENERATION 1-----------\n",
      "--------------GENERATION 2-----------\n",
      "--------------GENERATION 3-----------\n",
      "--------------GENERATION 4-----------\n",
      "--------------GENERATION 5-----------\n",
      "--------------GENERATION 6-----------\n",
      "--------------GENERATION 7-----------\n",
      "--------------GENERATION 8-----------\n",
      "--------------GENERATION 9-----------\n",
      "--------------GENERATION 10-----------\n",
      "--------------GENERATION 11-----------\n",
      "--------------GENERATION 12-----------\n",
      "--------------GENERATION 13-----------\n",
      "--------------GENERATION 14-----------\n",
      "--------------GENERATION 15-----------\n",
      "--------------GENERATION 16-----------\n",
      "--------------GENERATION 17-----------\n",
      "--------------GENERATION 18-----------\n",
      "--------------GENERATION 19-----------\n",
      "--------------GENERATION 20-----------\n",
      "--------------GENERATION 21-----------\n",
      "--------------GENERATION 22-----------\n",
      "--------------GENERATION 23-----------\n",
      "--------------GENERATION 24-----------\n",
      "--------------GENERATION 25-----------\n",
      "--------------GENERATION 26-----------\n",
      "--------------GENERATION 27-----------\n",
      "--------------GENERATION 28-----------\n",
      "--------------GENERATION 29-----------\n",
      "--------------GENERATION 30-----------\n",
      "--------------GENERATION 31-----------\n",
      "--------------GENERATION 32-----------\n",
      "--------------GENERATION 33-----------\n",
      "--------------GENERATION 34-----------\n",
      "--------------GENERATION 35-----------\n",
      "--------------GENERATION 36-----------\n",
      "--------------GENERATION 37-----------\n",
      "--------------GENERATION 38-----------\n",
      "--------------GENERATION 39-----------\n",
      "--------------GENERATION 40-----------\n",
      "--------------GENERATION 41-----------\n",
      "--------------GENERATION 42-----------\n",
      "--------------GENERATION 43-----------\n",
      "--------------GENERATION 44-----------\n",
      "--------------GENERATION 45-----------\n",
      "--------------GENERATION 46-----------\n",
      "--------------GENERATION 47-----------\n",
      "--------------GENERATION 48-----------\n",
      "--------------GENERATION 49-----------\n",
      "--------------GENERATION 50-----------\n",
      "--------------GENERATION 51-----------\n",
      "--------------GENERATION 52-----------\n",
      "--------------GENERATION 53-----------\n",
      "--------------GENERATION 54-----------\n",
      "--------------GENERATION 55-----------\n",
      "--------------GENERATION 56-----------\n",
      "--------------GENERATION 57-----------\n",
      "--------------GENERATION 58-----------\n",
      "--------------GENERATION 59-----------\n",
      "--------------GENERATION 60-----------\n",
      "--------------GENERATION 61-----------\n",
      "--------------GENERATION 62-----------\n",
      "--------------GENERATION 63-----------\n",
      "--------------GENERATION 64-----------\n",
      "--------------GENERATION 65-----------\n",
      "--------------GENERATION 66-----------\n",
      "--------------GENERATION 67-----------\n",
      "--------------GENERATION 68-----------\n",
      "--------------GENERATION 69-----------\n",
      "--------------GENERATION 70-----------\n",
      "--------------GENERATION 71-----------\n",
      "--------------GENERATION 72-----------\n",
      "--------------GENERATION 73-----------\n",
      "--------------GENERATION 74-----------\n",
      "--------------GENERATION 75-----------\n",
      "--------------GENERATION 76-----------\n",
      "--------------GENERATION 77-----------\n",
      "--------------GENERATION 78-----------\n",
      "--------------GENERATION 79-----------\n",
      "--------------GENERATION 80-----------\n",
      "--------------GENERATION 81-----------\n",
      "--------------GENERATION 82-----------\n",
      "--------------GENERATION 83-----------\n",
      "--------------GENERATION 84-----------\n",
      "--------------GENERATION 85-----------\n",
      "--------------GENERATION 86-----------\n",
      "--------------GENERATION 87-----------\n",
      "--------------GENERATION 88-----------\n",
      "--------------GENERATION 89-----------\n",
      "--------------GENERATION 90-----------\n",
      "--------------GENERATION 91-----------\n",
      "--------------GENERATION 92-----------\n",
      "--------------GENERATION 93-----------\n",
      "--------------GENERATION 94-----------\n",
      "--------------GENERATION 95-----------\n",
      "--------------GENERATION 96-----------\n",
      "--------------GENERATION 97-----------\n",
      "--------------GENERATION 98-----------\n",
      "--------------GENERATION 99-----------\n",
      "Fitness :  59.64182868626619\n",
      "Time taken :  1270.7174079418182\n",
      "\n",
      " Fitness :  59.64182868626619 \n",
      " Best Weights :  [-42, 26, 55, 26, -64, -97, 8, -73, 11, 13, -76, 82, -64, 8, 14, -70, -82, 19, -63, -37, 69, -86, 39, -61, -22, -49, -83, -44, 51, 30, -20, -76, -47, -55, 35, 38, 41, 69, -43, 95, -94, -35, -25, -88, 11, -13, -44, 29, -29, 74, 2, 69, -43, 70, 17, 63, 63, 90, -25, 88, -26, 49, 16, 4, 75, 85, 90, -23, 97, 90, 25, -26, 74, -100, -28, -92, 0, 59, 14, 5, -79, 68, 85, -32, -21, -27, -26, -11, 32, -83, -46, -5, -82, 31, 81, -38, 67, -26, -66, 29, -62, -100, 27, -25, 24, 3, -8, 27, 87, 78, 58, 42, -66, 9, 72, -70, 11, 96, -29, -20, -56, -77, -32, -72, 93, 92, 44, 25, 56, -21, 88, -99, -24, 61, 76, 79, -27, -37, -85, 75, 65, -68, -99, -35, 48, 62, -78, -12, 93, 51, 84, -2, 47, -84, -59, 20, -89, 1, -39, -12, 98, -42, 49, 77, -77, 87, 45, -67, 88, -91, 0, 80, 3, -51, -81, 52, -79, -68, 67, -28, 55, 39, 47, -16, -28, -54, 81, 41, -27, 94, 69, 29, -10, -19, -56, 59, 60, -72, -13, 72, 41, 37, -84, -95, -28, 45, -41, 83, 99, -81, 45, 26, 34, -59, 64, 35, 30, -2, 48, 0, 11, -29, -71, -7, -81, -83, 30, -100, -48, -71, -79, -18, 84, -17, 57, 28, 59, 78, 59, 42, 67, 11, 8, 22, -65, -94, 3, 43, 85, -87, -26, 24, -86, 81, -42, -86, -26, -48, -93, 90, -39, -26, 78, 88, 52, 3, 32, -5, -49, -40, -61, -27, 99, -93, -7, 82, 75, 88, -93, -92, -29, -32, -73, 43, 77, -50, -32, -92, -20, -32, 41, -79, -22, -63, -95, 98, -45, -86, 21, 42, -33, -12, -20, 95, 25, 82, 77, 74, 24, -77, 82, 31, 2, 5, 86, -48, 40, -94, 22, 56, -13, 28, 82, -31, -33, 5, -61, -83, -33, 7, 51, 20, 19, -14, 26, -4, 53, -64, -31, 13, 58, 39, -74, -34, 70, 0, -92, -14, 97, -86, -88, 31, 38, -75, 45, -19, 28, -60, 37, -19, -32, 61, -11, -60, 59, -76, 57, 0, -83, 83, -74, 26, -45, -15, 91, -93, 5, -13, 5, -31, 20, -55, 78, -2, 42, -35, -82, 79, 66, 53, 29, 11, 22, -18, 28, 26, 19, -79, 49, 54, 61, 48, 5, -89, 84, 19, 33, -39, -73, 96, 65, -75, 2, 55, 61, -65, -95, -31, -82, -55, -11, 52, -72, -93, -69, -99, -23, 14, 99, 91, 72, -12, 9, 53, -8, -99, -34, 76, -76, 42, 69, 97, -80, 12, -95, 83, -35, -39, -25, -92, -19, -80, 83, -29, -13, 89, -41, 80, 28, -79, 40, 59, -49, 88, 20, 72, 91, -63, 97, 66, 97, 52, -20, 58, 45, 76, -92, 61, -56, 69, 19, -54, -24, -33, 69, -93, 48, 23, -26, -35, -32, -26, 38, 95, 94, -56, 57, -95, 23, 0, -34, 74, 3, -70, -86, -15, -74, -96, 63, 9, -49, 43, -45, -12, -21, 22, 92, 47, 50, -48, -56, -40, 44, -90, 43, 75, -87, -6, -70, 6, 83, -17, -21, 57, -2, 83, -77, -69, 31, -99, 96, -24, 95, 43, -62, -84, 72, -52, -18, -43, 63, -31, -65, 79, -1, 85, -28, 53, -84, -93, -71, 95, 75, 93, -80, -97, 23, 78, -50, 14, -91, -32, -76, 19, 86, -65, -27, 46, -78, 4, 3, 16, -39, 31, -52, -51, 49, 8, 86, -82, -40, -19, 91, 83, 72, 5, 78, -93, -72, -8, 78, -91, 16, -42, 98, -55, 63, 60, 98, 66, -84, 54, -70, -87, 24, -87, -45, 87, 21, -76, -36, 37, 75, -9, -67, -97, -16, -54, -13, 79, 9, 51, -68, -54, -53, -22, -13, 86, 30, -31, 45, -4, 54, 30, -91, -92, 76, 5, 3, -61, -53, -17, 9, -88, 57, 58, 91, 22, -30, -79, 74, -93, 41, -25, -86, -21, 68, -99, 73, 47, 15, -23, 11, 58, 69, 68, 41, -17, 98, 56, 40, -64, 76, 36, -100, 34, 51, -41, 56, -9, -96, -93, 79, -16, -26, 23, 45, 83, 32, -2, 91, 2, 84, 45, 82, -73, 62, -3, -47, -86, -72, 63, -85, 78, 27, -74, 9, -71, 38, -98, -80, 13, -37, -42, -25, -97, -10, 14, -17, -48, 83, 62, 8, -80, -55, 26, -73, 10, 69, -39, -8, -61, -81, -19, 92, -39, 73, -13, 64, -19, -65, -69, -38, -45, 89, 10, 49, -39, 4, -83, 65, -99, 61, -55, 33, 12, -18, -19, 30, -88, 66, -47, -10, -54, 51, 74, -72, -83, -49, 41, -79, -34, -6, 17, 25, -32, 46, 79, 67, 0, -45, -16, 44, -47, -59, 39, -68, 93, 45, 23, -57, 50, -30, 67, 2, -11, -25, 9, 19, -59, 1, 23, -22, -33, -39, -82, -3, 29, 97, -34, -82, 75, 76, 94, 28, 89, -43, -62, -93, -68, 49, -30, -58, 21, -7, -73, -51, 77, 47, 67, 28, 64, -76, -96, -80, -32, -94, -96, 79, -6, -69, 26, -47, 11, -3, 79, -96, 22, -76, 29, -47, 16, 34, -53, -56, -68, 91, 80, -62, -27, -69, -52, -48, -60, 13, -37, -29, -34, 4, -40, -52, 57, -30, -13, 65, 41, 76, -52, -73, -89, 11, -64, 83, 65, -8, -45, 49, 5, -40, -59, -67, -90, 33, 88, -100, -21, 40, 22, -62, 90, -35, 13, 85, 56, -73, -43, -92, -83, -89, 28, -22, -15, -55, -44, -87, 16, -56, 49, 14, -1, 53, 94, 26, -81, 56, 47, 9, 4, -14, -14, -89, -18, 78, 5, -53, -2, 63, -67, 75, 9, -3, -71, 66, -40, -49, 41, 48, 73, -11, 81, -93, 91, -72, 90, 59, 81, -59, -22, -73, -92, 45, -63, -76, -55, -39, -30, -83, 60, 71, -26, 39, 8, -89, 2, -41, -31, -30, 77, -36, 60, -9, 70, 74, 57, 15, -17, -34, -22, -56, -92, -64, -48, 79, -67, -33, -93, -51, -25, -28, -68, 30, 77, 19, -8, -19, -55, -69, 41, 29, -95, 78, 86, -20, -43, 47, 40, 5, 43, 78, -39, 98, 84, 96, 67, 64, -27, -54, -40, 69, 48, 99, 81, 64, -94, -69, 8, -27, -33, 69, -7, 51, 19, -12, 85, -80, -5, 66, 79, -49, -51, 21, 69, -44, -36, 27, 85, -59, -20, 4, 89, 49, 97, -38, -58, 23, -30, 83, 67, 79, -91, 42, 4, -57, -26, 19, -89, -63, 16, -52, 55, -89, -93, 29, -8, -45, 10, 13, 99, 7, -89, 25, 33, -4, 59, -53, 86, 55, -17, 98, -31, -65, -25, 18, -93, -31, -21, -3, -71, -38, -29, -10, -95, 68, 23, 49, 94, 66, -95, 57, -31, -95, -86, 63, -24, 59, -17, 42, 66, 0, -24, 63, 39, 37, 2, -74, 57, 87, -19, 7, 3, -12, -90, 21, 96, 46, 92, -14, 40, -22, -25, 82, 65, 56, 56, -88, 13, -70, -7, 67, 98, -8, -89, 95, 90, -60, -11, 2, -97, 77, -11, 89, -64, -9, -2, -31, -6, -72, 53, 54, 62, -67, -80, -30, -38, -11, 71, 35, -10, -30, -53, -9, -78, -82, -85, -9, 90, 73, 92, -88, 4, 91, -80, -18, 62, 42, -84, -38, 70, -22, 47, 99, 35, 73, -67, -32, 51, 76, 8, -24, 57, -30, -20, -19, -63, -72, -98, -33, -56, -79, -66, 55, 91, 69, 65, 2, -20, 50, 64, -58, -44, 66, 61, -47, 11, 31, 30, 41, -50, -52, 27, 24, 90, -47, -11, 1, -82, -29, -18, 64, 83, -79, -76, -39, -44, 30, -60, -1, -48, -88, 1, -12, 29, -94, -90, -31, 85, 24, 42, 78, -95, 26, -96, 72, 55, 46, 82, -31, -41, -5, 35, -15, 63, -96, 2, -90, 26, -90, -79, -23, -28, 44, 9, 25, 97, -4, -84, -94, -37, -63, -54, -3, 95, 74, 62, -89, 83, 79, -42, 66, 15, 42, 94, -28, 99, -9, -1, -31, -14, 43, 1, 4, -87, -4, 27, 94, 62, -88, 21, 51, -94, 72, -69, 68, 27, -48, -63, 50, -12, -21, 30, -9, 76, 4, 70, 89, -72, -61, 8, 87, -56, 20, -78, 22, 95, 3, 22, 54, 37, -77, 85, 41, -94, 28, -99, 74, -38, 77, 91, -62, -22, 86, 59, 5, 97, 25, -8, 96, 21, 20, 94, 59, -33, -90, -76, 58, 86, 53, -26, 31, -53, 7, 66, -65, -22, 52, -44, 66, -30, -71, -32, -28, 16, 90, 25, -27, -1, -96, 80, 79, 33] \n",
      " Dimensions :  [4, 100, 10, 3]\n",
      "Time Taken :  1270.7802484035492\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAeSUlEQVR4nO3de5xcZZ3n8c+3u7qKdBqSTrrJhIQQLgFFRlB6WC7CssI4oGBcRQRRo4OT14yOouIKzsyKl9UVZLzMrLobBUTFKCAuyM7AYLyACmiCyB2JQEggl+aekJDrb/84pytl0+mu7q7qU3Xq+369+tV1Tp2q+h1OqG8/z3POcxQRmJmZAbRlXYCZmTUOh4KZmZU5FMzMrMyhYGZmZQ4FMzMrcyiYmVmZQ8FsEEn/IOmbdXjfT0r6bq3fN33vYyU9OMzzcyWFpEI9Pt/yw6FgDUHSGZJul/SCpHXp4/dJUp0/93hJqyrXRcTnIuK943jPb0naJmnm+CusTkTcEhEHVdTwqKQTJ+rzLT8cCpY5SecCXwG+APwZMAP4W+AYoJhhaaMmaTLwFuA54B0T9Jn+699qxqFgmZI0Bfg08L6IuDoi1kfidxFxVkRsTrcrSbpY0mOS1kr635Impc8dL2mVpHPTVsZqSe+p+IwhX5t+gf87sJekDenPXoO7eSS9RtKvJT0raaWkdw+zS28Bnk33acEI+/4uSSskPSXpv1f+dZ/W/GVJT6Q/X5ZUGrS/50laA1xW2eKR9B1gDvDjdJ8+VvGxZ6X/HZ6U9I8VtXxS0lWSvitpvaS7JR0o6ePpf9OVkl43/NG0PHAoWNaOAkrAtSNs93ngQOAw4ABgFvCJiuf/DJiSrj8b+Kqk7uFeGxEvACcDT0REV/rzROWHStqHJDj+FehN3+POYepcACwGvg+8TNLhQ20k6WDga8BZwMyK2gf8I3Bk+nmHAkcA/zRof6cB+wALK987It4JPAacmu7TRRVPvwY4CDgB+ISkl1c8dyrwHaAb+B1wI8l3xCySkPs/w+y35YRDwbLWAzwZEdsGVlT8Vb5J0nHpuMJC4MMR8XRErAc+B5xR8T5bgU9HxNaI+DdgA3BQla8dztuBn0TE4vS9n4qIO4faUNIc4L8A34uItcAS4F27eN/TgB9HxC8jYgtJwFVORHZWuj/rIqIf+BTwzorndwAXRMTmiNhU5b4AfCoiNkXE74HfkwTOgFsi4sb0WFxFEoKfj4itJCE3V9LUUXyWNSH3RVrWngJ6JBUGgiEijgZIu0PaSL6cOoFlFePOAtor36cyWICNQFeVrx3O3sAfq9z2ncD9FaFxBfDPkj6afrFW2gtYObAQERslPTXo+RUVyyvSdQP6I+LFKuuqtKbi8cB/owFrKx5vIgnr7RXLpNs/O4bPtSbhloJl7VZgMzB/mG2eJPlSekVETE1/pkRE1zCvqfa1I00TvBLYv4rPgaRVsJ+kNWlf/xdJWkKvH2Lb1cDsgYV0fGR6xfNPkHQNDZiTrhswUt2e/tjGxKFgmYqIZ0m6Rr4m6TRJu0tqk3QYMDndZgfwDeBLkvYEkDRL0l9V8f4jvXYtMD0d8B7KFcCJkk6XVJA0Pa3tT0g6iiQ8jiAZBzgMOAT4HkN3IV0NnCrpaElF4JMkLZgBi4F/ktQrqYeke2k01zisBfYbxfZmgEPBGkA6EPoR4GMkX2ZrSQY1zwN+nW52HrAcuE3S88BPSAZMq7HL10bEAyRfwA+n4xiVXTRExGMkf+mfCzxNMshc2Q8/YAFwbUTcHRFrBn5ITrU9RdK0Qe97L/ABkr761SRjIOtIWk0A/wNYCtwF3A3cka6r1v8kCZVnJX10FK+zFiffZMcse5IG+urnRcQjGZdjLcwtBbOMSDpVUmd6vcTFJC2CR7OtylqdQ8EsO/NJBo+fAOYBZ4Sb7pYxdx+ZmVlZ3VoKki5NL4+/p2LdFyQ9IOkuST+qvBAmvZx+uaQHqzmrxMzMaq9uLQVJx5GcUfHtiDgkXfc64KcRsU3ShQARcV56yf9iktP59iI5O+TAigtnhtTT0xNz586tS/1mZnm1bNmyJyOid6jn6nZFc0TcLGnuoHX/UbF4G8ml/pD0rX4/nfzsEUnLSQLi1uE+Y+7cuSxdurR2RZuZtQBJK3b1XJYDzX9NMtEYJBNurax4bhV/OjlYmaSFkpZKWtrf31/nEs3MWksmoZBO2buN5GrRUYmIRRHRFxF9vb1Dtn7MzGyMJnxCvHQu+lOAEypOv3ucZOKxAbPTdWZmNoEmtKUg6SSSqQzeGBEbK566DjgjvbHIviTnbP9mImszM7M6thQkLQaOJ5kWeRVwAfBxkhuq3JROY3xbRPxtRNwr6UrgPpJupfePdOaRmZnVXlNfvNbX1xc++8jMbHQkLYuIvqGe8zQXZmZW1pKhsPLpjXzqx/eydfuOrEsxM2soLRkKD6xZz2W/epTv3LrL6zfMzFpSS4bCiS/fk+MO7OVLP/kDT27YPPILzMxaREuGgiQ+ccrBbNqynYtvfDDrcszMGkZLhgLAAXt28e6j5/KDpSu5e9VzWZdjZtYQWjYUAD544jymTy7yievu4dY/PsWtf3yKZSueYceO5j1N18xsPCZ8motGssduHZx/8sv56FW/58xv3FZef9FbXsnpf7H3MK80M8unlg4FgNMOn83BM/fguU1bAfjUj+/lm798mLf2zSa96trMrGW0dPfRgIP32oOj9p/OUftP573H7scf1m7gloeezLosM7MJ51AY5NRDZ9LTVeKSXz6SdSlmZhPOoTBIqdDOgqP24Rd/6OehteuzLsfMbEI5FIZw1pH7UCq0cemv3Fows9bS8gPNQ5k2ucibXz2La+54nOPm9VJob6NNcPT+PUwqtmddnplZ3TgUduHs1+zLlUtX8XdX3FFe99fH7MsnTj04w6rMzOrLobALB+y5Oz//6PHlU1W/+rPl/OC3j3HOifOYMqkj4+rMzOrDYwrD2HtaJ4fMmsIhs6bw9689gBe2bGfxbx7Luiwzs7pxKFTpFXtN4ZgDpnPZrx5hyzbfh8HM8smhMArvPXY/1j6/mevveiLrUszM6sKhMArHH9jLvD27+MYtj9DM97Y2M9sVDzSPgiT+5tj9+NgP7+LT19/H9MnFcb/nfr1dvP7PZ9agOjOz8XMojNL8V+3F136+nMt+9WhN3q+9Tbzu4BkU2t1oM7PsORRGqVRo56fnHs/2GnQfffvWFXzm+vt4btNWpneValCdmdn4OBTGoK1NtDH+abV7upLup2c2OhTMrDG4zyJDUzuTUHh245aMKzEzS9QtFCRdKmmdpHsq1r1V0r2SdkjqG7T9xyUtl/SgpL+qV12NpLszuTL6mY1bM67EzCxRz5bCt4CTBq27B3gzcHPlSkkHA2cAr0hf8zVJuZ95rrtzoPvILQUzawx1C4WIuBl4etC6+yPiwSE2nw98PyI2R8QjwHLgiHrV1iimDrQUXnAomFljaJQxhVnAyorlVem6l5C0UNJSSUv7+/snpLh66SoV6GiXu4/MrGE0SihULSIWRURfRPT19vZmXc64SGJqZ9EDzWbWMBolFB4H9q5Ynp2uy73uzg6PKZhZw2iUULgOOENSSdK+wDzgNxnXNCGmdhZ55gV3H5lZY6jnKamLgVuBgyStknS2pP8qaRVwFPD/JN0IEBH3AlcC9wE3AO+PiO31qq2RTOssuqVgZg2jblc0R8SZu3jqR7vY/rPAZ+tVT6PqntzBMyvcUjCzxtAo3Ucta2Cg2VNxm1kjcChkrLuzg207gg2bt2VdipmZQyFrA/MfebDZzBqBQyFj0zzVhZk1EIdCxronD0yK51Aws+w5FDK2c/psdx+ZWfYcChkbmCn1aU+KZ2YNwKGQsSmTOpB8ox0zawwOhYy1t4kpkzo8U6qZNQSHQgPo9lQXZtYgHAoNYGpnhweazawhOBQaQHdn0QPNZtYQHAoNoNs32jGzBuFQaADJjXbcfWRm2XMoNIDuyUU2bd3Oi1tb4hYSZtbAHAoNYGqnp7ows8bgUGgA0zxTqpk1CIdCA9g5/5FbCmaWLYdCA9g5U6pbCmaWLYdCA+j2PRXMrEE4FBpAeaDZF7CZWcYcCg2gVGhncrHd3UdmljmHQoOY6quazawBOBQaRPfkDo8pmFnmHAoNoruzyNPuPjKzjNUtFCRdKmmdpHsq1k2TdJOkh9Lf3el6SfoXScsl3SXp1fWqq1F5UjwzawT1bCl8Czhp0LrzgSURMQ9Yki4DnAzMS38WAl+vY10Nqbuzw2cfmVnmCvV644i4WdLcQavnA8enjy8Hfg6cl67/dkQEcJukqZJmRsTqetXXaKZ2Fnn+xW1cdMMDSFlXs9OBM3Zn/mGzsi7DzCZI3UJhF2ZUfNGvAWakj2cBKyu2W5Wue0koSFpI0ppgzpw59at0gr1y9hR262hj0c0PZ11K2fYIOtraeOOhe6FGSiozq5uJDoWyiAhJMYbXLQIWAfT19Y369Y3qhJfP4IHPnJx1GX/ikl8+wmeuv4/nNm0tz89kZvk20WcfrZU0EyD9vS5d/ziwd8V2s9N1lqGeriQIntywOeNKzGyiTHQoXAcsSB8vAK6tWP+u9CykI4HnWmk8oVH1dpUA6F/vAXCzVlG37iNJi0kGlXskrQIuAD4PXCnpbGAFcHq6+b8BrweWAxuB99SrLqtez+5JKLilYNY66nn20Zm7eOqEIbYN4P31qsXGpqfLoWDWanxFs+3S1EkdtLfJoWDWQhwKtkttbWL65CJPekzBrGU4FGxYPV0ltxTMWohDwYbVs7tDwayVOBRsWD1dRZ7c4O4js1bhULBh9XaV6N+wmeQEMTPLO4eCDaunq8SWbTtYv3lb1qWY2QRwKNiwenZPprroX+9xBbNW4FCwYZUvYHMomLUEh4INa+dVzR5sNmsFDgUblqe6MGstDgUb1rTJRdrkUDBrFQ4FG1Z7m5g22RewmbUKh4KNqKer6HsqmLUIh4KNqNdTXZi1DIeCjciT4pm1DoeCjSiZ/8hTXZi1AoeCjainq8SLW3fwwpbtWZdiZnU26lCQ1C3plfUoxhqTr2o2ax1VhYKkn0vaQ9I04A7gG5K+WN/SrFH07O4L2MxaRbUthSkR8TzwZuDbEfGfgBPrV5Y1kp6uZFI8h4JZ/lUbCgVJM4HTgevrWI81oN60+6jf8x+Z5V61ofBp4EZgeUT8VtJ+wEP1K8saybTJRSSPKZi1gkI1G0XEVcBVFcsPA2+pV1HWWArtbXR3Ful395FZ7lU70HxROtDcIWmJpH5J76h3cdY4erqKbimYtYBqu49elw40nwI8ChwA/LexfqikcyTdI+leSR9K102TdJOkh9Lf3WN9f6s9X9Vs1hqqHmhOf78BuCoinhvrB0o6BPgb4AjgUOAUSQcA5wNLImIesCRdtgaRhIIHms3yrqoxBeB6SQ8Am4C/k9QLvDjGz3w5cHtEbASQ9AuSU13nA8en21wO/Bw4b4yfYTXW01Vi3foXueaOVS95bsqkDl77sj2RlEFlZlZL1Q40ny/pIuC5iNguaSPJl/hY3AN8VtJ0kpB5PbAUmBERq9Nt1gAzhnqxpIXAQoA5c+aMsQQbrf33nMyLW3fwkSt/P+TzS879z+zf2zXBVZlZrVUVCpI6gfcBc0i+kPcCDmIM1yxExP2SLgT+A3gBuBPYPmibkDTk7GsRsQhYBNDX1+cZ2ibI24+Yw3HzetkxaFK82x5+ivN+eDcbXtyWUWVmVkvVdh9dBiwDjk6XHyc5RXVMF7JFxCXAJQCSPgesAtZKmhkRq9ML5daN5b2tPiSx97TOl6xf9cwmADZv2zHRJZlZHVQ70Lx/RFwEbAVIxwPG3IEsac/09xyS8YTvAdcBC9JNFgDXjvX9beIUC8k/oc3bPIOqWR5U21LYImkSEACS9gfGc37iD9Mxha3A+yPiWUmfB66UdDawgmRKDWtwpTQUtrilYJYL1YbCBcANwN6SrgCOAd491g+NiGOHWPcUcMJY39OysbOl4FAwy4Nqzz66SdIdwJEk3UbnRMSTda3MmkKp0A64pWCWF9W2FAB2A55JX3OwJCLi5vqUZc2i5DEFs1yp9pTUC4G3AfcCA38SBuBQaHHuPjLLl2pbCm8CDooIT35jf8IDzWb5Uu0pqQ8DHfUsxJqTWwpm+VJtS2EjcKekJVScihoRH6xLVdY0iu0OBbM8qTYUrkt/KnmKCUMSpUKbB5rNcqLaUJgaEV+pXCHpnDrUY02oWGhj81a3FMzyoNoxhQVDrHt3DeuwJlYqtLNlu0PBLA+GbSlIOhN4O7CvpMruo92Bp+tZmDWPklsKZrkxUvfRr4HVQA/wzxXr1wN31asoay6lQptbCmY5MWwoRMQKksnpjpqYcqwZJWMKHmg2y4ORuo9+GRGvkbSePz3bSCT3wtmjrtVZU0jOPnJLwSwPRuo+OgsgInafgFqsSZUK7b6i2SwnRjr76EcDDyT9sM61WJMq+joFs9wYKRQq7662Xz0LseblgWaz/BgpFGIXj83KSh0+JdUsL0YaUzhU0vMkLYZJ6WPwQLNVKLZ7oNksL0Y6JbV9ogqx5uWBZrP8qHaaC7Nd8kCzWX44FGzcfJ2CWX44FGzcSh1t7j4yywmHgo1bsb2dbTuC7Tt8gppZs3Mo2LiVOnyfZrO8cCjYuO28JacHm82aXSahIOnDku6VdI+kxZJ2k7SvpNslLZf0A0nFLGqz0RtoKXiw2az5TXgoSJoFfBDoi4hDgHbgDOBC4EsRcQDwDHD2RNdmY1MqJJezuPvIrPll1X1UILlCugB0ktzI57XA1enzlwNvyqY0G61iwd1HZnkx4aEQEY8DFwOPkYTBc8Ay4NmI2JZutgqYNdTrJS2UtFTS0v7+/oko2UZQKrj7yCwvsug+6gbmA/sCewGTgZOqfX1ELIqIvojo6+3trVOVNhpFh4JZbmTRfXQi8EhE9EfEVuAa4BhgatqdBDAbeDyD2mwMyi0Fz5Rq1vSyCIXHgCMldUoScAJwH/Az4LR0mwXAtRnUZmNQHmj2PRXMml4WYwq3kwwo3wHcndawCDgP+Iik5cB04JKJrs3GZmdLwQPNZs1upPsp1EVEXABcMGj1w8ARGZRj4zQQCm4pmDU/X9Fs41b0mIJZbjgUbNwGxhR89pFZ83Mo2LiVu4988ZpZ03Mo2Lj5OgWz/HAo2LjtbCk4FMyanUPBxq3Q3kab3FIwywOHgtVEqdDuCfHMcsChYDXh+zSb5YNDwWqi2N7m7iOzHHAoWE24pWCWDw4Fqwm3FMzywaFgNeGBZrN8cChYTZQ63FIwywOHgtWEu4/M8sGhYDVR6mj3QLNZDjgUrCbcUjDLB4eC1UQypuCBZrNm51CwmigVfJ2CWR44FKwmSgV3H5nlgUPBaqJUaGfzVncfmTU7h4LVRLHQxpbtbimYNTuHgtXEQPdRRGRdipmNg0PBaqLY3kYEbNvhUDBrZg4Fq4lSh+/TbJYHDgWriVKhHcCDzWZNbsJDQdJBku6s+Hle0ockTZN0k6SH0t/dE12bjV2xkPxT8mCzWXOb8FCIiAcj4rCIOAw4HNgI/Ag4H1gSEfOAJemyNYlSGgqbtzoUzJpZ1t1HJwB/jIgVwHzg8nT95cCbsirKRs8tBbN8yDoUzgAWp49nRMTq9PEaYEY2JdlY7BxTcCiYNbPMQkFSEXgjcNXg5yI52X3IcxslLZS0VNLS/v7+Oldp1Sp3H3lSPLOmlmVL4WTgjohYmy6vlTQTIP29bqgXRcSiiOiLiL7e3t4JKtVGUu4+8impZk0ty1A4k51dRwDXAQvSxwuAaye8IhuznS0Fh4JZM8skFCRNBv4SuKZi9eeBv5T0EHBiumxNouhQMMuFQhYfGhEvANMHrXuK5Gwka0LlgWaPKZg1tazPPrKccPeRWT44FKwmSh5oNssFh4LVxM7uI4eCWTNzKFhN+JRUs3xwKFhNFH3xmlkuOBSsJtrbREe73H1k1uQcClYzxfY2dx+ZNTmHgtVMqaPd3UdmTc6hYDXjloJZ83MoWM2UOto8pmDW5BwKVjOlQpvvp2DW5BwKVjPFQpvvvGbW5BwKVjOlggeazZqdQ8Fqptju7iOzZudQsJopdbj7yKzZORSsZjzQbNb8HApWM8VCu1sKZk3OoWA1k7QUPNBs1swcClYzxYIvXjNrdg4Fq5lSwdNcmDU7h4LVTHKdgkPBrJk5FKxmBq5ojoisSzGzMXIoWM2Uyndfc2vBrFk5FKxmHApmzc+hYDUzEAoebDZrXg4Fq5lSoR3Ak+KZNbFMQkHSVElXS3pA0v2SjpI0TdJNkh5Kf3dnUZuNXdEtBbOml1VL4SvADRHxMuBQ4H7gfGBJRMwDlqTL1kQ8pmDW/CY8FCRNAY4DLgGIiC0R8SwwH7g83exy4E0TXZuNT9GhYNb0Chl85r5AP3CZpEOBZcA5wIyIWJ1uswaYMdSLJS0EFgLMmTOn/tVa1QbGFP7+e3cwqaM942rM8u1tf7E37z12v5q/bxahUABeDXwgIm6X9BUGdRVFREga8gqoiFgELALo6+vzVVIN5JV7T+Gth8/mhS3bsi7FLPd6ukp1ed8sQmEVsCoibk+XryYJhbWSZkbEakkzgXUZ1GbjsMduHXzhrYdmXYaZjcOEjylExBpgpaSD0lUnAPcB1wEL0nULgGsnujYzs1aXRUsB4APAFZKKwMPAe0gC6kpJZwMrgNMzqs3MrGVlEgoRcSfQN8RTJ0xwKWZmVsFXNJuZWZlDwczMyhwKZmZW5lAwM7Myh4KZmZWpmW+dKKmf5PTVsegBnqxhOc2iFfe7FfcZWnO/W3GfYfT7vU9E9A71RFOHwnhIWhoRQ50Wm2utuN+tuM/QmvvdivsMtd1vdx+ZmVmZQ8HMzMpaORQWZV1ARlpxv1txn6E197sV9xlquN8tO6ZgZmYv1cotBTMzG8ShYGZmZS0ZCpJOkvSgpOWSzh/5Fc1H0t6SfibpPkn3SjonXT9N0k2SHkp/d2ddaz1Iapf0O0nXp8v7Sro9PeY/SKdtzw1JUyVdLekBSfdLOqoVjrWkD6f/vu+RtFjSbnk81pIulbRO0j0V64Y8vkr8S7r/d0l69Wg+q+VCQVI78FXgZOBg4ExJB2dbVV1sA86NiIOBI4H3p/t5PrAkIuYBSxh0K9QcOQe4v2L5QuBLEXEA8AxwdiZV1c9XgBsi4mXAoST7nutjLWkW8EGgLyIOAdqBM8jnsf4WcNKgdbs6vicD89KfhcDXR/NBLRcKwBHA8oh4OCK2AN8H5mdcU81FxOqIuCN9vJ7kS2IWyb5enm52OfCmTAqsI0mzgTcA30yXBbyW5NavkLP9ljQFOA64BCAitkTEs7TAsSa5J8wkSQWgE1hNDo91RNwMPD1o9a6O73zg25G4DZia3uK4Kq0YCrOAlRXLq9J1uSVpLvAq4HZgRkSsTp9aA8zIqq46+jLwMWBHujwdeDYitqXLeTvm+wL9wGVpl9k3JU0m58c6Ih4HLgYeIwmD54Bl5PtYV9rV8R3Xd1wrhkJLkdQF/BD4UEQ8X/lcJOcj5+qcZEmnAOsiYlnWtUygAvBq4OsR8SrgBQZ1FeX0WHeT/FW8L7AXMJmXdrG0hFoe31YMhceBvSuWZ6frckdSB0kgXBER16Sr1w40JdPf67Kqr06OAd4o6VGSrsHXkvS3T027GCB/x3wVsCoibk+XryYJibwf6xOBRyKiPyK2AteQHP88H+tKuzq+4/qOa8VQ+C0wLz1DoUgyMHVdxjXVXNqPfglwf0R8seKp64AF6eMFwLUTXVs9RcTHI2J2RMwlObY/jYizgJ8Bp6Wb5Wq/I2INsFLSQemqE4D7yPmxJuk2OlJSZ/rvfWC/c3usB9nV8b0OeFd6FtKRwHMV3UwjaskrmiW9nqTfuR24NCI+m21FtSfpNcAtwN3s7Fv/B5JxhSuBOSTTjp8eEYMHsHJB0vHARyPiFEn7kbQcpgG/A94REZszLK+mJB1GMrBeBB4G3kPyR1+uj7WkTwFvIznb7nfAe0n6z3N1rCUtBo4nmSJ7LXAB8H8Z4vimAfm/SLrSNgLviYilVX9WK4aCmZkNrRW7j8zMbBccCmZmVuZQMDOzMoeCmZmVORTMzKzMoWBWBUnTJd2Z/qyR9Hj6eIOkr2Vdn1mt+JRUs1GS9ElgQ0RcnHUtZrXmloLZOEg6vuKeDZ+UdLmkWyStkPRmSRdJulvSDem0I0g6XNIvJC2TdONoZrA0qzeHgllt7U8y39Ibge8CP4uIPwc2AW9Ig+FfgdMi4nDgUiB3V9Rb8yqMvImZjcK/R8RWSXeTTKNyQ7r+bmAucBBwCHBTMhsB7STTPps1BIeCWW1tBoiIHZK2xs5Bux0k/78JuDcijsqqQLPhuPvIbGI9CPRKOgqS6c0lvSLjmszKHApmEyi9BexpwIWSfg/cCRydaVFmFXxKqpmZlbmlYGZmZQ4FMzMrcyiYmVmZQ8HMzMocCmZmVuZQMDOzMoeCmZmV/X/ec4hJ7XjmXAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "i = InputData(fileName=\"../ANN/iris\")\n",
    "input_val, output_val = i.main()\n",
    "end_time = time.time()\n",
    "print(\"Time for inputting data : \", end_time - start_time)\n",
    "        \n",
    "print(\"============ Calling GA to get best weights ===============\")\n",
    "\n",
    "n_iterations = 100\n",
    "e_rate = 0.1\n",
    "\n",
    "start_time = time.time()\n",
    "a = gaAnn(initialPopSize=100, m = 10, dimensions = [100,10], bestCount = 20, input_values=input_val , output_values_expected=output_val, iterations = n_iterations, elicitation_rate = e_rate)\n",
    "\n",
    "fit, b, weights, dim = a.main()\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Time taken : \", end_time - start_time)\n",
    "\n",
    "print(\"\\n Fitness : \", fit, \"\\n Best Weights : \", weights, \"\\n Dimensions : \", dim)\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "x=b[:]\n",
    "z=[i for i in range(0,100)]\n",
    "plt.plot(z,x)\n",
    "\n",
    "plt.title(\"Genetic Algorithm\")\n",
    "plt.ylabel(\"Fitness\")\n",
    "plt.xlabel(\"Time\")\n",
    "end_time = time.time()\n",
    "print(\"Time Taken : \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55db955e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============= MLP Program Begins ============\n",
      "Training\n",
      "\n",
      " Actual / Expected [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      " Predictions [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 1 1 0 0 0 2 1 0 0 1 2 1 1 0 0 0 2\n",
      " 0 0 0 1 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "[[40  0  0]\n",
      " [22 15  3]\n",
      " [ 0  0 40]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.65      1.00      0.78        40\n",
      "     class 1       1.00      0.38      0.55        40\n",
      "     class 2       0.93      1.00      0.96        40\n",
      "\n",
      "    accuracy                           0.79       120\n",
      "   macro avg       0.86      0.79      0.76       120\n",
      "weighted avg       0.86      0.79      0.76       120\n",
      "\n",
      "Time taken =  0.010971784591674805\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n============= MLP Program Begins ============\")\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Training\")\n",
    "m = MultiLayerPerceptron(fileName=\"../ANN/iris_train\", dimensions=dim, all_weights=weights)\n",
    "m.main()\n",
    "end_time = time.time()\n",
    "print(\"Time taken = \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5037ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n",
      "\n",
      " Actual / Expected [0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      " Predictions [0 0 0 0 0 0 0 0 0 0 2 1 0 0 1 0 0 0 0 0 2 2 2 2 2 2 2 2 1 2]\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "[[10  0  0]\n",
      " [ 7  2  1]\n",
      " [ 0  1  9]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.59      1.00      0.74        10\n",
      "     class 1       0.67      0.20      0.31        10\n",
      "     class 2       0.90      0.90      0.90        10\n",
      "\n",
      "    accuracy                           0.70        30\n",
      "   macro avg       0.72      0.70      0.65        30\n",
      "weighted avg       0.72      0.70      0.65        30\n",
      "\n",
      "Time taken =  0.005984306335449219\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(\"Testing\")\n",
    "m = MultiLayerPerceptron(fileName=\"../ANN/iris_test\", dimensions=dim, all_weights=weights)\n",
    "m.main()\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Time taken = \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6609313e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
