{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a71460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2364f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn import preprocessing\n",
    "from scipy.special import expit\n",
    "\n",
    "from numpy.random import default_rng\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import time\n",
    "\n",
    "from gaAnn_thread_V_I import *\n",
    "\n",
    "\n",
    "\n",
    "class MultiLayerPerceptron():\n",
    "    # ================== Activation Functions ================ #\n",
    "\n",
    "    # accepts a vector or list and returns a list after performing corresponding function on all elements\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(vectorSig):\n",
    "        \"\"\"returns 1/(1+exp(-x)), where the output values lies between zero and one\"\"\"\n",
    "        sig = expit(vectorSig)\n",
    "        return sig\n",
    "\n",
    "    @staticmethod\n",
    "    def binaryStep(x):\n",
    "        \"\"\" It returns '0' is the input is less then zero otherwise it returns one \"\"\"\n",
    "        return np.heaviside(x, 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def linear(x):\n",
    "        \"\"\" y = f(x) It returns the input as it is\"\"\"\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        \"\"\" It returns the value (1-exp(-2x))/(1+exp(-2x)) and the value returned will be lies in between -1 to 1\"\"\"\n",
    "        return np.tanh(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(x):  # Rectified Linear Unit\n",
    "        \"\"\" It returns zero if the input is less than zero otherwise it returns the given input\"\"\"\n",
    "        x1 = []\n",
    "        for i in x:\n",
    "            if i < 0:\n",
    "                x1.append(0)\n",
    "            else:\n",
    "                x1.append(i)\n",
    "\n",
    "        return x1\n",
    "\n",
    "    @staticmethod\n",
    "    def leakyRelu(x):\n",
    "        \"\"\" It returns zero if the input is less than zero otherwise it returns the given input\"\"\"\n",
    "        x1 = []\n",
    "        for i in x:\n",
    "            if i < 0:\n",
    "                x1.append((0.01 * i))\n",
    "            else:\n",
    "                x1.append(i)\n",
    "\n",
    "        return x1\n",
    "\n",
    "    @staticmethod\n",
    "    def parametricRelu(self, a, x):\n",
    "        \"\"\" It returns zero if the input is less than zero otherwise it returns the given input\"\"\"\n",
    "        x1 = []\n",
    "        for i in x:\n",
    "            if i < 0:\n",
    "                x1.append((a * i))\n",
    "            else:\n",
    "                x1.append(i)\n",
    "\n",
    "        return x1\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(self, x):\n",
    "        \"\"\" Compute softmax values for each sets of scores in x\"\"\"\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "    # ============ Activation Functions Part Ends ============= #\n",
    "\n",
    "    # ================= Distance Calculation ================== #\n",
    "\n",
    "    @staticmethod\n",
    "    def chebishev(self, cord1, cord2, exponent_h):\n",
    "        dist = 0.0\n",
    "        if ((type(cord1) == int and type(cord2) == int) or ((type(cord1) == float and type(cord2) == float))):\n",
    "            dist = math.pow((cord1 - cord2), exponent_h)\n",
    "        else:\n",
    "            for i, j in zip(cord1, cord2):\n",
    "                dist += math.pow((i - j), exponent_h)\n",
    "        dist = math.pow(dist, (1.0 / exponent_h))\n",
    "        return dist\n",
    "\n",
    "    @staticmethod\n",
    "    def minimum_distance(self, cord1, cord2):\n",
    "        # min(|x1-y1|, |x2-y2|, |x3-y3|, ...)\n",
    "        dist = float('inf')\n",
    "        if ((type(cord1) == int and type(cord2) == int) or ((type(cord1) == float and type(cord2) == float))):\n",
    "            dist = math.fabs(cord1 - cord2)\n",
    "        else:\n",
    "            for i, j in zip(cord1, cord2):\n",
    "                temp_dist = math.fabs(i - j)\n",
    "                if (temp_dist < dist):\n",
    "                    dist = temp_dist\n",
    "        return dist\n",
    "\n",
    "    @staticmethod\n",
    "    def maximum_distance(self, cord1, cord2):\n",
    "        # max(|x1-y1|, |x2-y2|, |x3-y3|, ...)\n",
    "        dist = float('-inf')\n",
    "        if ((type(cord1) == int and type(cord2) == int) or ((type(cord1) == float and type(cord2) == float))):\n",
    "            dist = math.fabs(cord1 - cord2)\n",
    "        else:\n",
    "            for i, j in zip(cord1, cord2):\n",
    "                temp_dist = math.fabs(i - j)\n",
    "                if (temp_dist > dist):\n",
    "                    dist = temp_dist\n",
    "        return dist\n",
    "\n",
    "    @staticmethod\n",
    "    def manhattan(self, cord1, cord2):\n",
    "        # |x1-y1| + |x2-y2| + |x3-y3| + ...\n",
    "        dist = 0.0\n",
    "        if ((type(cord1) == int and type(cord2) == int) or ((type(cord1) == float and type(cord2) == float))):\n",
    "            dist = math.fabs(cord1 - cord2)\n",
    "        else:\n",
    "            for i, j in zip(cord1, cord2):\n",
    "                dist += math.fabs(i - j)\n",
    "        return dist\n",
    "\n",
    "    @staticmethod\n",
    "    def eucledian(self, cord1, cord2):\n",
    "        dist = 0.0\n",
    "        if ((type(cord1) == int and type(cord2) == int) or ((type(cord1) == float and type(cord2) == float))):\n",
    "            dist = math.pow((cord1 - cord2), 2)\n",
    "        else:\n",
    "            for i, j in zip(cord1, cord2):\n",
    "                dist += math.pow((i - j), 2)\n",
    "        return math.pow(dist, 0.5)\n",
    "\n",
    "    # =========== Distance Calculation Ends ============== #\n",
    "\n",
    "    def __init__(self, dimensions=(8, 5), all_weights=(0.1, 0.2), fileName=\"iris\"):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dimensions : dimension of the neural network\n",
    "            all_weights : the optimal weights we get from the bio-algoANN models\n",
    "        \"\"\"\n",
    "\n",
    "        self.allPop_Weights = []\n",
    "        self.allPopl_Chromosomes = []\n",
    "        self.allPop_ReceivedOut = []\n",
    "        self.allPop_ErrorVal = []\n",
    "\n",
    "        self.all_weights = all_weights\n",
    "\n",
    "        self.fitness = []\n",
    "\n",
    "        # ================== Input dataset and corresponding output ========================= #\n",
    "\n",
    "        self.fileName = fileName\n",
    "        self.fileName += \".csv\"\n",
    "        data = pd.read_csv(self.fileName)\n",
    "\n",
    "        classes = []\n",
    "        output_values_expected = []\n",
    "        input_values = []\n",
    "\n",
    "        # ~~~~ encoding ~~~~#\n",
    "\n",
    "        # labelencoder = LabelEncoder()\n",
    "        # data[data.columns[-1]] = labelencoder.fit_transform(data[data.columns[-1]])\n",
    "\n",
    "        # one hot encoding - for multi-column\n",
    "        # enc = OneHotEncoder(handle_unknown='ignore')\n",
    "        # combinedData = np.vstack((data[data.columns[-2]], data[data.columns[-1]])).T\n",
    "        # print(combinedData)\n",
    "        # y = enc.fit_transform(combinedData).toarray()\n",
    "        # y = OneHotEncoder().fit_transform(combinedData).toarray()\n",
    "\n",
    "        #\n",
    "        y = LabelBinarizer().fit_transform(data[data.columns[-1]])\n",
    "        # print(y)\n",
    "\n",
    "        # ~~~~ encoding ends~~~~#\n",
    "\n",
    "        for j in range(len(data)):\n",
    "            output_values_expected.append(y[j])\n",
    "\n",
    "        # print(output_values_expected)\n",
    "\n",
    "        input_values = []\n",
    "        for j in range(len(data)):\n",
    "            b = []\n",
    "            for i in range(1, len(data.columns) - 1):\n",
    "                b.append(data[data.columns[i]][j])\n",
    "            input_values.append(b)\n",
    "\n",
    "        self.X = input_values[:]\n",
    "        self.Y = output_values_expected[:]\n",
    "\n",
    "        # input and output\n",
    "        self.X = input_values[:]\n",
    "        self.Y = output_values_expected[:]\n",
    "\n",
    "        self.dimension = dimensions\n",
    "        # print(self.dimension)\n",
    "\n",
    "        # ================ Finding Initial Weights ================ #\n",
    "\n",
    "        self.pop = []  # weights\n",
    "        reshaped_all_weights = []\n",
    "        start = 0\n",
    "        for i in range(len(self.dimension) - 1):\n",
    "            end = start + self.dimension[i + 1] * self.dimension[i]\n",
    "            temp_arr = self.all_weights[start:end]\n",
    "            w = np.reshape(temp_arr[:], (self.dimension[i + 1], self.dimension[i]))\n",
    "            reshaped_all_weights.append(w)\n",
    "            start = end\n",
    "        self.pop.append(reshaped_all_weights)\n",
    "\n",
    "        self.init_pop = self.all_weights\n",
    "\n",
    "    # ================ Initial Weights Part Ends ================ #\n",
    "\n",
    "\n",
    "    def Predict(self, chromo):\n",
    "        # X, Y and pop are used\n",
    "        self.fitness = []\n",
    "        total_error = 0\n",
    "        m_arr = []\n",
    "        k1 = 0\n",
    "        for i in range(len(self.dimension) - 1):\n",
    "            p = self.dimension[i]\n",
    "            q = self.dimension[i + 1]\n",
    "            k2 = k1 + p * q\n",
    "            m_temp = chromo[k1:k2]\n",
    "            m_arr.append(np.reshape(m_temp, (p, q)))\n",
    "            k1 = k2\n",
    "\n",
    "        y_predicted = []\n",
    "        for x, y in zip(self.X, self.Y):\n",
    "\n",
    "            yo = x\n",
    "\n",
    "            for mCount in range(len(m_arr)):\n",
    "                yo = np.dot(yo, m_arr[mCount])\n",
    "                yo = self.sigmoid(yo)\n",
    "            \n",
    "            # converting to sklearn acceptable form\n",
    "            max_yo = max(yo)\n",
    "            for y_vals in range(len(yo)):\n",
    "                if(yo[y_vals] == max_yo):\n",
    "                    yo[y_vals] = 1\n",
    "                else:\n",
    "                    yo[y_vals] = 0\n",
    "            y_predicted.append(yo)\n",
    "        return (y_predicted, self.Y)\n",
    "\n",
    "    def main(self):\n",
    "        Y_PREDICT, Y_ACTUAL = self.Predict(self.init_pop)\n",
    "        Y_PREDICT = np.array(Y_PREDICT)\n",
    "        Y_ACTUAL = np.array(Y_ACTUAL)\n",
    "        \n",
    "        n_classes = 3\n",
    "        \n",
    "        label_binarizer = LabelBinarizer()\n",
    "        label_binarizer.fit(range(n_classes))\n",
    "        Y_PREDICT = label_binarizer.inverse_transform(np.array(Y_PREDICT))\n",
    "        Y_ACTUAL = label_binarizer.inverse_transform(np.array(Y_ACTUAL))\n",
    "        \n",
    "        # find error\n",
    "        \n",
    "        print(\"\\n Actual / Expected\", Y_ACTUAL)\n",
    "        print(\"\\n Predictions\", Y_PREDICT)\n",
    "        print(\"\\n\\nConfusion Matrix\")\n",
    "        print(confusion_matrix(Y_ACTUAL, Y_PREDICT))\n",
    "        \n",
    "        print(\"\\n\\nClassification Report\")\n",
    "        target_names = ['class 0', 'class 1', 'class 2']\n",
    "        print(classification_report(Y_ACTUAL, Y_PREDICT, target_names=target_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e6305ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inputting data :  0.006981372833251953\n",
      "============ Calling GA to get best weights ===============\n",
      "--------------GENERATION 0-----------\n",
      "--------------GENERATION 1-----------\n",
      "--------------GENERATION 2-----------\n",
      "--------------GENERATION 3-----------\n",
      "--------------GENERATION 4-----------\n",
      "--------------GENERATION 5-----------\n",
      "--------------GENERATION 6-----------\n",
      "--------------GENERATION 7-----------\n",
      "--------------GENERATION 8-----------\n",
      "--------------GENERATION 9-----------\n",
      "--------------GENERATION 10-----------\n",
      "--------------GENERATION 11-----------\n",
      "--------------GENERATION 12-----------\n",
      "--------------GENERATION 13-----------\n",
      "--------------GENERATION 14-----------\n",
      "--------------GENERATION 15-----------\n",
      "--------------GENERATION 16-----------\n",
      "--------------GENERATION 17-----------\n",
      "--------------GENERATION 18-----------\n",
      "--------------GENERATION 19-----------\n",
      "--------------GENERATION 20-----------\n",
      "--------------GENERATION 21-----------\n",
      "--------------GENERATION 22-----------\n",
      "--------------GENERATION 23-----------\n",
      "--------------GENERATION 24-----------\n",
      "--------------GENERATION 25-----------\n",
      "--------------GENERATION 26-----------\n",
      "--------------GENERATION 27-----------\n",
      "--------------GENERATION 28-----------\n",
      "--------------GENERATION 29-----------\n",
      "--------------GENERATION 30-----------\n",
      "--------------GENERATION 31-----------\n",
      "--------------GENERATION 32-----------\n",
      "--------------GENERATION 33-----------\n",
      "--------------GENERATION 34-----------\n",
      "--------------GENERATION 35-----------\n",
      "--------------GENERATION 36-----------\n",
      "--------------GENERATION 37-----------\n",
      "--------------GENERATION 38-----------\n",
      "--------------GENERATION 39-----------\n",
      "--------------GENERATION 40-----------\n",
      "--------------GENERATION 41-----------\n",
      "--------------GENERATION 42-----------\n",
      "--------------GENERATION 43-----------\n",
      "--------------GENERATION 44-----------\n",
      "--------------GENERATION 45-----------\n",
      "--------------GENERATION 46-----------\n",
      "--------------GENERATION 47-----------\n",
      "--------------GENERATION 48-----------\n",
      "--------------GENERATION 49-----------\n",
      "--------------GENERATION 50-----------\n",
      "--------------GENERATION 51-----------\n",
      "--------------GENERATION 52-----------\n",
      "--------------GENERATION 53-----------\n",
      "--------------GENERATION 54-----------\n",
      "--------------GENERATION 55-----------\n",
      "--------------GENERATION 56-----------\n",
      "--------------GENERATION 57-----------\n",
      "--------------GENERATION 58-----------\n",
      "--------------GENERATION 59-----------\n",
      "--------------GENERATION 60-----------\n",
      "--------------GENERATION 61-----------\n",
      "--------------GENERATION 62-----------\n",
      "--------------GENERATION 63-----------\n",
      "--------------GENERATION 64-----------\n",
      "--------------GENERATION 65-----------\n",
      "--------------GENERATION 66-----------\n",
      "--------------GENERATION 67-----------\n",
      "--------------GENERATION 68-----------\n",
      "--------------GENERATION 69-----------\n",
      "--------------GENERATION 70-----------\n",
      "--------------GENERATION 71-----------\n",
      "--------------GENERATION 72-----------\n",
      "--------------GENERATION 73-----------\n",
      "--------------GENERATION 74-----------\n",
      "--------------GENERATION 75-----------\n",
      "--------------GENERATION 76-----------\n",
      "--------------GENERATION 77-----------\n",
      "--------------GENERATION 78-----------\n",
      "--------------GENERATION 79-----------\n",
      "--------------GENERATION 80-----------\n",
      "--------------GENERATION 81-----------\n",
      "--------------GENERATION 82-----------\n",
      "--------------GENERATION 83-----------\n",
      "--------------GENERATION 84-----------\n",
      "--------------GENERATION 85-----------\n",
      "--------------GENERATION 86-----------\n",
      "--------------GENERATION 87-----------\n",
      "--------------GENERATION 88-----------\n",
      "--------------GENERATION 89-----------\n",
      "--------------GENERATION 90-----------\n",
      "--------------GENERATION 91-----------\n",
      "--------------GENERATION 92-----------\n",
      "--------------GENERATION 93-----------\n",
      "--------------GENERATION 94-----------\n",
      "--------------GENERATION 95-----------\n",
      "--------------GENERATION 96-----------\n",
      "--------------GENERATION 97-----------\n",
      "--------------GENERATION 98-----------\n",
      "--------------GENERATION 99-----------\n",
      "Fitness :  -60.785917958364514\n",
      "Time taken :  3320.0137643814087\n",
      "\n",
      " Fitness :  -60.785917958364514 \n",
      " Best Weights :  [4, 3, -3, -4, 2, 8, 4, 4, -4, 3, -1, 1, -3, -10, -3, -3, 3, 2, -9, 0, -8, 3, -4, -5, 3, 5, -5, 7, 1, -5, -3, 5, -10, 1, 5, 1, 0, -1, -6, -10, -8, 4, -2, -4, -6, 7, -10, 6, -4, 8, 7, -6, 8, 4, 5, 9, -3, 6, -3, 1, -3, -9, 3, 5, 4, -6, -9, -9, -9, 1, 7, 8, 5, 2, -10, -8, -5, 0, -5, -7, -10, 9, -3, -9, 8, 4, -3, 7, 8, -6, -3, -3, 3, 2, 2, -6, -10, 6, -4, -3, -2, 7, -10, -1, -4, -6, -6, -10, 3, -7, -2, -6, 6, 9, -7, 7, -5, 6, 1, -5, -2, 7, -1, 6, -7, 6, -5, 5, 4, -5, 3, 6, 5, -3, -9, -1, -3, 2, 2, -6, 6, -9, 0, -4, -6, 3, 5, 3, 4, 5, -8, -4, -10, 2, 1, -1, -7, -8, 2, 0, 8, -9, -1, 7, 8, -4, 4, 1, 0, 4, 9, -5, -7, -8, 1, -5, 0, -7, -10, 5, 2, 2, -10, 6, 5, 0, 9, -2, 1, 4, -6, 2, -5, -1, -9, -9, 3, -10, -1, 2, -10, 7, -9, 4, -3, -8, 6, -4, 2, -3, 9, 3, -4, -2, -6, 0, -4, 6, -7, -8, 0, 0, 7, 9, -2, 5, 7, -5, 0, -4, 9, -8, 7, 0, -4, -4, 4, 7, 1, -2, -10, -10, 1, -5, -2, -3, -1, 4, -5, 6, -2, 8, 5, 0, -4, 2, 5, -4, -1, 2, -5, -1, 1, -6, 5, 5, 6, -6, 3, -8, 7, 9, 6, 3, 7, 9, 9, -10, 7, -7, 6, -7, -1, -3, 5, -5, -8, -3, 4, 8, 0, -7, 7, -1, -5, -1, -8, 8, 0, -4, -8, -2, 1, 1, 6, -8, -5, 2, 6, -3, -4, -6, -8, -5, -10, 2, 4, 8, -8, 0, -9, -10, 9, -1, -7, 9, 9, 0, -4, -8, -3, -5, 8, 1, -3, -6, -1, 7, 5, -1, -2, -10, -2, 8, 4, 4, -4, 1, -10, 1, 1, 2, 5, -4, 9, -3, -3, 4, -4, 4, 1, 3, 7, -8, 5, -3, 6, 6, 2, -8, -9, 6, 0, 4, 6, 8, -9, -1, 6, 0, 1, -4, 8, 8, 7, 6, -3, -3, -4, 9, 9, 4, 8, -9, -4, -1, 1, -1, -7, 3, 6, 8, -6, 7, 7, -2, -10, 4, -6, -10, 1, -7, -2, 5, -6, -2, -3, -7, 3, -1, 9, 2, 8, 9, 6, -3, 6, 7, -1, 3, 1, -8, 3, 0, -2, -10, 4, -10, 5, -9, -5, 1, -9, -1, -7, 0, 7, -7, 9, 3, -5, -5, -8, -2, 7, 2, -5, -4, -3, 3, -9, 3, 0, -3, -9, -5, 4, -3, 1, -2, -7, -1, 4, 4, 5, -10, 6, 6, -10, 4, 7, -10, -5, 1, 3, -10, -5, -1, -10, -4, -4, 2, -2, 1, 9, 1, -3, 6, -10, -6, -6, -2, 5, 9, 3, -10, 7, 9, 2, 5, -9, 8, 7, -2, 9, 0, -1, 9, -1, -2, -9, -7, -7, -3, -1, -6, -9, 5, 2, 3, -3, 0, -7, -1, -8, 7, 8, 7, 0, 9, 1, -2, 9, -1, 0, 5, 1, 0, -10, 8, 4, -3, -2, 9, 6, -4, 9, -8, -7, 7, 7, 4, 5, -1, -4, 6, 0, -6, 2, 2, 3, -2, 2, -3, 8, 2, -6, 3, -5, 0, -2, -7, -8, 1, 5, 4, 9, 5, 0, -4, -5, 9, -3, -7, 6, -9, 6, -6, 2, -4, -7, -7, 0, -6, 8, 7, 5, -2, 3, 1, -3, -3, -1, 8, 6, 8, 8, 5, 5, 8, -4, -2, 2, -5, 5, -1, -6, 5, 8, 0, 7, -10, 7, 6, 7, -8, -4, 7, 6, 9, -4, -10, 3, -4, 0, -9, -5, -9, -6, -7, 8, -8, -1, 5, 7, -9, -5, -10, 1, -6, 6, 9, -9, -4, 2, 2, 8, -7, -8, -10, 9, -10, 6, -6, -5, -6, 9, 4, 9, 9, -3, 0, -9, -10, -9, 7, 5, -6, -6, 6, 2, -9, -5, 3, -1, -1, 5, 0, -1, 0, 1, 8, 0, 4, -1, -3, 8, -9, 3, -7, 2, -4, 8, -5, -1, 1, 6, 9, -3, -8, -1, 7, 1, 7, 4, -8, -9, -2, 6, -2, 0, -4, 2, 4, -2, -2, -2, -9, -6, 4, 0, -3, -9, 1, -3, 5, 5, -5, -6, 8, 9, 7, -7, -3, -9, 0, 1, 5, -3, -8, 9, -2, -7, 7, 1, 8, -8, -4, 2, 1, 0, 2, 7, 3, 5, -7, 3, 4, -8, 9, -2, 3, -3, -10, -2, 5, -6, 0, -1, -9, 6, 8, -1, -2, -8, -10, -6, 5, 0, 1, -6, 9, -3, -3, 7, -9, 3, 1, -5, -5, 2, 4, 1, 5, 7, -9, 4, -9, 6, -4, 7, -4, -6, -6, 7, -3, 9, 4, -2, -6, 6, 2, -3, -9, -4, 9, 4, -5, 4, 4, -7, -1, -7, 4, 2, 0, -4, -2, -1, -10, 5, -10, 6, -8, 8, 1, 4, 5, -5, 7, 5, -5, 8, 6, 6, 3, 3, -3, -1, 4, -3, 6, -7, -8, 1, -2, -10, 1, 5, 1, -6, 0, -6, 9, 7, -9, 5, -9, -7, 4, -5, -1, 0, -9, -10, 1, -7, 7, -9, 2, 6, -10, 8, -3, -8, 5, -2, 6, 4, -2, -9, -3, 8, -2, -6, 9, 7, 1, -10, 2, 6, 2, 0, 8, 9, -9, 6, 1, 9, 4, 6, -6, 7, -10, 9, -8, -9, 8, 0, 2, -4, -1, 2, -5, 6, -1, -7, -1, -6, -10, 0, -3, 6, -8, -7, -1, -7, 8, 8, -3, -8, -5, -4, -5, -7, -1, -4, 6, -10, 0, -10, -2, 3, 2, -3, -5, -3, -9, -2, -3, -3, 9, 1, -6, 7, -8, -5, 0, 6, -4, -5, 6, 6, 9, 5, -2, 8, -4, -10, -5, 0, -10, -5, 1, 1, 5, 9, 9, -6, 2, -4, 8, 0, 3, 6, -6, -2, -1, -6, -1, 2, -1, -6, -9, -10, 5, -10, 6, -3, 6, -3, -6, -10, -9, 3, 4, 4, -4, 8, 4, 3, -10, 6, 7, 8, -9, -6, -4, 2, -6, 5, 4, 6, 7, 7, -10, -4, 6, 8, -7, -1, -10, -2, -7, 9, -8, -3, -2, 3, 8, -4, 5, 3, 4, 1, 5, -7, 6, -3, -8, -4, 3, 0, -2, 5, -3, 8, -5, 0, 5, 6, 5, -8, 1, -8, -9, 7, 8, -7, -7, 0, -3, 9, 2, 5, 3, 1, -2, -1, 3, -5, -3, 7, 3, -1, 3, -5, -4, 6, -7, 9, -7, 6, -5, -10, -6, 9, -9, 8, 3, -7, 9, -10, 2, 1, 5, 2, -1, -4, -8, 6, -2, -2, 3, -8, 0, -1, -7, -6, -3, 0, 4, -8, -9, 1, 6, 6, 8, 4, -7, -2, 4, -4, -6, -6, 9, 1, -5, -7, -6, -2, -2, -3, -3, 8, 9, -9, 3, 8, -9, -6, 5, -3, -2, -2, 5, 4, -9, -6, 5, 2, 0, 1, 9, -3, 0, -10, -6, -7, -3, -5, -3, -10, 8, -1, 8, -4, -3, 9, 2, 1, 9, 9, 1, -3, 4, -1, 0, -6, -2, 7, 9, 8, -5, 6, 5, 7, 9, 4, 8, -8, 7, -4, -6, -3, 6, 9, -3, 0, 5, -7, 9, -10, 9, -5, -1, -5, 0, -10, 1, -1, 9, -8, 5, 6, -9, 5, -4, 8, 2, -2, -9, 4, 8, -2, -9, -3, 6, 4, 7, 3, 1, -8, -1, 0, 0, 5, -9, -10, 6, -5, -1, -2, -3, -9, -9, -2, -9, -9, 9, 8, -8, 5, 7, -6, 3, -6, 0, 2, 5, 0, -7, 0, 5, -3, -6, 2, -1, 1, -4, -10, 2, -4, 3, -7, 1, -5, 4, 8, 5, 9, -10, 8, 8, 4, -4, 5, 3, 9, 2, 6, 1, -6, -1, 1, -5, -8, 0, -2, 7, -5, -9, 3, -2, 2, -1, -3, -9, -3, 2, 0, -2, -6, -4, -7, 5, 8, 1, 5, -2, -2, 9, 4, -5, -7, 9, 6, -7, 5, 7, 3, 3, -8, 7, 4, 1, -3, 4, 4, 3, -6, -3, 0, 5, -5, 7, -9, -6, -7, -6, -8, 8, 5, -4, -2, 9, -7, -6, 9, 4, -7, -9, 8, -3, -10, -8, 9, 9, 2, -7, 2, -10, 7, -6, -6, 3, 8, 7, 6, -2, -7, 5, -3, 5, -3, 6, -4, -8, -1, 6] \n",
      " Dimensions :  [4, 100, 10, 3]\n",
      "Time Taken :  3320.02872467041\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "i = InputData(fileName=\"../ANN/iris\")\n",
    "input_val, output_val = i.main()\n",
    "end_time = time.time()\n",
    "print(\"Time for inputting data : \", end_time - start_time)\n",
    "        \n",
    "print(\"============ Calling GA to get best weights ===============\")\n",
    "\n",
    "n_iterations = 100\n",
    "e_rate = 0.1\n",
    "\n",
    "start_time = time.time()\n",
    "a = gaAnn(initialPopSize=100, m = 10, dimensions = [100,10], bestCount = 30, input_values=input_val , output_values_expected=output_val, iterations = n_iterations, elicitation_rate = e_rate)\n",
    "\n",
    "fit, b, weights, dim = a.main()\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Time taken : \", end_time - start_time)\n",
    "\n",
    "print(\"\\n Fitness : \", fit, \"\\n Best Weights : \", weights, \"\\n Dimensions : \", dim)\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "x=b[:]\n",
    "z=[i for i in range(0,100)]\n",
    "plt.plot(z,x)\n",
    "\n",
    "plt.title(\"Genetic Algorithm\")\n",
    "plt.ylabel(\"Fitness\")\n",
    "plt.xlabel(\"Time\")\n",
    "end_time = time.time()\n",
    "print(\"Time Taken : \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55db955e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============= MLP Program Begins ============\n",
      "Training\n",
      "\n",
      " Actual / Expected [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      " Predictions [0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 2 0 0 2\n",
      " 0 0 0 1 2 1 2 2 2 2 1 1 2 1 2 1 2 1 1 2 1 2 1 2 1 2 2 1 1 1 2 2 1 1 1 1 2\n",
      " 2 1 1 1 1 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 1 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 1 2 2 2]\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "[[36  0  4]\n",
      " [ 0 22 18]\n",
      " [ 0  4 36]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      0.90      0.95        40\n",
      "     class 1       0.85      0.55      0.67        40\n",
      "     class 2       0.62      0.90      0.73        40\n",
      "\n",
      "    accuracy                           0.78       120\n",
      "   macro avg       0.82      0.78      0.78       120\n",
      "weighted avg       0.82      0.78      0.78       120\n",
      "\n",
      "Time taken =  0.013962507247924805\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n============= MLP Program Begins ============\")\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Training\")\n",
    "m = MultiLayerPerceptron(fileName=\"../ANN/iris_train\", dimensions=dim, all_weights=weights)\n",
    "m.main()\n",
    "end_time = time.time()\n",
    "print(\"Time taken = \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5037ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n",
      "\n",
      " Actual / Expected [0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      " Predictions [0 0 0 0 0 0 0 0 0 0 2 2 1 1 2 1 2 1 1 2 1 2 2 1 1 2 2 2 1 2]\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "[[10  0  0]\n",
      " [ 0  5  5]\n",
      " [ 0  4  6]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      1.00      1.00        10\n",
      "     class 1       0.56      0.50      0.53        10\n",
      "     class 2       0.55      0.60      0.57        10\n",
      "\n",
      "    accuracy                           0.70        30\n",
      "   macro avg       0.70      0.70      0.70        30\n",
      "weighted avg       0.70      0.70      0.70        30\n",
      "\n",
      "Time taken =  0.009974241256713867\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(\"Testing\")\n",
    "m = MultiLayerPerceptron(fileName=\"../ANN/iris_test\", dimensions=dim, all_weights=weights)\n",
    "m.main()\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Time taken = \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6609313e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
