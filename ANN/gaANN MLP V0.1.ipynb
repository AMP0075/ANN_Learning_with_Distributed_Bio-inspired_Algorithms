{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a71460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2364f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn import preprocessing\n",
    "from scipy.special import expit\n",
    "\n",
    "from numpy.random import default_rng\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import time\n",
    "\n",
    "from gaAnn_thread_V_I import *\n",
    "\n",
    "\n",
    "\n",
    "class MultiLayerPerceptron():\n",
    "    # ================== Activation Functions ================ #\n",
    "\n",
    "    # accepts a vector or list and returns a list after performing corresponding function on all elements\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(vectorSig):\n",
    "        \"\"\"returns 1/(1+exp(-x)), where the output values lies between zero and one\"\"\"\n",
    "        sig = expit(vectorSig)\n",
    "        return sig\n",
    "\n",
    "    @staticmethod\n",
    "    def binaryStep(x):\n",
    "        \"\"\" It returns '0' is the input is less then zero otherwise it returns one \"\"\"\n",
    "        return np.heaviside(x, 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def linear(x):\n",
    "        \"\"\" y = f(x) It returns the input as it is\"\"\"\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        \"\"\" It returns the value (1-exp(-2x))/(1+exp(-2x)) and the value returned will be lies in between -1 to 1\"\"\"\n",
    "        return np.tanh(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(x):  # Rectified Linear Unit\n",
    "        \"\"\" It returns zero if the input is less than zero otherwise it returns the given input\"\"\"\n",
    "        x1 = []\n",
    "        for i in x:\n",
    "            if i < 0:\n",
    "                x1.append(0)\n",
    "            else:\n",
    "                x1.append(i)\n",
    "\n",
    "        return x1\n",
    "\n",
    "    @staticmethod\n",
    "    def leakyRelu(x):\n",
    "        \"\"\" It returns zero if the input is less than zero otherwise it returns the given input\"\"\"\n",
    "        x1 = []\n",
    "        for i in x:\n",
    "            if i < 0:\n",
    "                x1.append((0.01 * i))\n",
    "            else:\n",
    "                x1.append(i)\n",
    "\n",
    "        return x1\n",
    "\n",
    "    @staticmethod\n",
    "    def parametricRelu(self, a, x):\n",
    "        \"\"\" It returns zero if the input is less than zero otherwise it returns the given input\"\"\"\n",
    "        x1 = []\n",
    "        for i in x:\n",
    "            if i < 0:\n",
    "                x1.append((a * i))\n",
    "            else:\n",
    "                x1.append(i)\n",
    "\n",
    "        return x1\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(self, x):\n",
    "        \"\"\" Compute softmax values for each sets of scores in x\"\"\"\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "    # ============ Activation Functions Part Ends ============= #\n",
    "\n",
    "    # ================= Distance Calculation ================== #\n",
    "\n",
    "    @staticmethod\n",
    "    def chebishev(self, cord1, cord2, exponent_h):\n",
    "        dist = 0.0\n",
    "        if ((type(cord1) == int and type(cord2) == int) or ((type(cord1) == float and type(cord2) == float))):\n",
    "            dist = math.pow((cord1 - cord2), exponent_h)\n",
    "        else:\n",
    "            for i, j in zip(cord1, cord2):\n",
    "                dist += math.pow((i - j), exponent_h)\n",
    "        dist = math.pow(dist, (1.0 / exponent_h))\n",
    "        return dist\n",
    "\n",
    "    @staticmethod\n",
    "    def minimum_distance(self, cord1, cord2):\n",
    "        # min(|x1-y1|, |x2-y2|, |x3-y3|, ...)\n",
    "        dist = float('inf')\n",
    "        if ((type(cord1) == int and type(cord2) == int) or ((type(cord1) == float and type(cord2) == float))):\n",
    "            dist = math.fabs(cord1 - cord2)\n",
    "        else:\n",
    "            for i, j in zip(cord1, cord2):\n",
    "                temp_dist = math.fabs(i - j)\n",
    "                if (temp_dist < dist):\n",
    "                    dist = temp_dist\n",
    "        return dist\n",
    "\n",
    "    @staticmethod\n",
    "    def maximum_distance(self, cord1, cord2):\n",
    "        # max(|x1-y1|, |x2-y2|, |x3-y3|, ...)\n",
    "        dist = float('-inf')\n",
    "        if ((type(cord1) == int and type(cord2) == int) or ((type(cord1) == float and type(cord2) == float))):\n",
    "            dist = math.fabs(cord1 - cord2)\n",
    "        else:\n",
    "            for i, j in zip(cord1, cord2):\n",
    "                temp_dist = math.fabs(i - j)\n",
    "                if (temp_dist > dist):\n",
    "                    dist = temp_dist\n",
    "        return dist\n",
    "\n",
    "    @staticmethod\n",
    "    def manhattan(self, cord1, cord2):\n",
    "        # |x1-y1| + |x2-y2| + |x3-y3| + ...\n",
    "        dist = 0.0\n",
    "        if ((type(cord1) == int and type(cord2) == int) or ((type(cord1) == float and type(cord2) == float))):\n",
    "            dist = math.fabs(cord1 - cord2)\n",
    "        else:\n",
    "            for i, j in zip(cord1, cord2):\n",
    "                dist += math.fabs(i - j)\n",
    "        return dist\n",
    "\n",
    "    @staticmethod\n",
    "    def eucledian(self, cord1, cord2):\n",
    "        dist = 0.0\n",
    "        if ((type(cord1) == int and type(cord2) == int) or ((type(cord1) == float and type(cord2) == float))):\n",
    "            dist = math.pow((cord1 - cord2), 2)\n",
    "        else:\n",
    "            for i, j in zip(cord1, cord2):\n",
    "                dist += math.pow((i - j), 2)\n",
    "        return math.pow(dist, 0.5)\n",
    "\n",
    "    # =========== Distance Calculation Ends ============== #\n",
    "\n",
    "    def __init__(self, dimensions=(8, 5), all_weights=(0.1, 0.2), fileName=\"iris\"):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dimensions : dimension of the neural network\n",
    "            all_weights : the optimal weights we get from the bio-algoANN models\n",
    "        \"\"\"\n",
    "\n",
    "        self.allPop_Weights = []\n",
    "        self.allPopl_Chromosomes = []\n",
    "        self.allPop_ReceivedOut = []\n",
    "        self.allPop_ErrorVal = []\n",
    "\n",
    "        self.all_weights = all_weights\n",
    "\n",
    "        self.fitness = []\n",
    "\n",
    "        # ================== Input dataset and corresponding output ========================= #\n",
    "\n",
    "        self.fileName = fileName\n",
    "        self.fileName += \".csv\"\n",
    "        data = pd.read_csv(self.fileName)\n",
    "\n",
    "        classes = []\n",
    "        output_values_expected = []\n",
    "        input_values = []\n",
    "\n",
    "        # ~~~~ encoding ~~~~#\n",
    "\n",
    "        # labelencoder = LabelEncoder()\n",
    "        # data[data.columns[-1]] = labelencoder.fit_transform(data[data.columns[-1]])\n",
    "\n",
    "        # one hot encoding - for multi-column\n",
    "        # enc = OneHotEncoder(handle_unknown='ignore')\n",
    "        # combinedData = np.vstack((data[data.columns[-2]], data[data.columns[-1]])).T\n",
    "        # print(combinedData)\n",
    "        # y = enc.fit_transform(combinedData).toarray()\n",
    "        # y = OneHotEncoder().fit_transform(combinedData).toarray()\n",
    "\n",
    "        #\n",
    "        y = LabelBinarizer().fit_transform(data[data.columns[-1]])\n",
    "        # print(y)\n",
    "\n",
    "        # ~~~~ encoding ends~~~~#\n",
    "\n",
    "        for j in range(len(data)):\n",
    "            output_values_expected.append(y[j])\n",
    "\n",
    "        # print(output_values_expected)\n",
    "\n",
    "        input_values = []\n",
    "        for j in range(len(data)):\n",
    "            b = []\n",
    "            for i in range(1, len(data.columns) - 1):\n",
    "                b.append(data[data.columns[i]][j])\n",
    "            input_values.append(b)\n",
    "\n",
    "        self.X = input_values[:]\n",
    "        self.Y = output_values_expected[:]\n",
    "\n",
    "        # input and output\n",
    "        self.X = input_values[:]\n",
    "        self.Y = output_values_expected[:]\n",
    "\n",
    "        self.dimension = dimensions\n",
    "        # print(self.dimension)\n",
    "\n",
    "        # ================ Finding Initial Weights ================ #\n",
    "\n",
    "        self.pop = []  # weights\n",
    "        reshaped_all_weights = []\n",
    "        start = 0\n",
    "        for i in range(len(self.dimension) - 1):\n",
    "            end = start + self.dimension[i + 1] * self.dimension[i]\n",
    "            temp_arr = self.all_weights[start:end]\n",
    "            w = np.reshape(temp_arr[:], (self.dimension[i + 1], self.dimension[i]))\n",
    "            reshaped_all_weights.append(w)\n",
    "            start = end\n",
    "        self.pop.append(reshaped_all_weights)\n",
    "\n",
    "        self.init_pop = self.all_weights\n",
    "\n",
    "    # ================ Initial Weights Part Ends ================ #\n",
    "\n",
    "\n",
    "    def Predict(self, chromo):\n",
    "        # X, Y and pop are used\n",
    "        self.fitness = []\n",
    "        total_error = 0\n",
    "        m_arr = []\n",
    "        k1 = 0\n",
    "        for i in range(len(self.dimension) - 1):\n",
    "            p = self.dimension[i]\n",
    "            q = self.dimension[i + 1]\n",
    "            k2 = k1 + p * q\n",
    "            m_temp = chromo[k1:k2]\n",
    "            m_arr.append(np.reshape(m_temp, (p, q)))\n",
    "            k1 = k2\n",
    "\n",
    "        y_predicted = []\n",
    "        for x, y in zip(self.X, self.Y):\n",
    "\n",
    "            yo = x\n",
    "\n",
    "            for mCount in range(len(m_arr)):\n",
    "                yo = np.dot(yo, m_arr[mCount])\n",
    "                yo = self.sigmoid(yo)\n",
    "            \n",
    "            # converting to sklearn acceptable form\n",
    "            max_yo = max(yo)\n",
    "            for y_vals in range(len(yo)):\n",
    "                if(yo[y_vals] == max_yo):\n",
    "                    yo[y_vals] = 1\n",
    "                else:\n",
    "                    yo[y_vals] = 0\n",
    "            y_predicted.append(yo)\n",
    "        return (y_predicted, self.Y)\n",
    "\n",
    "    def main(self):\n",
    "        Y_PREDICT, Y_ACTUAL = self.Predict(self.init_pop)\n",
    "        Y_PREDICT = np.array(Y_PREDICT)\n",
    "        Y_ACTUAL = np.array(Y_ACTUAL)\n",
    "        \n",
    "        n_classes = 3\n",
    "        \n",
    "        label_binarizer = LabelBinarizer()\n",
    "        label_binarizer.fit(range(n_classes))\n",
    "        Y_PREDICT = label_binarizer.inverse_transform(np.array(Y_PREDICT))\n",
    "        Y_ACTUAL = label_binarizer.inverse_transform(np.array(Y_ACTUAL))\n",
    "        \n",
    "        # find error\n",
    "        \n",
    "        print(\"\\n Actual / Expected\", Y_ACTUAL)\n",
    "        print(\"\\n Predictions\", Y_PREDICT)\n",
    "        print(\"\\n\\nConfusion Matrix\")\n",
    "        print(confusion_matrix(Y_ACTUAL, Y_PREDICT))\n",
    "        \n",
    "        print(\"\\n\\nClassification Report\")\n",
    "        target_names = ['class 0', 'class 1', 'class 2']\n",
    "        print(classification_report(Y_ACTUAL, Y_PREDICT, target_names=target_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e6305ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inputting data :  0.006978511810302734\n",
      "============ Calling GA to get best weights ===============\n",
      "--------------GENERATION 0-----------\n",
      "--------------GENERATION 1-----------\n",
      "--------------GENERATION 2-----------\n",
      "--------------GENERATION 3-----------\n",
      "--------------GENERATION 4-----------\n",
      "--------------GENERATION 5-----------\n",
      "--------------GENERATION 6-----------\n",
      "--------------GENERATION 7-----------\n",
      "--------------GENERATION 8-----------\n",
      "--------------GENERATION 9-----------\n",
      "--------------GENERATION 10-----------\n",
      "--------------GENERATION 11-----------\n",
      "--------------GENERATION 12-----------\n",
      "--------------GENERATION 13-----------\n",
      "--------------GENERATION 14-----------\n",
      "--------------GENERATION 15-----------\n",
      "--------------GENERATION 16-----------\n",
      "--------------GENERATION 17-----------\n",
      "--------------GENERATION 18-----------\n",
      "--------------GENERATION 19-----------\n",
      "--------------GENERATION 20-----------\n",
      "--------------GENERATION 21-----------\n",
      "--------------GENERATION 22-----------\n",
      "--------------GENERATION 23-----------\n",
      "--------------GENERATION 24-----------\n",
      "--------------GENERATION 25-----------\n",
      "--------------GENERATION 26-----------\n",
      "--------------GENERATION 27-----------\n",
      "--------------GENERATION 28-----------\n",
      "--------------GENERATION 29-----------\n",
      "--------------GENERATION 30-----------\n",
      "--------------GENERATION 31-----------\n",
      "--------------GENERATION 32-----------\n",
      "--------------GENERATION 33-----------\n",
      "--------------GENERATION 34-----------\n",
      "--------------GENERATION 35-----------\n",
      "--------------GENERATION 36-----------\n",
      "--------------GENERATION 37-----------\n",
      "--------------GENERATION 38-----------\n",
      "--------------GENERATION 39-----------\n",
      "--------------GENERATION 40-----------\n",
      "--------------GENERATION 41-----------\n",
      "--------------GENERATION 42-----------\n",
      "--------------GENERATION 43-----------\n",
      "--------------GENERATION 44-----------\n",
      "--------------GENERATION 45-----------\n",
      "--------------GENERATION 46-----------\n",
      "--------------GENERATION 47-----------\n",
      "--------------GENERATION 48-----------\n",
      "--------------GENERATION 49-----------\n",
      "--------------GENERATION 50-----------\n",
      "--------------GENERATION 51-----------\n",
      "--------------GENERATION 52-----------\n",
      "--------------GENERATION 53-----------\n",
      "--------------GENERATION 54-----------\n",
      "--------------GENERATION 55-----------\n",
      "--------------GENERATION 56-----------\n",
      "--------------GENERATION 57-----------\n",
      "--------------GENERATION 58-----------\n",
      "--------------GENERATION 59-----------\n",
      "--------------GENERATION 60-----------\n",
      "--------------GENERATION 61-----------\n",
      "--------------GENERATION 62-----------\n",
      "--------------GENERATION 63-----------\n",
      "--------------GENERATION 64-----------\n",
      "--------------GENERATION 65-----------\n",
      "--------------GENERATION 66-----------\n",
      "--------------GENERATION 67-----------\n",
      "--------------GENERATION 68-----------\n",
      "--------------GENERATION 69-----------\n",
      "--------------GENERATION 70-----------\n",
      "--------------GENERATION 71-----------\n",
      "--------------GENERATION 72-----------\n",
      "--------------GENERATION 73-----------\n",
      "--------------GENERATION 74-----------\n",
      "--------------GENERATION 75-----------\n",
      "--------------GENERATION 76-----------\n",
      "--------------GENERATION 77-----------\n",
      "--------------GENERATION 78-----------\n",
      "--------------GENERATION 79-----------\n",
      "--------------GENERATION 80-----------\n",
      "--------------GENERATION 81-----------\n",
      "--------------GENERATION 82-----------\n",
      "--------------GENERATION 83-----------\n",
      "--------------GENERATION 84-----------\n",
      "--------------GENERATION 85-----------\n",
      "--------------GENERATION 86-----------\n",
      "--------------GENERATION 87-----------\n",
      "--------------GENERATION 88-----------\n",
      "--------------GENERATION 89-----------\n",
      "--------------GENERATION 90-----------\n",
      "--------------GENERATION 91-----------\n",
      "--------------GENERATION 92-----------\n",
      "--------------GENERATION 93-----------\n",
      "--------------GENERATION 94-----------\n",
      "--------------GENERATION 95-----------\n",
      "--------------GENERATION 96-----------\n",
      "--------------GENERATION 97-----------\n",
      "--------------GENERATION 98-----------\n",
      "--------------GENERATION 99-----------\n",
      "Fitness :  81.47852160176494\n",
      "Time taken :  3371.291170835495\n",
      "\n",
      " Fitness :  81.47852160176494 \n",
      " Best Weights :  [-97, 3, -44, 79, 15, -30, -7, 11, 74, -63, 96, 46, 26, -26, 82, 88, 47, -79, 32, 88, -19, -48, 91, 76, 48, 59, 99, 93, 90, -39, -65, -2, -56, -36, 72, -83, 25, -9, -79, -65, -10, 81, 82, -2, 23, 8, -10, -49, -63, -30, -15, 29, 81, 80, 27, -29, -41, -41, -76, -53, 54, -44, 47, -23, -45, -72, 24, -40, -15, -11, 9, -17, 64, 63, 64, -73, -48, -81, -26, -69, -37, 94, -90, -14, -72, -44, -98, -44, 89, 63, 51, 67, -25, -61, -53, -41, -88, -82, -7, 95, 23, -81, -4, 6, 96, 1, 17, -35, -41, -79, 10, -27, -6, 68, -74, -48, 35, -55, 49, 12, 34, -12, -6, -52, -84, 84, -59, 86, 56, 95, -79, -24, 53, -87, 83, -90, -92, -62, -81, 38, 60, -40, 1, -35, 81, -18, 10, 62, 89, -26, 39, -58, -100, 53, 11, -55, 55, -85, -22, 43, -27, -29, 59, -83, -5, -11, -92, 70, 87, -67, -3, -93, -35, 70, 46, -85, 23, -23, 72, -10, 79, 92, -15, -58, 1, -84, 5, -84, 60, 83, 25, 74, -8, -68, 54, -94, 44, 79, 67, -28, 29, -95, 64, 44, 7, 3, 56, 60, -18, -86, 97, 49, 76, 70, 16, 96, -96, 8, 71, -50, 86, 6, -64, -72, 24, 95, 80, -47, -17, -78, -38, -74, 95, 87, -69, 92, -2, -83, -99, -11, -65, -26, -12, 24, -49, -43, 57, 84, -32, -53, -81, 23, -94, -96, -59, 89, -51, -25, -82, 45, 75, 71, -76, -16, -97, -91, 92, 0, -56, 25, -65, 76, -55, 80, -90, -47, -93, -6, -57, -18, 78, 38, 46, 56, -51, -48, -78, 85, -5, 80, 36, 88, -85, 93, -33, -60, -63, 43, -99, -75, -18, -71, 43, 4, -45, 45, -97, -18, 58, 71, -98, 50, 7, -97, -95, -15, 22, 39, -43, -11, 24, -45, 31, -13, 44, 96, 55, 16, 18, 49, -53, 91, -100, 41, -59, 69, 23, 75, -53, 79, -76, -7, 33, -20, 72, -82, 48, 1, 12, -2, 28, 75, -17, -42, -71, -28, -96, 35, 13, -2, -98, 30, -4, 51, -54, -85, 78, 86, 45, 16, 99, -12, -20, 37, 20, 25, 53, -43, -38, 95, -80, 66, 57, 31, -71, 94, 44, -43, -70, -31, 50, 64, -31, 86, -42, -33, -38, 6, -31, -10, 40, -1, 60, -84, -9, -43, 15, 57, 20, -25, 48, -43, 18, 13, -20, -91, 22, 37, 69, 66, -11, 1, -35, 62, 32, -31, -52, 40, 52, -61, -40, -12, 91, 33, -23, -84, -46, 98, 79, -65, 57, -8, -55, 60, -36, 34, 36, 40, 62, 25, -89, -80, -1, -28, 68, -76, -16, -70, 55, 0, -58, -7, -60, -98, -33, 82, 58, -62, -75, -63, 49, -18, -43, 75, 70, -60, -64, 40, -10, 28, -39, -42, 34, -82, 85, 79, -28, -3, 55, -76, -15, -64, 76, 7, -52, 76, -87, -22, -25, 70, -8, 76, -87, 12, 71, 94, 7, 34, -32, 52, 59, 92, -13, -96, -81, -85, -39, 31, -39, 4, -66, 86, 92, 98, -65, 54, -91, 88, 40, 5, -80, 90, 73, -44, 74, 92, -21, -46, -20, 2, 48, -94, -79, 94, 79, -34, -26, -29, 93, -53, 79, -43, 73, -59, 49, 93, 23, -86, -68, 13, 97, 14, -18, 18, 94, -64, -37, -100, -90, 91, 85, 18, 41, -99, 71, 67, -54, -35, 91, 43, -42, -22, 76, -12, -53, 73, -34, -65, -5, 73, 42, -99, 66, -87, -67, 71, 30, 25, 32, 96, 65, -86, 83, -35, 40, -48, -42, -61, 29, -14, 48, 36, -93, 39, -85, 40, 35, -30, -80, -49, -34, -10, 73, -62, -97, -54, 92, 41, -82, 91, 39, -51, 42, 41, -78, 35, -51, -47, 65, -24, 56, 22, -6, -67, 71, -100, -13, 74, 31, -62, -21, -20, 55, -18, 7, -11, -55, 26, 11, -89, 11, 49, -31, -23, -74, 74, 51, 93, 93, -56, 95, -56, -19, -96, -66, 37, -83, -89, 78, 96, -73, -58, 27, -32, -84, 16, 88, -52, 86, -98, -9, -89, 35, -92, -43, 80, -59, 76, -30, 28, 9, -33, 69, -74, -41, -55, 47, 19, 29, -23, -57, -88, 7, 74, -81, 70, -43, 80, -89, -34, -90, -41, -57, -45, 50, -9, 71, -88, -92, -39, -98, -96, 89, 90, 31, -100, -84, -99, 64, 39, 31, 59, -17, -6, 93, 31, 10, 80, -38, 92, 23, 37, -66, -83, -58, 40, -29, -57, 94, 8, 28, 59, -21, 8, -33, 37, -63, -16, 13, -84, -41, -77, -58, 36, 89, -54, -12, -25, -33, 75, -31, 87, 82, 26, -30, 94, -49, 69, -15, -8, 55, -8, 13, -60, 5, -86, -28, -76, -23, -45, 99, -27, -49, 95, 37, -18, 71, -60, -42, 29, -79, -28, -66, -53, 51, 79, -67, 4, -81, 57, 20, -48, 35, 50, -76, -43, -91, 45, -69, 91, 31, 96, 31, -15, 34, -22, -59, -53, 98, 3, -3, -61, 98, -41, -8, 29, -9, -81, -1, 88, -27, -40, -100, -61, 66, -7, 47, -92, -53, 17, -89, 41, -80, -46, -71, -52, -70, 0, -24, 70, -63, 37, 65, 39, 83, -16, 2, 26, 31, 58, -84, 1, -51, 55, 36, -2, 30, 55, -71, 81, -73, -54, 94, 29, -35, 3, 44, 46, 51, 51, -7, -52, 55, -59, 27, 19, -69, 34, 4, -88, -51, 21, 46, 21, 37, -84, -68, 68, -61, -73, -27, -98, -78, 27, 40, 35, 26, -98, 45, 0, 66, -29, -95, -84, -20, -68, 15, -16, 71, 79, -19, 18, 11, 62, 90, 63, -13, 80, 90, 43, 88, -21, -44, -65, 59, -59, -100, 43, 44, 85, 66, 71, -86, 25, -64, 50, -43, 96, 49, -54, 93, -61, 21, 11, 93, -61, 76, 47, -93, -98, 33, 30, 83, 55, 3, -95, -85, -21, 10, -6, -63, -17, -64, -7, 5, 17, 89, 74, 18, -37, -69, 63, -9, -41, 54, -18, -43, 58, -2, 5, 48, 26, -5, 89, 67, 26, 21, 12, 72, 21, 96, -4, -36, -44, 5, 99, -72, -59, -66, -19, -26, -12, -96, 91, 33, 36, 14, -32, -98, -86, 47, 16, -76, -36, 99, -95, -76, 3, 25, 56, -59, 76, -71, -60, 28, 76, -19, 41, -24, -9, 55, -78, 57, -79, -93, 54, 79, -95, 11, -61, -12, 87, -57, -91, 1, -7, 41, -47, 27, -99, -91, -42, 84, 38, -100, -22, -32, 66, -44, 6, 51, 44, 31, -25, -56, -28, -51, 84, -80, -76, 73, 95, -52, -66, 73, -69, 59, -72, 24, -49, 58, -22, -20, -35, -84, 91, 54, 15, 94, -97, 27, 35, -79, -82, -90, 25, -84, 16, -8, 86, -81, -30, -45, 38, -62, -77, -52, 89, -45, -5, 26, -93, 31, 97, -91, -51, -20, 22, -58, 54, 28, 81, 94, -1, -87, -47, 30, -81, 67, 40, 27, 68, 78, 25, -56, 95, -16, -24, 87, 92, 3, 85, 85, -89, -92, 51, 63, -42, -10, -48, 96, -93, 84, 15, 49, -73, -76, 0, -96, -68, -82, 99, 54, 61, -97, -9, -69, 54, -25, 12, 68, -38, -47, -91, -52, 93, 63, 88, -93, 53, -56, 11, -57, 47, -38, -46, -93, -98, 19, -91, -66, 37, 0, 90, 63, -24, 64, 80, -80, 95, -71, -2, -39, 5, 3, 86, 55, -77, 11, -19, 19, -33, 5, -10, -51, 22, 97, 73, -52, -93, -61, -18, -66, -63, 61, 56, -9, 83, 67, 27, 89, 28, 5, -38, -98, -79, 11, 29, 61, 74, -40, 30, 43, -33, -63, 99, -47, 24, 33, -86, 79, -67, 97, 52, -78, -32, -78, -35, 47, -78, 59, -52, 95, 3, -30, 17, 87, 14, 77, -91, -8, -60, 46, -12, 48, -4, 0, 54, 76, 57, -89, 17, -61, 7, 63, -27, -38, 59, -4, 74, -6, -2, -89, -63, 71, 65, -33, -82, -30, -53, 9, -44, 92, 75, 86, -65, 89, 52, 83, -97, 3, -97, -29, 42, 4, 16, 43, 9, 25, -51, -69, -22, -34, 42, -32, 41, 89, -20, -77, 22, 20, 1, 54, 38, -39, 66, 93, -100, 73, -15, -46, 66, 45, 44, 83, -58, -52, -52, 39, 12, 41, 31, -60, -81, 3, 11, -75, 38, -12, -66, 75, 74, 76, -7, 53, 16, -60, 4, -58, -93, -24, 70, -66, 42, -41, 54, -3, -95, -61, -24, 95, -75, 14, -25, 92, 14, -14, 21, 5, 14, 92, 4, 82, -34, -80, 53, 94, 76, -56, -56] \n",
      " Dimensions :  [4, 100, 10, 3]\n",
      "Time Taken :  3371.3464357852936\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAZ80lEQVR4nO3de7hcdX3v8fdnZgISsJDLFkMIBDCiyJEoW05A8UmFtnKxocqDKGik2LTVc0SPVvFSQZ9eQPFWq7aoSKyaFtBTqEdAmkORPlZqgAihoFAk3HLZgOFiMOTy7R9r7cl0u3f2hOy1fr8983k9zzx7Zs2aNd+VBfuzf7/f+q2liMDMzAygkboAMzPLh0PBzMzaHApmZtbmUDAzszaHgpmZtTkUzMyszaFgNoKkD0n6SgXbPV/SNyZ6u+W2j5X00x28P1dSSGpV8f3WOxwKlgVJp0u6SdIvJa0vn79Dkir+3oWSHuxcFhF/ERFv34VtXippi6RZu15hdyLixog4tKOG+yQdX9f3W+9wKFhykt4LfA74JPB8YF/gj4BXArslLG2nSdoTeAPwOHBmTd/pv/5twjgULClJewMfB94REVdExJNRuDUizoiITeV6u0u6SNL9ktZJ+htJe5TvLZT0oKT3lq2MNZLO6viOUT9b/gK/GthP0lPlY7+R3TySXiXph5I2SHpA0tt2sEtvADaU+7R4nH1/q6TVkh6V9Kedf92XNX9W0sPl47OSdh+xvx+QtBb4WmeLR9LfAQcA/1Tu0/s7vvaM8t/hEUkf7qjlfEmXS/qGpCcl3S7phZI+WP6bPiDpt3d8NK0XOBQstaOB3YErx1nvAuCFwHzgBcBs4KMd7z8f2LtcfjbwBUnTdvTZiPglcALwcETsVT4e7vxSSQdSBMfngYFyGyt3UOdiYBnw98CLJB052kqSDgO+CJwBzOqofdiHgQXl9x0BHAV8ZMT+TgcOBJZ0bjsi3gLcD7yu3KdPdLz9KuBQ4Djgo5Je3PHe64C/A6YBtwLXUvyOmE0Rcn+7g/22HuFQsNRmAo9ExJbhBR1/lT8t6dXluMIS4D0R8VhEPAn8BXB6x3Y2Ax+PiM0R8T3gKeDQLj+7I28G/jkilpXbfjQiVo62oqQDgN8EvhUR64DlwFvH2O6pwD9FxL9GxDMUAdd5IbIzyv1ZHxFDwMeAt3S8vw04LyI2RcTTXe4LwMci4umI+AnwE4rAGXZjRFxbHovLKULwgojYTBFycyXtsxPfZZOQ+yIttUeBmZJaw8EQEccAlN0hDYpfTlOBmzvGnQU0O7fTGSzARmCvLj+7I3OA/+xy3bcAd3aExjeBT0l6X/mLtdN+wAPDLyJio6RHR7y/uuP16nLZsKGI+FWXdXVa2/F8+N9o2LqO509ThPXWjteU6294Ft9rk4RbCpbavwGbgEU7WOcRil9KL4mIfcrH3hGx1w4+0+1nx7tM8APAIV18DxStgoMlrS37+j9N0RI6cZR11wD7D78ox0dmdLz/MEXX0LADymXDxqvblz+2Z8WhYElFxAaKrpEvSjpV0nMlNSTNB/Ys19kGfBn4jKTnAUiaLel3utj+eJ9dB8woB7xH803geEmnSWpJmlHW9t9IOpoiPI6iGAeYDxwOfIvRu5CuAF4n6RhJuwHnU7Rghi0DPiJpQNJMiu6lnZnjsA44eCfWNwMcCpaBciD0/wDvp/hlto5iUPMDwA/L1T4A3AP8SNITwD9TDJh2Y8zPRsRdFL+A7y3HMTq7aIiI+yn+0n8v8BjFIHNnP/ywxcCVEXF7RKwdflCcanuypOkjtnsH8L8p+urXUIyBrKdoNQH8GbACuA24HbilXNatv6QIlQ2S3rcTn7M+J99kxyw9ScN99fMi4ueJy7E+5paCWSKSXidpajlf4iKKFsF9aauyfudQMEtnEcXg8cPAPOD0cNPdEnP3kZmZtbmlYGZmbZN68trMmTNj7ty5qcswM5tUbr755kciYmC09yZ1KMydO5cVK1akLsPMbFKRtHqs99x9ZGZmbQ4FMzNrqywUJF1SXod9Vcey6ZKuk3R3+XNauXyhpMclrSwfHx17y2ZmVpUqWwqXAq8dsexcYHlEzKO4rPC5He/dGBHzy8fHK6zLzMzGUFkoRMQPKK4V02kRsLR8vhQ4parvNzOznVf3mMK+EbGmfL6W4l68w46W9BNJV0t6yVgbkLRE0gpJK4aGhiot1sys3yQbaC6n8w9Pp74FODAijqC47eE/7uBzF0fEYEQMDgyMepqtmZk9S3XPU1gnaVZErJE0i+JSwUTEE8MrRMT3JH1R0syIeKSKIn669kn+320Pj7ve7lOanPk/D2TvqVOqKMPMLDt1h8JVFNedv6D8eSWApOcD6yIiJB1F0YJ5dMyt7KJ71j/F56+/Z9z1IuBn657kc6e/rKpSzMyyUlkoSFoGLKS4/+6DwHkUYXCZpLMp7jl7Wrn6qcAfS9pCcevESq8WedJLZ3HSS08ad71PXnsXX7j+P/n9Vx7EEXP2qaocM7NsTOqrpA4ODkaVl7l4atMWFn7yeg6euRf/8IcL6Ljxu5nZpCXp5ogYHO09z2jegb12b/Hu41/Iv9/3GNfesS51OWZmlXMojOP0V8xh3vP24oKr7+SZLdtSl2NmVqlJfZXUOrSaDT504os569If88KPXM1E9CAtOGgGy5Ys2PUNmZlNMIdCFxYeOsBn3zife4ee2uVt3fCzIe5a+8T4K5qZJeBQ6IIkTnnZ7AnZ1lObtnLvI7+ckG2ZmU00jynUrNUUW7dN3jO+zKy3ORRq1myILQ4FM8uUQ6FmrYZbCmaWL4dCzZplKEzmSYNm1rscCjVrNYpzWt1aMLMcORRq1mwU/+QeVzCzHDkUauaWgpnlzKFQs2YZCm4pmFmOHAo1azXLUNjq6yiZWX4cCjVruvvIzDLmUKjZFA80m1nGHAo1c0vBzHLmUKhZe0zBoWBmGXIo1Gx7S8EDzWaWH4dCzVo+JdXMMuZQqFl7RvNWh4KZ5cehUDPPaDaznDkUauYZzWaWM4dCzdpjCp7RbGYZcijUzPMUzCxnDoWatZqe0Wxm+XIo1MwDzWaWM4dCzTzQbGY5cyjUbPgyF57RbGY5cijUzDOazSxnDoWaDc9o9piCmeXIoVCz7fMUHApmlh+HQs08T8HMcuZQqNlwS2GzB5rNLEOVhYKkSyStl7SqY9l0SddJurv8Oa1cLkl/JekeSbdJenlVdaXmloKZ5azKlsKlwGtHLDsXWB4R84Dl5WuAE4B55WMJ8KUK60qq5Utnm1nGKguFiPgB8NiIxYuApeXzpcApHcu/HoUfAftImlVVbSltn6fgUDCz/NQ9prBvRKwpn68F9i2fzwYe6FjvwXLZr5G0RNIKSSuGhoaqq7QintFsZjlLNtAcEQHs9G/GiLg4IgYjYnBgYKCCyqrV8j2azSxjdYfCuuFuofLn+nL5Q8CcjvX2L5f1HLcUzCxndYfCVcDi8vli4MqO5W8tz0JaADze0c3UUyTRbMhjCmaWpVZVG5a0DFgIzJT0IHAecAFwmaSzgdXAaeXq3wNOBO4BNgJnVVVXDpoNuaVgZlmqLBQi4k1jvHXcKOsG8M6qaslNyy0FM8uUZzQn0GzI8xTMLEsOhQRaDbHFZx+ZWYYcCgk0Gw2PKZhZlhwKCUxpiq3uPjKzDDkUEvDZR2aWK4dCAsXZRx5TMLP8OBQScEvBzHLlUEig1Wh4noKZZcmhkIBbCmaWK4dCAq2mZzSbWZ4cCgm4pWBmuXIoJNBqiC1bffaRmeXHoZCAWwpmliuHQgI++8jMcuVQSKDVdEvBzPLkUEjAM5rNLFcOhQR8PwUzy5VDIQGPKZhZrhwKCTR9O04zy5RDIYGWT0k1s0w5FBJwS8HMcuVQSKA4JdVnH5lZfhwKCfjsIzPLlUMhgVaj4TEFM8uSQyGBlscUzCxTDoUEmh5TMLNMORQScEvBzHLlUEig6TEFM8uUQyGBVkNEwDYHg5llxqGQQLMhALcWzCw7DoUEWmUoeFzBzHLjUEhge0vBZyCZWV4cCgkMtxQ8q9nMcpMkFCSdI2mVpDskvbtcdr6khyStLB8npqitDs1m8c/uMQUzy02r7i+UdDjwB8BRwDPANZK+W779mYi4qO6a6uYxBTPLVe2hALwYuCkiNgJIugF4fYI6kml5TMHMMpWi+2gVcKykGZKmAicCc8r3/pek2yRdImnaaB+WtETSCkkrhoaG6qp5QrWabimYWZ5qD4WIuBO4EPg+cA2wEtgKfAk4BJgPrAE+NcbnL46IwYgYHBgYqKPkCddseEzBzPK006EgaZqkl+7Kl0bEVyPiyIh4NfAL4GcRsS4itkbENuDLFGMOPcljCmaWq65CQdK/SPoNSdOBW4AvS/r0s/1SSc8rfx5AMZ7wLUmzOlb5PYpupp7U9CmpZpapbgea946IJyS9Hfh6RJwn6bZd+N5vS5oBbAbeGREbJH1e0nwggPuAP9yF7WfNLQUzy1W3odAq/5I/Dfjwrn5pRBw7yrK37Op2JwvPaDazXHU7pvBx4Frgnoj4saSDgburK6u3tcqBZrcUzCw3XbUUIuJy4PKO1/cCb6iqqF433FLY7DEFM8tMtwPNnygHmqdIWi5pSNKZVRfXqzxPwcxy1W330W9HxBPAyRSDwC8A/qSqonqdZzSbWa66DYXhbqaTgMsj4vGK6ukLHlMws1x1e/bRdyXdBTwN/LGkAeBX1ZXV23znNTPLVVcthYg4FzgGGIyIzcBGYFGVhfUyjymYWa66HWieCryD4vpEAPsBg1UV1evcUjCzXHU7pvA1insfHFO+fgj4s0oq6gPbZzR7oNnM8tJtKBwSEZ+guCwF5b0QVFlVPc7XPjKzXHUbCs9I2oPiukRIOgTYVFlVPc5nH5lZrro9++g8insfzJH0TeCVwNuqKqrXtWc0OxTMLDPdXubiOkm3AAsouo3OiYhHKq2sh7XHFLZ6TMHM8rIz92h+DsUNcVrAYZKIiB9UU1ZvGz4l1WcfmVluugoFSRcCbwTuAIb/vA3AofAseEzBzHLVbUvhFODQiPDg8gTwPAUzy1W3Zx/dC0ypspB+4juvmVmuum0pbARWSlpOx6moEfGuSqrqcY2GkNxSMLP8dBsKV5WPTv6NtgtaDXlGs5llp9tQ2CciPte5QNI5FdTTN5oNuaVgZtnpdkxh8SjL3jaBdfSdVqPBVl/mwswys8OWgqQ3AW8GDpLU2X30XOCxKgvrdW4pmFmOxus++iGwBpgJfKpj+ZPAbVUV1Q9aDfl2nGaWnR2GQkSsBlYDR9dTTv9oNuRTUs0sO+N1H/1rRLxK0pP897ONBERE/Eal1fWwKc2GL51tZtkZr/voDICIeG4NtfQVtxTMLEfjnX30f4efSPp2xbX0lZYHms0sQ+OFQufd1Q6uspB+45aCmeVovFCIMZ7bLmr67CMzy9B4YwpHSHqCosWwR/kcPNC8y1pNtxTMLD/jnZLarKuQftNsNDymYGbZ6fYyFzbBWh5TMLMMORQSaTbEZt+j2cwy41BIxC0FM8tRklCQdI6kVZLukPTuctl0SddJurv8OS1FbXVpNT2mYGb5qT0UJB0O/AFwFHAEcLKkFwDnAssjYh6wvHzds9xSMLMcpWgpvBi4KSI2RsQW4Abg9cAiYGm5zlLglAS11abZkK99ZGbZSREKq4BjJc2QNBU4EZgD7BsRa8p11gL7jvZhSUskrZC0YmhoqJ6KK+CWgpnlqPZQiIg7gQuB7wPXACuBrSPWCcaYQR0RF0fEYEQMDgwMVFxtdTyj2cxylGSgOSK+GhFHRsSrgV8APwPWSZoFUP5cn6K2urilYGY5SnX20fPKnwdQjCd8C7iK7feCXgxcmaK2unhGs5nlaLxrH1Xl25JmAJuBd0bEBkkXAJdJOpvibm+nJaqtFm4pmFmOkoRCRBw7yrJHgeMSlJNEs+n7KZhZfjyjOZFWQ2zxZS7MLDMOhUSavvOamWXIoZDIlGbDYwpmlh2HQiJuKZhZjhwKifjsIzPLkUMhkWYZCsXkbTOzPDgUEmk1BODWgpllxaGQSLNR/NN7XMHMcuJQSMQtBTPLkUMhkWYZCm4pmFlOHAqJtJplKHhWs5llxKGQSNPdR2aWIYdCIlM80GxmGXIoJOKWgpnlyKGQSHtMwaFgZhlxKCSyvaXggWYzy4dDIZGWT0k1sww5FBJpz2je6lAws3w4FBLxjGYzy5FDIRHPaDazHDkUEmmPKXhGs5llxKGQiOcpmFmOHAqJeJ6CmeXIoZBIqzz7yC0FM8uJQyERDzSbWY4cCokMdx95RrOZ5cShkIhnNJtZjhwKiTQ9pmBmGXIoJLJ9noJDwczy4VBIxPMUzCxHDoVEPKZgZjlyKCSy/ZRUn31kZvlwKCTS8qWzzSxDDoVEts9TcCiYWT6ShIKk90i6Q9IqScskPUfSpZJ+Lmll+Zifora6eEazmeWoVfcXSpoNvAs4LCKelnQZcHr59p9ExBV115RCy/doNrMMpeo+agF7SGoBU4GHE9WRjFsKZpaj2kMhIh4CLgLuB9YAj0fE98u3/1zSbZI+I2n30T4vaYmkFZJWDA0N1VT1xJNEsyGPKZhZVmoPBUnTgEXAQcB+wJ6SzgQ+CLwIeAUwHfjAaJ+PiIsjYjAiBgcGBmqquhrNhtxSMLOspOg+Oh74eUQMRcRm4DvAMRGxJgqbgK8BRyWorVYttxTMLDMpQuF+YIGkqZIEHAfcKWkWQLnsFGBVgtpq1WzI8xTMLCu1n30UETdJugK4BdgC3ApcDFwtaQAQsBL4o7prq1urIc9oNrOs1B4KABFxHnDeiMWvSVFLSs1Gw2MKZpYVz2hOqNUQW919ZGYZcSgk1Gr67CMzy4tDIaHi7COPKZhZPhwKCXmegpnlxqGQUKvR8DwFM8uKQyEhtxTMLDcOhYRaTc9oNrO8OBQSckvBzHLjUEjIZx+ZWW4cCgk1G2KzJ6+ZWUYcCgn57CMzy41DISHPaDaz3DgUEvKYgpnlxqGQkO+nYGa5cSgk5DEFM8uNQyGhpm/HaWaZcSgk1PLkNTPLjEMhIbcUzCw3DoWEilNSffaRmeXDoZCQzz4ys9w4FBJqNRoeUzCzrDgUEmp5TMHMMuNQSKjpMQUzy0wrdQH9rNUQv9q8jd/69A2pSzGzSeaNr5jD2489eMK361BI6ITDZ7H60Y1sC3chmdnOmbnX7pVs16GQ0OGz9+av3/zy1GWYmbV5TMHMzNocCmZm1uZQMDOzNoeCmZm1ORTMzKzNoWBmZm0OBTMza3MomJlZm2ISz6aVNASsfpYfnwk8MoHlTBb9uN/9uM/Qn/vdj/sMO7/fB0bEwGhvTOpQ2BWSVkTEYOo66taP+92P+wz9ud/9uM8wsfvt7iMzM2tzKJiZWVs/h8LFqQtIpB/3ux/3Gfpzv/txn2EC97tvxxTMzOzX9XNLwczMRnAomJlZW1+GgqTXSvqppHsknZu6nipImiPpekn/IekOSeeUy6dLuk7S3eXPaalrrYKkpqRbJX23fH2QpJvKY/4PknZLXeNEkrSPpCsk3SXpTklH98OxlvSe8r/vVZKWSXpOLx5rSZdIWi9pVceyUY+vCn9V7v9tknbqTl59FwqSmsAXgBOAw4A3STosbVWV2AK8NyIOAxYA7yz381xgeUTMA5aXr3vROcCdHa8vBD4TES8AfgGcnaSq6nwOuCYiXgQcQbHvPX2sJc0G3gUMRsThQBM4nd481pcCrx2xbKzjewIwr3wsAb60M1/Ud6EAHAXcExH3RsQzwN8DixLXNOEiYk1E3FI+f5Lil8Rsin1dWq62FDglSYEVkrQ/cBLwlfK1gNcAV5Sr9NR+S9obeDXwVYCIeCYiNtAHx5rilsJ7SGoBU4E19OCxjogfAI+NWDzW8V0EfD0KPwL2kTSr2+/qx1CYDTzQ8frBclnPkjQXeBlwE7BvRKwp31oL7Juqrgp9Fng/sK18PQPYEBFbyte9dswPAoaAr5VdZl+RtCc9fqwj4iHgIuB+ijB4HLiZ3j7WncY6vrv0O64fQ6GvSNoL+Dbw7oh4ovO9KM5H7qlzkiWdDKyPiJtT11KjFvBy4EsR8TLgl4zoKurRYz2N4q/ig4D9gD359S6WvjCRx7cfQ+EhYE7H6/3LZT1H0hSKQPhmRHynXLxuuClZ/lyfqr6KvBL4XUn3UXQNvoaiv32fsosBeu+YPwg8GBE3la+voAiJXj/WxwM/j4ihiNgMfIfi+Pfyse401vHdpd9x/RgKPwbmlWco7EYxMHVV4pomXNmP/lXgzoj4dMdbVwGLy+eLgSvrrq1KEfHBiNg/IuZSHNv/HxFnANcDp5ar9dR+R8Ra4AFJh5aLjgP+gx4/1hTdRgskTS3/ex/e75491iOMdXyvAt5anoW0AHi8o5tpXH05o1nSiRT9zk3gkoj487QVTTxJrwJuBG5ne9/6hyjGFS4DDqC47PhpETFyAKsnSFoIvC8iTpZ0MEXLYTpwK3BmRGxKWN6EkjSfYmB9N+Be4CyKP/p6+lhL+hjwRoqz7W4F3k7Rf95Tx1rSMmAhxSWy1wHnAf/IKMe3DMi/puhK2wicFREruv6ufgwFMzMbXT92H5mZ2RgcCmZm1uZQMDOzNoeCmZm1ORTMzKzNoWDWBUkzJK0sH2slPVQ+f0rSF1PXZzZRfEqq2U6SdD7wVERclLoWs4nmloLZLpC0sOOeDedLWirpRkmrJb1e0ick3S7pmvKyI0g6UtINkm6WdO3OXMHSrGoOBbOJdQjF9ZZ+F/gGcH1E/A/gaeCkMhg+D5waEUcClwA9N6PeJq/W+KuY2U64OiI2S7qd4jIq15TLbwfmAocChwPXFVcjoElx2WezLDgUzCbWJoCI2CZpc2wftNtG8f+bgDsi4uhUBZrtiLuPzOr1U2BA0tFQXN5c0ksS12TW5lAwq1F5C9hTgQsl/QRYCRyTtCizDj4l1czM2txSMDOzNoeCmZm1ORTMzKzNoWBmZm0OBTMza3MomJlZm0PBzMza/gtFd6zVDBtuCQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "i = InputData(fileName=\"../ANN/iris\")\n",
    "input_val, output_val = i.main()\n",
    "end_time = time.time()\n",
    "print(\"Time for inputting data : \", end_time - start_time)\n",
    "        \n",
    "print(\"============ Calling GA to get best weights ===============\")\n",
    "\n",
    "n_iterations = 100\n",
    "e_rate = 0.1\n",
    "\n",
    "start_time = time.time()\n",
    "a = gaAnn(initialPopSize=100, m = 10, dimensions = [100,10], bestCount = 30, input_values=input_val , output_values_expected=output_val, iterations = n_iterations, elicitation_rate = e_rate)\n",
    "\n",
    "fit, b, weights, dim = a.main()\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Time taken : \", end_time - start_time)\n",
    "\n",
    "print(\"\\n Fitness : \", fit, \"\\n Best Weights : \", weights, \"\\n Dimensions : \", dim)\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "x=b[:]\n",
    "z=[i for i in range(0,100)]\n",
    "plt.plot(z,x)\n",
    "\n",
    "plt.title(\"Genetic Algorithm\")\n",
    "plt.ylabel(\"Fitness\")\n",
    "plt.xlabel(\"Time\")\n",
    "end_time = time.time()\n",
    "print(\"Time Taken : \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55db955e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============= MLP Program Begins ============\n",
      "Training\n",
      "\n",
      " Actual / Expected [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      " Predictions [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 2 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 2 0 1 1 0 1 2 1 1 1 1 1 2 1 2 2 2 1 2 1 2 2 1 2 2 1 2 2 2 0 2 1 2 2 1 1 1\n",
      " 2 1 1 2 1 2 2 2 2]\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "[[39  1  0]\n",
      " [11 27  2]\n",
      " [ 1 17 22]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.76      0.97      0.86        40\n",
      "     class 1       0.60      0.68      0.64        40\n",
      "     class 2       0.92      0.55      0.69        40\n",
      "\n",
      "    accuracy                           0.73       120\n",
      "   macro avg       0.76      0.73      0.73       120\n",
      "weighted avg       0.76      0.73      0.73       120\n",
      "\n",
      "Time taken =  0.010970830917358398\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n============= MLP Program Begins ============\")\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Training\")\n",
    "m = MultiLayerPerceptron(fileName=\"../ANN/iris_train\", dimensions=dim, all_weights=weights)\n",
    "m.main()\n",
    "end_time = time.time()\n",
    "print(\"Time taken = \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5037ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n",
      "\n",
      " Actual / Expected [0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      " Predictions [0 1 0 0 0 0 0 0 0 0 2 1 1 1 1 0 0 1 0 1 2 0 1 1 2 2 1 2 2 2]\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "[[9 1 0]\n",
      " [3 6 1]\n",
      " [1 3 6]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.69      0.90      0.78        10\n",
      "     class 1       0.60      0.60      0.60        10\n",
      "     class 2       0.86      0.60      0.71        10\n",
      "\n",
      "    accuracy                           0.70        30\n",
      "   macro avg       0.72      0.70      0.70        30\n",
      "weighted avg       0.72      0.70      0.70        30\n",
      "\n",
      "Time taken =  0.00698089599609375\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(\"Testing\")\n",
    "m = MultiLayerPerceptron(fileName=\"../ANN/iris_test\", dimensions=dim, all_weights=weights)\n",
    "m.main()\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Time taken = \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6609313e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
