{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e62cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fedefee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn import preprocessing\n",
    "from scipy.special import expit\n",
    "\n",
    "from numpy.random import default_rng\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import time\n",
    "\n",
    "from psoAnn_thread_V_I import *\n",
    "\n",
    "\n",
    "\n",
    "class MultiLayerPerceptron():\n",
    "    # ================== Activation Functions ================ #\n",
    "\n",
    "    # accepts a vector or list and returns a list after performing corresponding function on all elements\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(vectorSig):\n",
    "        \"\"\"returns 1/(1+exp(-x)), where the output values lies between zero and one\"\"\"\n",
    "        sig = expit(vectorSig)\n",
    "        return sig\n",
    "\n",
    "    @staticmethod\n",
    "    def binaryStep(x):\n",
    "        \"\"\" It returns '0' is the input is less then zero otherwise it returns one \"\"\"\n",
    "        return np.heaviside(x, 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def linear(x):\n",
    "        \"\"\" y = f(x) It returns the input as it is\"\"\"\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        \"\"\" It returns the value (1-exp(-2x))/(1+exp(-2x)) and the value returned will be lies in between -1 to 1\"\"\"\n",
    "        return np.tanh(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(x):  # Rectified Linear Unit\n",
    "        \"\"\" It returns zero if the input is less than zero otherwise it returns the given input\"\"\"\n",
    "        x1 = []\n",
    "        for i in x:\n",
    "            if i < 0:\n",
    "                x1.append(0)\n",
    "            else:\n",
    "                x1.append(i)\n",
    "\n",
    "        return x1\n",
    "\n",
    "    @staticmethod\n",
    "    def leakyRelu(x):\n",
    "        \"\"\" It returns zero if the input is less than zero otherwise it returns the given input\"\"\"\n",
    "        x1 = []\n",
    "        for i in x:\n",
    "            if i < 0:\n",
    "                x1.append((0.01 * i))\n",
    "            else:\n",
    "                x1.append(i)\n",
    "\n",
    "        return x1\n",
    "\n",
    "    @staticmethod\n",
    "    def parametricRelu(self, a, x):\n",
    "        \"\"\" It returns zero if the input is less than zero otherwise it returns the given input\"\"\"\n",
    "        x1 = []\n",
    "        for i in x:\n",
    "            if i < 0:\n",
    "                x1.append((a * i))\n",
    "            else:\n",
    "                x1.append(i)\n",
    "\n",
    "        return x1\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(self, x):\n",
    "        \"\"\" Compute softmax values for each sets of scores in x\"\"\"\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "    # ============ Activation Functions Part Ends ============= #\n",
    "\n",
    "    # ================= Distance Calculation ================== #\n",
    "\n",
    "    @staticmethod\n",
    "    def chebishev(self, cord1, cord2, exponent_h):\n",
    "        dist = 0.0\n",
    "        if ((type(cord1) == int and type(cord2) == int) or ((type(cord1) == float and type(cord2) == float))):\n",
    "            dist = math.pow((cord1 - cord2), exponent_h)\n",
    "        else:\n",
    "            for i, j in zip(cord1, cord2):\n",
    "                dist += math.pow((i - j), exponent_h)\n",
    "        dist = math.pow(dist, (1.0 / exponent_h))\n",
    "        return dist\n",
    "\n",
    "    @staticmethod\n",
    "    def minimum_distance(self, cord1, cord2):\n",
    "        # min(|x1-y1|, |x2-y2|, |x3-y3|, ...)\n",
    "        dist = float('inf')\n",
    "        if ((type(cord1) == int and type(cord2) == int) or ((type(cord1) == float and type(cord2) == float))):\n",
    "            dist = math.fabs(cord1 - cord2)\n",
    "        else:\n",
    "            for i, j in zip(cord1, cord2):\n",
    "                temp_dist = math.fabs(i - j)\n",
    "                if (temp_dist < dist):\n",
    "                    dist = temp_dist\n",
    "        return dist\n",
    "\n",
    "    @staticmethod\n",
    "    def maximum_distance(self, cord1, cord2):\n",
    "        # max(|x1-y1|, |x2-y2|, |x3-y3|, ...)\n",
    "        dist = float('-inf')\n",
    "        if ((type(cord1) == int and type(cord2) == int) or ((type(cord1) == float and type(cord2) == float))):\n",
    "            dist = math.fabs(cord1 - cord2)\n",
    "        else:\n",
    "            for i, j in zip(cord1, cord2):\n",
    "                temp_dist = math.fabs(i - j)\n",
    "                if (temp_dist > dist):\n",
    "                    dist = temp_dist\n",
    "        return dist\n",
    "\n",
    "    @staticmethod\n",
    "    def manhattan(self, cord1, cord2):\n",
    "        # |x1-y1| + |x2-y2| + |x3-y3| + ...\n",
    "        dist = 0.0\n",
    "        if ((type(cord1) == int and type(cord2) == int) or ((type(cord1) == float and type(cord2) == float))):\n",
    "            dist = math.fabs(cord1 - cord2)\n",
    "        else:\n",
    "            for i, j in zip(cord1, cord2):\n",
    "                dist += math.fabs(i - j)\n",
    "        return dist\n",
    "\n",
    "    @staticmethod\n",
    "    def eucledian(self, cord1, cord2):\n",
    "        dist = 0.0\n",
    "        if ((type(cord1) == int and type(cord2) == int) or ((type(cord1) == float and type(cord2) == float))):\n",
    "            dist = math.pow((cord1 - cord2), 2)\n",
    "        else:\n",
    "            for i, j in zip(cord1, cord2):\n",
    "                dist += math.pow((i - j), 2)\n",
    "        return math.pow(dist, 0.5)\n",
    "\n",
    "    # =========== Distance Calculation Ends ============== #\n",
    "\n",
    "    def __init__(self, dimensions=(8, 5), all_weights=(0.1, 0.2), fileName=\"iris\"):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dimensions : dimension of the neural network\n",
    "            all_weights : the optimal weights we get from the bio-algoANN models\n",
    "        \"\"\"\n",
    "\n",
    "        self.allPop_Weights = []\n",
    "        self.allPopl_Chromosomes = []\n",
    "        self.allPop_ReceivedOut = []\n",
    "        self.allPop_ErrorVal = []\n",
    "\n",
    "        self.all_weights = all_weights\n",
    "\n",
    "        self.fitness = []\n",
    "\n",
    "        # ================== Input dataset and corresponding output ========================= #\n",
    "\n",
    "        self.fileName = fileName\n",
    "        self.fileName += \".csv\"\n",
    "        data = pd.read_csv(self.fileName)\n",
    "\n",
    "        classes = []\n",
    "        output_values_expected = []\n",
    "        input_values = []\n",
    "\n",
    "        # ~~~~ encoding ~~~~#\n",
    "\n",
    "        # labelencoder = LabelEncoder()\n",
    "        # data[data.columns[-1]] = labelencoder.fit_transform(data[data.columns[-1]])\n",
    "\n",
    "        # one hot encoding - for multi-column\n",
    "        # enc = OneHotEncoder(handle_unknown='ignore')\n",
    "        # combinedData = np.vstack((data[data.columns[-2]], data[data.columns[-1]])).T\n",
    "        # print(combinedData)\n",
    "        # y = enc.fit_transform(combinedData).toarray()\n",
    "        # y = OneHotEncoder().fit_transform(combinedData).toarray()\n",
    "\n",
    "        #\n",
    "        y = LabelBinarizer().fit_transform(data[data.columns[-1]])\n",
    "        # print(y)\n",
    "\n",
    "        # ~~~~ encoding ends~~~~#\n",
    "\n",
    "        for j in range(len(data)):\n",
    "            output_values_expected.append(y[j])\n",
    "\n",
    "        # print(output_values_expected)\n",
    "\n",
    "        input_values = []\n",
    "        for j in range(len(data)):\n",
    "            b = []\n",
    "            for i in range(1, len(data.columns) - 1):\n",
    "                b.append(data[data.columns[i]][j])\n",
    "            input_values.append(b)\n",
    "\n",
    "        self.X = input_values[:]\n",
    "        self.Y = output_values_expected[:]\n",
    "\n",
    "        # input and output\n",
    "        self.X = input_values[:]\n",
    "        self.Y = output_values_expected[:]\n",
    "\n",
    "        self.dimension = dimensions\n",
    "        # print(self.dimension)\n",
    "\n",
    "        # ================ Finding Initial Weights ================ #\n",
    "\n",
    "        self.pop = []  # weights\n",
    "        reshaped_all_weights = []\n",
    "        start = 0\n",
    "        for i in range(len(self.dimension) - 1):\n",
    "            end = start + self.dimension[i + 1] * self.dimension[i]\n",
    "            temp_arr = self.all_weights[start:end]\n",
    "            w = np.reshape(temp_arr[:], (self.dimension[i + 1], self.dimension[i]))\n",
    "            reshaped_all_weights.append(w)\n",
    "            start = end\n",
    "        self.pop.append(reshaped_all_weights)\n",
    "\n",
    "        self.init_pop = self.all_weights\n",
    "\n",
    "    # ================ Initial Weights Part Ends ================ #\n",
    "\n",
    "\n",
    "    def Predict(self, chromo):\n",
    "        # X, Y and pop are used\n",
    "        self.fitness = []\n",
    "        total_error = 0\n",
    "        m_arr = []\n",
    "        k1 = 0\n",
    "        for i in range(len(self.dimension) - 1):\n",
    "            p = self.dimension[i]\n",
    "            q = self.dimension[i + 1]\n",
    "            k2 = k1 + p * q\n",
    "            m_temp = chromo[k1:k2]\n",
    "            m_arr.append(np.reshape(m_temp, (p, q)))\n",
    "            k1 = k2\n",
    "\n",
    "        y_predicted = []\n",
    "        for x, y in zip(self.X, self.Y):\n",
    "\n",
    "            yo = x\n",
    "\n",
    "            for mCount in range(len(m_arr)):\n",
    "                yo = np.dot(yo, m_arr[mCount])\n",
    "                yo = self.sigmoid(yo)\n",
    "            \n",
    "            # converting to sklearn acceptable form\n",
    "            max_yo = max(yo)\n",
    "            for y_vals in range(len(yo)):\n",
    "                if(yo[y_vals] == max_yo):\n",
    "                    yo[y_vals] = 1\n",
    "                else:\n",
    "                    yo[y_vals] = 0\n",
    "            y_predicted.append(yo)\n",
    "        return (y_predicted, self.Y)\n",
    "\n",
    "    def main(self):\n",
    "        Y_PREDICT, Y_ACTUAL = self.Predict(self.init_pop)\n",
    "        Y_PREDICT = np.array(Y_PREDICT)\n",
    "        Y_ACTUAL = np.array(Y_ACTUAL)\n",
    "        \n",
    "        n_classes = 3\n",
    "        \n",
    "        label_binarizer = LabelBinarizer()\n",
    "        label_binarizer.fit(range(n_classes))\n",
    "        Y_PREDICT = label_binarizer.inverse_transform(np.array(Y_PREDICT))\n",
    "        Y_ACTUAL = label_binarizer.inverse_transform(np.array(Y_ACTUAL))\n",
    "        \n",
    "        # find error\n",
    "        \n",
    "        print(\"\\n Actual / Expected\", Y_ACTUAL)\n",
    "        print(\"\\n Predictions\", Y_PREDICT)\n",
    "        print(\"\\n\\nConfusion Matrix\")\n",
    "        print(confusion_matrix(Y_ACTUAL, Y_PREDICT))\n",
    "        \n",
    "        print(\"\\n\\nClassification Report\")\n",
    "        target_names = ['class 0', 'class 1', 'class 2']\n",
    "        print(classification_report(Y_ACTUAL, Y_PREDICT, target_names=target_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6cab7fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inputting data :  0.007979631423950195\n",
      "============ Calling PSO to get best weights ===============\n",
      "--------------GENERATION 0-----------\n",
      "-204.0015253341516\n",
      "--------------GENERATION 1-----------\n",
      "-206.99975601378623\n",
      "--------------GENERATION 2-----------\n",
      "-216.72164293025472\n",
      "--------------GENERATION 3-----------\n",
      "-237.05636408758627\n",
      "--------------GENERATION 4-----------\n",
      "-250.69618319571026\n",
      "--------------GENERATION 5-----------\n",
      "-286.0839611996855\n",
      "--------------GENERATION 6-----------\n",
      "-250.00117453932418\n",
      "--------------GENERATION 7-----------\n",
      "-232.98776219095373\n",
      "--------------GENERATION 8-----------\n",
      "-133.69416971239522\n",
      "--------------GENERATION 9-----------\n",
      "-199.79388691440235\n",
      "--------------GENERATION 10-----------\n",
      "-199.99999988994423\n",
      "--------------GENERATION 11-----------\n",
      "-199.99999999994492\n",
      "--------------GENERATION 12-----------\n",
      "-199.99999999999\n",
      "--------------GENERATION 13-----------\n",
      "-199.99999999942548\n",
      "--------------GENERATION 14-----------\n",
      "-199.99999925748708\n",
      "--------------GENERATION 15-----------\n",
      "-199.9933399910117\n",
      "--------------GENERATION 16-----------\n",
      "-152.32927278144388\n",
      "--------------GENERATION 17-----------\n",
      "-106.99892252015718\n",
      "--------------GENERATION 18-----------\n",
      "-107.23224059846777\n",
      "--------------GENERATION 19-----------\n",
      "-85.99999975140872\n",
      "--------------GENERATION 20-----------\n",
      "-64.99773160104449\n",
      "--------------GENERATION 21-----------\n",
      "-81.9879095549059\n",
      "--------------GENERATION 22-----------\n",
      "-67.96068153302764\n",
      "--------------GENERATION 23-----------\n",
      "-56.98595644520526\n",
      "--------------GENERATION 24-----------\n",
      "-56.99381185672521\n",
      "--------------GENERATION 25-----------\n",
      "-99.98907847190911\n",
      "--------------GENERATION 26-----------\n",
      "-77.99180118171712\n",
      "--------------GENERATION 27-----------\n",
      "-65.98850239519155\n",
      "--------------GENERATION 28-----------\n",
      "-43.40508426056502\n",
      "--------------GENERATION 29-----------\n",
      "-53.98676810690777\n",
      "--------------GENERATION 30-----------\n",
      "-52.00379018286976\n",
      "--------------GENERATION 31-----------\n",
      "-54.270646189305154\n",
      "--------------GENERATION 32-----------\n",
      "-58.78350142623266\n",
      "--------------GENERATION 33-----------\n",
      "-63.19565204518672\n",
      "--------------GENERATION 34-----------\n",
      "-72.7767946257518\n",
      "--------------GENERATION 35-----------\n",
      "-79.78495828085184\n",
      "--------------GENERATION 36-----------\n",
      "-80.2911018751364\n",
      "--------------GENERATION 37-----------\n",
      "-80.06515841764808\n",
      "--------------GENERATION 38-----------\n",
      "-82.00370322322783\n",
      "--------------GENERATION 39-----------\n",
      "-82.00370679476217\n",
      "--------------GENERATION 40-----------\n",
      "-82.00370804113484\n",
      "--------------GENERATION 41-----------\n",
      "-82.00773276861133\n",
      "--------------GENERATION 42-----------\n",
      "-82.01007328988021\n",
      "--------------GENERATION 43-----------\n",
      "-82.01153706848511\n",
      "--------------GENERATION 44-----------\n",
      "-82.01170149569224\n",
      "--------------GENERATION 45-----------\n",
      "-82.0176888743228\n",
      "--------------GENERATION 46-----------\n",
      "-80.04748976678681\n",
      "--------------GENERATION 47-----------\n",
      "-80.05939966875599\n",
      "--------------GENERATION 48-----------\n",
      "-80.05938258259796\n",
      "--------------GENERATION 49-----------\n",
      "-80.0533381450472\n",
      "--------------GENERATION 50-----------\n",
      "-80.05196048768059\n",
      "--------------GENERATION 51-----------\n",
      "-80.05196048768059\n",
      "--------------GENERATION 52-----------\n",
      "-58.1349361964498\n",
      "--------------GENERATION 53-----------\n",
      "-51.831044760574734\n",
      "--------------GENERATION 54-----------\n",
      "-50.62332243933943\n",
      "--------------GENERATION 55-----------\n",
      "-48.035746715388576\n",
      "--------------GENERATION 56-----------\n",
      "-44.035658895786234\n",
      "--------------GENERATION 57-----------\n",
      "-44.035657249702254\n",
      "--------------GENERATION 58-----------\n",
      "-44.035659349975695\n",
      "--------------GENERATION 59-----------\n",
      "-42.03568746647933\n",
      "--------------GENERATION 60-----------\n",
      "-42.03586914104771\n",
      "--------------GENERATION 61-----------\n",
      "-42.022339785127045\n",
      "--------------GENERATION 62-----------\n",
      "-42.0771880096514\n",
      "--------------GENERATION 63-----------\n",
      "-42.3303743716733\n",
      "--------------GENERATION 64-----------\n",
      "-43.08669090729898\n",
      "--------------GENERATION 65-----------\n",
      "-46.23407649309429\n",
      "--------------GENERATION 66-----------\n",
      "-47.166274542776\n",
      "--------------GENERATION 67-----------\n",
      "-47.166274542776\n",
      "--------------GENERATION 68-----------\n",
      "-47.166274542776\n",
      "--------------GENERATION 69-----------\n",
      "-47.16627454277637\n",
      "--------------GENERATION 70-----------\n",
      "-47.16627454277638\n",
      "--------------GENERATION 71-----------\n",
      "-47.16627454277638\n",
      "--------------GENERATION 72-----------\n",
      "-47.16627454277638\n",
      "--------------GENERATION 73-----------\n",
      "-47.16627454277638\n",
      "--------------GENERATION 74-----------\n",
      "-47.16627454277638\n",
      "--------------GENERATION 75-----------\n",
      "-47.16627454277638\n",
      "--------------GENERATION 76-----------\n",
      "-47.16627454277638\n",
      "--------------GENERATION 77-----------\n",
      "-47.16627454277638\n",
      "--------------GENERATION 78-----------\n",
      "-47.16627454277638\n",
      "--------------GENERATION 79-----------\n",
      "-47.16627454277638\n",
      "--------------GENERATION 80-----------\n",
      "-47.16627454277638\n",
      "--------------GENERATION 81-----------\n",
      "-47.16627454277638\n",
      "--------------GENERATION 82-----------\n",
      "-47.16627454277638\n",
      "--------------GENERATION 83-----------\n",
      "-47.16627454277638\n",
      "--------------GENERATION 84-----------\n",
      "-47.16627454277638\n",
      "--------------GENERATION 85-----------\n",
      "-47.16627454277638\n",
      "--------------GENERATION 86-----------\n",
      "-47.16627454277638\n",
      "--------------GENERATION 87-----------\n",
      "-47.16627454277638\n",
      "--------------GENERATION 88-----------\n",
      "-47.16627454277638\n",
      "--------------GENERATION 89-----------\n",
      "-47.16627454277638\n",
      "--------------GENERATION 90-----------\n",
      "-47.16627454277638\n",
      "--------------GENERATION 91-----------\n",
      "-47.16627454277638\n",
      "--------------GENERATION 92-----------\n",
      "-47.16627454277638\n",
      "--------------GENERATION 93-----------\n",
      "-47.16627454277638\n",
      "--------------GENERATION 94-----------\n",
      "-47.16627454277638\n",
      "--------------GENERATION 95-----------\n",
      "-45.16627642423633\n",
      "--------------GENERATION 96-----------\n",
      "-45.17223513328466\n",
      "--------------GENERATION 97-----------\n",
      "-45.17819364879475\n",
      "--------------GENERATION 98-----------\n",
      "-41.18415614156734\n",
      "--------------GENERATION 99-----------\n",
      "-37.18416180238296\n",
      "Global :  19.994230689076016\n",
      "Time taken :  35.80925989151001\n",
      "\n",
      " Fitness :  [-19.994230689076016] \n",
      " Best Weights :  [  60.35878972   41.55265012 -105.46304781 ...  -79.59531984  -37.80517996\n",
      "  107.52434055] \n",
      " Dimensions :  [4, 100, 10, 3]\n",
      "Time Taken :  35.83618712425232\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAcm0lEQVR4nO3deZhcdZ3v8fent3Q1nXSnSacTSEgHElBEFumwiAMIeodFBJVBXJHLPHjVuXDVe5VxFh0fnQHHQXHjDgIK6uAC3AF9FAVEEMWYhEXAIGSykECWhqyQPf29f9RJ0SbdSXWnTp8+VZ/X89TTXadOVX0PJ+ST33J+RxGBmZkZQF3WBZiZ2ejhUDAzsxKHgpmZlTgUzMysxKFgZmYlDgUzMytxKJiZWYlDwWwIJC2WtEnSS5JWSvq2pFZJr5H0C0mrJa2VNE/SWf3e1y7pWkkrJG2U9Liki7M8FrOBOBTMhu6ciGgFXgf0AH8P/Bi4G5gETAQuA9YDSGoC7gGmAScCbcD/Aa6U9LERr95sDxqyLsAsryLiOUk/A44ApgPfjIitycu/6bfr+4CDgFMi4uVk212SLgNukHR9RKwfscLN9sAtBbNhkjQVOAt4BFgAfFfSeZK6dtn1zcDP+gXCTrcBzRRbD2ajgkPBbOj+U9Ja4EHgfuCfgTcCi4F/A5ZLekDSzGT/CcDyXT8kIrYDLySvm40KDgWzoTsvItojYlpEfDgiNkXEsoj4m4g4hOLYwcvAzcn+LwCTd/0QSQ0UA+GFEavcbC8cCmYVFhFLga9THGuA4iDzmZL222XXdwBbgN+NYHlme+RQMNtHksZL+idJMyTVSZoA/Hde+cv+O8Ay4EeSuiU1SvpL4CvAZyJiXUalm+3GoWC277YC3RRbBOuBJyi2AD4AEBFbgDcBS4HZyT5XA38XEf868uWaDU6+yY6Zme3kloKZmZU4FMzMrMShYGZmJQ4FMzMryfXaRxMmTIju7u6syzAzy5V58+a9EBGdA72W61Do7u5m7ty5WZdhZpYrkpYM9pq7j8zMrMShYGZmJQ4FMzMrcSiYmVmJQ8HMzEocCmZmVuJQMDOzklxfpzBccxav5tdP95ae7986hvefOA1JGVZlZpa9mgyFh5es4av3LQBg58rhxxzUzpFT2rMrysxsFKjJUPjgKYfwwVMOAWDFus2c8C/38vtFqx0KZlbzan5MYVJbM9P2b+H3i1ZnXYqZWeZqPhQAjuvuYM7i1fT1+S50ZlbbHArAcdM7WLNxGwt6X8q6FDOzTDkUKIYC4C4kM6t5DgXgoI4WusaNcSiYWc1zKACSOG76/vx+0WoiPK5gZrXLoZA4bnoHK9ZvZunqTVmXYmaWGYdC4vhkXGH2ohczrsTMLDsOhcSMzlbGtzQyZ7HHFcysdjkUEnV1YlZ3hwebzaym1eQyF4M5bnoHv/jjSs79+m8QMLa5gc+eewTTJ+yXdWlmZiPCLYV+zj5yMm8+vIu2QiPjCo08unQtl93yCFu392VdmpnZiHBLoZ/JbQW++f6e0vO7nljB//juPL58z9N84oxXZViZmdnIcEthD844YhIXzprKtff/F79b6FlJZlb93FLYi394y+HMXrSaj/7gUS56ffeQ3vvmw7s4pLM1ncLMzFLgUNiL/cY0cM2FR/Pe62dz5c+eGtJ7n16xgavfeXQ6hZmZpcChUIYjp7Qz7x/ezPYd5S+B8bZv/IYNW7anWJWZWeU5FMrUWF9HY335+7c01bN52470CjIzS4EHmlNSaKpn41aHgpnli0MhJYXGejY5FMwsZxwKKSk0NbDJ3UdmljOphYKkGyWtkvREv20dku6W9Ezyc3yyXZK+ImmBpD9Iel1adY2UQmOdWwpmljtpthS+DZyxy7YrgHsjYiZwb/Ic4ExgZvK4FLg2xbpGREtTAxu3evaRmeVLaqEQEQ8Auy45ei5wU/L7TcB5/bbfHEW/A9olTU6rtpHQ3FjP5m1eM8nM8mWkxxS6ImJ58vsKoCv5/UBgab/9liXbdiPpUklzJc3t7e1Nr9J91NJUz9YdfWzf4WAws/zIbKA5ijdDHvINkSPiuojoiYiezs7OFCqrjEJyUYMHm80sT0Y6FFbu7BZKfq5Ktj8HTO2335RkW24VmhwKZpY/Ix0KdwIXJb9fBNzRb/v7k1lIJwDr+nUz5VKppeAZSGaWI6ktcyHpFuBUYIKkZcCngSuBH0q6BFgCXJDs/lPgLGABsBG4OK26RkqLWwpmlkOphUJEvGuQl04fYN8APpJWLVloTkLBS12YWZ74iuaUtCTdR5sdCmaWIw6FlBTcUjCzHHIopMRjCmaWRw6FlDR79pGZ5ZBDISW+eM3M8sihkJKWpuLELo8pmFmeOBRSMqah+J/WLQUzyxOHQkrq6pTcfc3LZ5tZfjgUUlRoqndLwcxyxaGQomJLwUtnm1l+OBRSVGwpuPvIzPLDoZCilqZ6X6dgZrniUEhRc2O9p6SaWa44FFLU0lTPZg80m1mOOBRSVHBLwcxyxqGQIk9JNbO8cSikqDgl1aFgZvnhUEhRi1sKZpYzDoUUFRqLoVC826iZ2ejnUEhRc1M9EbBlu69qNrN8cCikaOd9mj0DyczywqGQooJvyWlmOeNQSFEhudGOl882s7xwKKSodEtOr5RqZjnhUEhRi7uPzCxnHAopai4NNLv7yMzywaGQop0tBS+KZ2Z54VBIUcFTUs0sZxwKKfKYgpnljUMhRc07Q8EtBTPLCYdCil6ZkupQMLN8cCikqLG+jsZ6sdHdR2aWEw6FlPmeCmaWJw6FlBWaHApmlh+ZhIKkj0p6UtITkm6R1CxpuqTZkhZI+oGkpixqq7Sd91QwM8uDEQ8FSQcClwE9EXEEUA9cCFwFfCkiZgBrgEtGurY0FJoafJ2CmeVGVt1HDUBBUgPQAiwHTgNuTV6/CTgvm9Iqq9BY5yuazSw3RjwUIuI54IvAsxTDYB0wD1gbETsXCVoGHDjQ+yVdKmmupLm9vb0jUfI+aWlq8NpHZpYbWXQfjQfOBaYDBwD7AWeU+/6IuC4ieiKip7OzM6UqK6e5sZ5N27x0tpnlQxbdR28CFkVEb0RsA24HTgLak+4kgCnAcxnUVnEtTfXuPjKz3MgiFJ4FTpDUIknA6cAfgfuA85N9LgLuyKC2iis01rv7yMxyI4sxhdkUB5QfBh5PargO+CTwMUkLgP2BG0a6tjT4OgUzy5OGve9SeRHxaeDTu2xeCByXQTmpKjT5OgUzyw9f0ZyylsZ6tu0Itu3wYLOZjX4OhZQVfE8FM8sRh0LKdobCZo8rmFkOOBRS5ltymlmeOBRS5ltymlmeOBRS1uyWgpnliEMhZTu7j3xVs5nlgUMhZS1NxUtB3FIwszxwKKSs0FT8T+wxBTPLgyGHgqTxko5Mo5hqVEhaCpu8/pGZ5UBZoSDpV5LGSeqguGbRNyVdnW5p1WHnmILXPzKzPCi3pdAWEeuBtwM3R8TxFJfAtr14ZUqql7kws9Gv3FBokDQZuAD4SYr1VJ0xDXVI7j4ys3woNxQ+C/wcWBARcyQdDDyTXlnVQxKFRq+Uamb5UNbS2RHxI+BH/Z4vBN6RVlHVpnijHYeCmY1+5Q40fyEZaG6UdK+kXknvTbu4auF7KphZXpR7k53/FhGfkPQ2YDHFAecHgO+mVVg1KTTWs3ztZh5+dk1FP3e/pgYOmzS2op9pZrWt3FDYud/ZwI8iYl3x9spWjo79mnho4Yu8/Ru/rfhn3/ah13PstPEV/1wzq03lhsJPJD0FbAI+JKkT2JxeWdXlmguP4akV6yv6mVu29/HB78zjof96waFgZhVT7kDzFZK+AKyLiB2SNgLnplta9ZjU1syktuaKf+7Mia3MXVLZLikzq23lDjS3AB8Grk02HQD0pFWUlaenezzzlqyhry+yLsXMqkS51yl8C9gKvD55/hzwuVQqsrL1TOtgw+btPL1qQ9almFmVKDcUDomILwDbACJiI+CR5ozN6u4AYM5idyGZWWWUGwpbJRWAAJB0CLAltaqsLFM7CnSOHcO8xauzLsXMqkS5s48+DdwFTJX0PeAk4ANpFWXlkcSs7vFuKZhZxZQ7++huSQ8DJ1DsNro8Il5ItTIry7HTOvjp4ytYvm4Tk9sKWZdjZjk3lJvsNANrgPXA4ZJOTqckG4pZ3cVrFOa6tWBmFVBWS0HSVcA7gSeBnTcGCIpLXViGXj15HIXGeuYtWcM5Rx2QdTlmlnPljimcBxwWER5cHmUa6+s45qB25niw2cwqoNzuo4VAY5qF2PD1dHcwf/l6XtriG/mY2b4pt6WwEXhU0r30m4oaEZelUpUNSc+08fQFzFuyhlMO7cy6HDPLsXJD4c7k0Z/XVhglerrH01Rfx4PP9DoUzGyflNt91B4RN/V/AF6ac5RoaWpg1vTxPPC0Zwmb2b4pNxQuGmDbBypYh+2jUw7t5E8rN7B83aasSzGzHNtjKEh6l6QfA9Ml3dnvcR8w7Okuktol3SrpKUnzJZ0oqUPS3ZKeSX66JTIEpxw6EYBfu7VgZvtgb2MKvwWWAxOAf+u3fQPwh3343muAuyLifElNQAvwKeDeiLhS0hXAFcAn9+E7asqhXa1MGtfM/U/3csGsqVmXY2Y5tcdQiIglwBLgxEp9oaQ24GSS7qeI2Epxwb1zgVOT3W4CfoVDoWySOPnQCfz8yZVs39FHQ/1QLlY3MyvaW/fRg8nPDZLW93tskDTc+0tOB3qBb0l6RNL1kvYDuiJiebLPCqBrkJoulTRX0tze3t5hllCdTj60k3WbtvHYsnVZl2JmObW3f06+ByAixkbEuH6PsRExbpjf2QC8Drg2Io4BXqbYVVQSEcEgU14j4rqI6ImIns5OT7/s7w0zJlAnuP9ph6WZDc/eQuH/7fxF0m0V+s5lwLKImJ08v5ViSKyUNDn5rsnAqgp9X81ob2ni6KntPOBQMLNh2lso9L+72sGV+MKIWAEslXRYsul04I8UL47bOfX1IuCOSnxfrTn50E4eW7aWF17awo6+2O1hZrYne5t9FIP8vq/+J/C9ZObRQuBiigH1Q0mXUBzcvqCC31czTjm0ky/f8ww9n7tnwNf/+W2v5d3HHzTCVZlZXuwtFI5KBpQFFPoNLoti1/+wxhUi4lGgZ4CXTh/O59krjp7azufOO4LVL2/d7bXrf72QR5eucSiY2aD2NiW1fqQKscqQxHtPmDbga/fMX8mK9V793MwG58nsNaRrXDMr123OugwzG8UcCjVk0rhmVqx3KJjZ4BwKNWRSWzPrNm1j87YdWZdiZqOUQ6GGdI1rBmCFu5DMbBAOhRoyaWcouAvJzAbhUKghXePGALDSoWBmg3Ao1JCutmJLwaFgZoNxKNSQsWMaaGmqZ8U6X6tgZgNzKNQQSUwa1+yWgpkNyqFQY7p8rYKZ7YFDocZMamv2lFQzG5RDocZMHDeGVRs20+dltM1sAA6FGjNpXDPbdgRrNu6+iqqZmUOhxvgCNjPbE4dCjfG1Cma2Jw6FGlNqKfhaBTMbgEOhxnSOHYPk7iMzG5hDocY01tcxoXWMb7ZjZgNyKNSgrnFj3FIwswE5FGqQl7ows8E4FGpQl0PBzAbhUKhBk8Y1s2ajb8tpZrtzKNSgndcqrFrvaalm9uccCjXIVzWb2WAcCjVoUptDwcwG5lCoQV1jk6UufK2Cme2iIesCbOSNKzTQ3FjHd2cv4aGFLw7rM6ZP2I+/P/vVSKpwdWaWJYdCDZLEu4+bxpzFq+ndMPTB5o1bt/PLp1Zx2qsmctKMCSlUaGZZcSjUqH885/Bhv3fzth284ar7+PcHFjoUzKqMxxRsyJob67n4pG4eeLqX+cvXZ12OmVWQQ8GG5b3HT6OlqZ7rHliYdSlmVkEOBRuWtpZGLpx1ED9+7HmeX7sp63LMrEIcCjZsl/zFdAK48cFFWZdiZhWS2UCzpHpgLvBcRLxF0nTg+8D+wDzgfRHhu8uPYge2FzjnyMlc/+Airh8gGFrHNPDzj57Mge2FDKozs+HIcvbR5cB8YFzy/CrgSxHxfUn/F7gEuDar4qw8V5z5ag7ubGVHX/zZ9rUbt3LTQ0t49Nm1DgWzHMkkFCRNAc4GPg98TMUroE4D3p3schPwGRwKo96ktmYuO33mbts3bd3Bzb9bwjOrNgCTR74wMxuWrMYUvgx8AuhLnu8PrI2I7cnzZcCBA71R0qWS5kqa29vbm3qhNjyFpnqmjm/hmVUvZV2KmQ3BiIeCpLcAqyJi3nDeHxHXRURPRPR0dnZWuDqrpJkTW1mw0qFglidZdB+dBLxV0llAM8UxhWuAdkkNSWthCvBcBrVZBc3oauWBZ3rZvqOPhnpPdDPLgxH/PzUi/jYipkREN3Ah8MuIeA9wH3B+sttFwB0jXZtV1syJY9m2I3h29casSzGzMo2mf759kuKg8wKKYww3ZFyP7aOZE1sBPK5gliOZLogXEb8CfpX8vhA4Lst6rLIOSUJhwaqX+MvXZFyMmZVlNLUUrMq0jmngwPYCz6zckHUpZlYmh4KlasbEVncfmeWIQ8FSNXNiKwtWvbTbFc9mNjo5FCxVM7ta2bK9j+fWeCVVszxwKFiqZkwcC5Asd2Fmo51DwVI1w9NSzXLFoWCpais00jVuDAscCma54FCw1M2cONYtBbOccChY6mZMbGXByg1EeAaS2WiX6RXNVhtmTGzl5a07uHf+Ksbv17jb622FRjpbmxlXaKB4aw0zy4pDwVL3mgOKN9f765vn7nG/poY63nhYJ5e84WBmdY93QJhlwKFgqTt6aju3fej1vLxl+26v9UWwbtM2ejdsYenqjdzx2PP8/MmVHDmljaOntg/pe444oI0LZk2tUNVmtcmhYKmTxLHTxpe17xVnvprbH1nGdx5awo8fe77s79jeF9z80BKWr9vM5W/a/fagZlYeh4KNKoWmet5z/DTec/y0Ib2vry/4xG1/4Ev3PE19HfzNaQ4Gs+FwKFhVqKsTV73jSPr6gi/+4mmWr9vMQR0t+/y59XXir46dSlvL7gPkZtXIoWBVo75O/OtfHYUkvjf72Yp97rI1m/jMW31DCKsNyvPc8Z6enpg7d88zWqw2bdq6g2Df/2z/4x1P8uPHnuc3V5zGhNYxFajMLHuS5kVEz0Cv+eI1q0qFpnpamhr2+fHhUw9h644+bnhwUdaHZDYiHApme3BwZytnvXYy33loCes2bcu6HLPUORTM9uIjp87gpS3b+c5Di7MuxSx1DgWzvTj8gHGc9qqJ3PDgItZt3MaW7TvKfuR5zM5qk2cfmZXhI2+cwTuu/S1HffYXQ3rfOUcdwFffdUxKVZlVnkPBrAzHThvPl955FM+v3Vz2ex54upd7569k+44+GurdKLd8cCiYleltx0wZ0v5TO1q47JZHmL98A6+d0pZSVWaV5X++mKVkVndxvac5i1dnXIlZ+RwKZimZ3FZgyvgCc5c4FCw/HApmKZrV3cGcxWs8C8lyw6FglqKe7vH0btjCkhc3Zl2KWVkcCmYpmtXdAXhcwfLDoWCWohmdrbQVGpm7eE3WpZiVxaFglqK6OtEzbTxzPNhsOeFQMEvZrOkdLOx9mRdf2pJ1KWZ75VAwS9kr1yu4C8lGP4eCWcqOOLCNpoY65nqw2XJgxJe5kDQVuBnoAgK4LiKukdQB/ADoBhYDF0SE/2lluTemoZ6jp7Rz428W8R+/r9xtQvekTuKcow7g785+Na1jvJqNlS+LPy3bgY9HxMOSxgLzJN0NfAC4NyKulHQFcAXwyQzqM6u4K856FT97fPmIfd+LL2/l+3Oe5cEFvVx9wdGlqbFme5P5PZol3QF8LXmcGhHLJU0GfhURh+3pvb5Hs9ng5ixezcd/+BhL12zkkM5WlHVBVlGXnT6Tc446YFjv3dM9mjNtV0rqBo4BZgNdEbHzn1IrKHYvDfSeS4FLAQ466KARqNIsn2Z1d/DTy/+Cr/1yAc+ufjnrcqzC2gqNqXxuZi0FSa3A/cDnI+J2SWsjor3f62siYvyePsMtBTOzodtTSyGT2UeSGoHbgO9FxO3J5pVJtxHJz1VZ1GZmVstGPBQkCbgBmB8RV/d76U7gouT3i4A7Rro2M7Nal8WYwknA+4DHJT2abPsUcCXwQ0mXAEuACzKozcyspo14KETEgzDoRIjTR7IWMzP7c76i2czMShwKZmZW4lAwM7MSh4KZmZVkvszFvpDUS3Gm0nBMAF6oYDl5UYvHXYvHDLV53LV4zDD0454WEZ0DvZDrUNgXkuYOdkVfNavF467FY4baPO5aPGao7HG7+8jMzEocCmZmVlLLoXBd1gVkpBaPuxaPGWrzuGvxmKGCx12zYwpmZra7Wm4pmJnZLhwKZmZWUpOhIOkMSX+StCC5H3TVkTRV0n2S/ijpSUmXJ9s7JN0t6Znk5x5vZJRHkuolPSLpJ8nz6ZJmJ+f7B5Kasq6x0iS1S7pV0lOS5ks6sUbO9UeTP99PSLpFUnO1nW9JN0paJemJftsGPLcq+kpy7H+Q9Lqhfl/NhYKkeuDrwJnA4cC7JB2ebVWp2A58PCIOB04APpIc5xXAvRExE7g3eV5tLgfm93t+FfCliJgBrAEuyaSqdF0D3BURrwKOonj8VX2uJR0IXAb0RMQRQD1wIdV3vr8NnLHLtsHO7ZnAzORxKXDtUL+s5kIBOA5YEBELI2Ir8H3g3IxrqriIWB4RDye/b6D4l8SBFI/1pmS3m4DzMikwJZKmAGcD1yfPBZwG3JrsUo3H3AacTPHmVUTE1ohYS5Wf60QDUJDUALQAy6my8x0RDwCrd9k82Lk9F7g5in4HtO+8o2W5ajEUDgSW9nu+LNlWtSR1A8cAs4GuiFievLQC6MqqrpR8GfgE0Jc83x9YGxHbk+fVeL6nA73At5Jus+sl7UeVn+uIeA74IvAsxTBYB8yj+s83DH5u9/nvt1oMhZoiqZXi/bD/V0Ss7/9aFOcjV82cZElvAVZFxLysaxlhDcDrgGsj4hjgZXbpKqq2cw2Q9KOfSzEUDwD2Y/dulqpX6XNbi6HwHDC13/MpybaqI6mRYiB8LyJuTzav3NmcTH6uyqq+FJwEvFXSYordgqdR7GtvT7oXoDrP9zJgWUTMTp7fSjEkqvlcA7wJWBQRvRGxDbid4p+Baj/fMPi53ee/32oxFOYAM5MZCk0UB6buzLimikv60m8A5kfE1f1euhO4KPn9IuCOka4tLRHxtxExJSK6KZ7XX0bEe4D7gPOT3arqmAEiYgWwVNJhyabTgT9Sxec68SxwgqSW5M/7zuOu6vOdGOzc3gm8P5mFdAKwrl83U1lq8opmSWdR7HuuB26MiM9nW1HlSXoD8GvgcV7pX/8UxXGFHwIHUVx2/IKI2HUQK/cknQr874h4i6SDKbYcOoBHgPdGxJYMy6s4SUdTHFxvAhYCF1P8R19Vn2tJ/wS8k+Jsu0eAv6bYh14151vSLcCpFJfHXgl8GvhPBji3STh+jWI32kbg4oiYO6Tvq8VQMDOzgdVi95GZmQ3CoWBmZiUOBTMzK3EomJlZiUPBzMxKHApmZZC0v6RHk8cKSc8lv78k6RtZ12dWKZ6SajZEkj4DvBQRX8y6FrNKc0vBbB9IOrXffRs+I+kmSb+WtETS2yV9QdLjku5Klh1B0rGS7pc0T9LPh7qKpVmaHApmlXUIxTWX3gp8F7gvIl4LbALOToLhq8D5EXEscCNQdVfUW3417H0XMxuCn0XENkmPU1xG5a5k++NAN3AYcARwd3FFAuopLvtsNio4FMwqawtARPRJ2havDNr1Ufz/TcCTEXFiVgWa7Ym7j8xG1p+ATkknQnF5c0mvybgmsxKHgtkISm4Bez5wlaTHgEeB12dalFk/npJqZmYlbimYmVmJQ8HMzEocCmZmVuJQMDOzEoeCmZmVOBTMzKzEoWBmZiX/HwZMoQSaGuc6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "i = InputData(fileName=\"../ANN/iris\")\n",
    "input_val, output_val = i.main()\n",
    "end_time = time.time()\n",
    "print(\"Time for inputting data : \", end_time - start_time)\n",
    "        \n",
    "print(\"============ Calling PSO to get best weights ===============\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "a = psoAnn(initialPopSize=100, m=10, input_values=input_val, output_values_expected=output_val, iterations = 100, dimensions = [100,10])\n",
    "\n",
    "fit, b, weights, dim = a.main()\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Time taken : \", end_time - start_time)\n",
    "\n",
    "print(\"\\n Fitness : \", fit, \"\\n Best Weights : \", weights, \"\\n Dimensions : \", dim)\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "x=b[:]\n",
    "z=[i for i in range(0,100)]\n",
    "plt.plot(z,x)\n",
    "\n",
    "plt.title(\"PSO\")\n",
    "plt.ylabel(\"Fitness\")\n",
    "plt.xlabel(\"Time\")\n",
    "end_time = time.time()\n",
    "print(\"Time Taken : \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "676443f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============= MLP Program Begins ============\n",
      "Training\n",
      "\n",
      " Actual / Expected [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      " Predictions [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 2 1 2 1 1 1 1 1 1 1 1 1 1 2\n",
      " 2 1 1 1 1 1 0 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "[[40  0  0]\n",
      " [ 0 34  6]\n",
      " [ 2  0 38]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.95      1.00      0.98        40\n",
      "     class 1       1.00      0.85      0.92        40\n",
      "     class 2       0.86      0.95      0.90        40\n",
      "\n",
      "    accuracy                           0.93       120\n",
      "   macro avg       0.94      0.93      0.93       120\n",
      "weighted avg       0.94      0.93      0.93       120\n",
      "\n",
      "Time taken =  0.012965679168701172\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n============= MLP Program Begins ============\")\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Training\")\n",
    "m = MultiLayerPerceptron(fileName=\"../ANN/iris_train\", dimensions=dim, all_weights=weights)\n",
    "m.main()\n",
    "end_time = time.time()\n",
    "print(\"Time taken = \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c834af83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n",
      "\n",
      " Actual / Expected [0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      " Predictions [0 0 0 0 0 0 0 0 0 0 2 1 1 1 1 1 1 1 1 1 2 1 2 2 2 1 2 2 2 2]\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "[[10  0  0]\n",
      " [ 0  9  1]\n",
      " [ 0  2  8]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      1.00      1.00        10\n",
      "     class 1       0.82      0.90      0.86        10\n",
      "     class 2       0.89      0.80      0.84        10\n",
      "\n",
      "    accuracy                           0.90        30\n",
      "   macro avg       0.90      0.90      0.90        30\n",
      "weighted avg       0.90      0.90      0.90        30\n",
      "\n",
      "Time taken =  0.006982564926147461\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(\"Testing\")\n",
    "m = MultiLayerPerceptron(fileName=\"../ANN/iris_test\", dimensions=dim, all_weights=weights)\n",
    "m.main()\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Time taken = \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e52f45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
