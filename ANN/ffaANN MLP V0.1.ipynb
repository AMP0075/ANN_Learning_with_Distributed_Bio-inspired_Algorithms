{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb5a447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50dfaafc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn import preprocessing\n",
    "from scipy.special import expit\n",
    "\n",
    "from numpy.random import default_rng\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import time\n",
    "\n",
    "from ffaAnn_thread_V_I import *\n",
    "\n",
    "\n",
    "\n",
    "class MultiLayerPerceptron():\n",
    "    # ================== Activation Functions ================ #\n",
    "\n",
    "    # accepts a vector or list and returns a list after performing corresponding function on all elements\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(vectorSig):\n",
    "        \"\"\"returns 1/(1+exp(-x)), where the output values lies between zero and one\"\"\"\n",
    "        sig = expit(vectorSig)\n",
    "        return sig\n",
    "\n",
    "    @staticmethod\n",
    "    def binaryStep(x):\n",
    "        \"\"\" It returns '0' is the input is less then zero otherwise it returns one \"\"\"\n",
    "        return np.heaviside(x, 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def linear(x):\n",
    "        \"\"\" y = f(x) It returns the input as it is\"\"\"\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        \"\"\" It returns the value (1-exp(-2x))/(1+exp(-2x)) and the value returned will be lies in between -1 to 1\"\"\"\n",
    "        return np.tanh(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(x):  # Rectified Linear Unit\n",
    "        \"\"\" It returns zero if the input is less than zero otherwise it returns the given input\"\"\"\n",
    "        x1 = []\n",
    "        for i in x:\n",
    "            if i < 0:\n",
    "                x1.append(0)\n",
    "            else:\n",
    "                x1.append(i)\n",
    "\n",
    "        return x1\n",
    "\n",
    "    @staticmethod\n",
    "    def leakyRelu(x):\n",
    "        \"\"\" It returns zero if the input is less than zero otherwise it returns the given input\"\"\"\n",
    "        x1 = []\n",
    "        for i in x:\n",
    "            if i < 0:\n",
    "                x1.append((0.01 * i))\n",
    "            else:\n",
    "                x1.append(i)\n",
    "\n",
    "        return x1\n",
    "\n",
    "    @staticmethod\n",
    "    def parametricRelu(self, a, x):\n",
    "        \"\"\" It returns zero if the input is less than zero otherwise it returns the given input\"\"\"\n",
    "        x1 = []\n",
    "        for i in x:\n",
    "            if i < 0:\n",
    "                x1.append((a * i))\n",
    "            else:\n",
    "                x1.append(i)\n",
    "\n",
    "        return x1\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(self, x):\n",
    "        \"\"\" Compute softmax values for each sets of scores in x\"\"\"\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "    # ============ Activation Functions Part Ends ============= #\n",
    "\n",
    "    # ================= Distance Calculation ================== #\n",
    "\n",
    "    @staticmethod\n",
    "    def chebishev(self, cord1, cord2, exponent_h):\n",
    "        dist = 0.0\n",
    "        if ((type(cord1) == int and type(cord2) == int) or ((type(cord1) == float and type(cord2) == float))):\n",
    "            dist = math.pow((cord1 - cord2), exponent_h)\n",
    "        else:\n",
    "            for i, j in zip(cord1, cord2):\n",
    "                dist += math.pow((i - j), exponent_h)\n",
    "        dist = math.pow(dist, (1.0 / exponent_h))\n",
    "        return dist\n",
    "\n",
    "    @staticmethod\n",
    "    def minimum_distance(self, cord1, cord2):\n",
    "        # min(|x1-y1|, |x2-y2|, |x3-y3|, ...)\n",
    "        dist = float('inf')\n",
    "        if ((type(cord1) == int and type(cord2) == int) or ((type(cord1) == float and type(cord2) == float))):\n",
    "            dist = math.fabs(cord1 - cord2)\n",
    "        else:\n",
    "            for i, j in zip(cord1, cord2):\n",
    "                temp_dist = math.fabs(i - j)\n",
    "                if (temp_dist < dist):\n",
    "                    dist = temp_dist\n",
    "        return dist\n",
    "\n",
    "    @staticmethod\n",
    "    def maximum_distance(self, cord1, cord2):\n",
    "        # max(|x1-y1|, |x2-y2|, |x3-y3|, ...)\n",
    "        dist = float('-inf')\n",
    "        if ((type(cord1) == int and type(cord2) == int) or ((type(cord1) == float and type(cord2) == float))):\n",
    "            dist = math.fabs(cord1 - cord2)\n",
    "        else:\n",
    "            for i, j in zip(cord1, cord2):\n",
    "                temp_dist = math.fabs(i - j)\n",
    "                if (temp_dist > dist):\n",
    "                    dist = temp_dist\n",
    "        return dist\n",
    "\n",
    "    @staticmethod\n",
    "    def manhattan(self, cord1, cord2):\n",
    "        # |x1-y1| + |x2-y2| + |x3-y3| + ...\n",
    "        dist = 0.0\n",
    "        if ((type(cord1) == int and type(cord2) == int) or ((type(cord1) == float and type(cord2) == float))):\n",
    "            dist = math.fabs(cord1 - cord2)\n",
    "        else:\n",
    "            for i, j in zip(cord1, cord2):\n",
    "                dist += math.fabs(i - j)\n",
    "        return dist\n",
    "\n",
    "    @staticmethod\n",
    "    def eucledian(self, cord1, cord2):\n",
    "        dist = 0.0\n",
    "        if ((type(cord1) == int and type(cord2) == int) or ((type(cord1) == float and type(cord2) == float))):\n",
    "            dist = math.pow((cord1 - cord2), 2)\n",
    "        else:\n",
    "            for i, j in zip(cord1, cord2):\n",
    "                dist += math.pow((i - j), 2)\n",
    "        return math.pow(dist, 0.5)\n",
    "\n",
    "    # =========== Distance Calculation Ends ============== #\n",
    "\n",
    "    def __init__(self, dimensions=(8, 5), all_weights=(0.1, 0.2), fileName=\"iris\"):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dimensions : dimension of the neural network\n",
    "            all_weights : the optimal weights we get from the bio-algoANN models\n",
    "        \"\"\"\n",
    "\n",
    "        self.allPop_Weights = []\n",
    "        self.allPopl_Chromosomes = []\n",
    "        self.allPop_ReceivedOut = []\n",
    "        self.allPop_ErrorVal = []\n",
    "\n",
    "        self.all_weights = all_weights\n",
    "\n",
    "        self.fitness = []\n",
    "\n",
    "        # ================== Input dataset and corresponding output ========================= #\n",
    "\n",
    "        self.fileName = fileName\n",
    "        self.fileName += \".csv\"\n",
    "        data = pd.read_csv(self.fileName)\n",
    "\n",
    "        classes = []\n",
    "        output_values_expected = []\n",
    "        input_values = []\n",
    "\n",
    "        # ~~~~ encoding ~~~~#\n",
    "\n",
    "        # labelencoder = LabelEncoder()\n",
    "        # data[data.columns[-1]] = labelencoder.fit_transform(data[data.columns[-1]])\n",
    "\n",
    "        # one hot encoding - for multi-column\n",
    "        # enc = OneHotEncoder(handle_unknown='ignore')\n",
    "        # combinedData = np.vstack((data[data.columns[-2]], data[data.columns[-1]])).T\n",
    "        # print(combinedData)\n",
    "        # y = enc.fit_transform(combinedData).toarray()\n",
    "        # y = OneHotEncoder().fit_transform(combinedData).toarray()\n",
    "\n",
    "        #\n",
    "        y = LabelBinarizer().fit_transform(data[data.columns[-1]])\n",
    "        # print(y)\n",
    "\n",
    "        # ~~~~ encoding ends~~~~#\n",
    "\n",
    "        for j in range(len(data)):\n",
    "            output_values_expected.append(y[j])\n",
    "\n",
    "        # print(output_values_expected)\n",
    "\n",
    "        input_values = []\n",
    "        for j in range(len(data)):\n",
    "            b = []\n",
    "            for i in range(1, len(data.columns) - 1):\n",
    "                b.append(data[data.columns[i]][j])\n",
    "            input_values.append(b)\n",
    "\n",
    "        self.X = input_values[:]\n",
    "        self.Y = output_values_expected[:]\n",
    "\n",
    "        # input and output\n",
    "        self.X = input_values[:]\n",
    "        self.Y = output_values_expected[:]\n",
    "\n",
    "        self.dimension = dimensions\n",
    "        # print(self.dimension)\n",
    "\n",
    "        # ================ Finding Initial Weights ================ #\n",
    "\n",
    "        self.pop = []  # weights\n",
    "        reshaped_all_weights = []\n",
    "        start = 0\n",
    "        for i in range(len(self.dimension) - 1):\n",
    "            end = start + self.dimension[i + 1] * self.dimension[i]\n",
    "            temp_arr = self.all_weights[start:end]\n",
    "            w = np.reshape(temp_arr[:], (self.dimension[i + 1], self.dimension[i]))\n",
    "            reshaped_all_weights.append(w)\n",
    "            start = end\n",
    "        self.pop.append(reshaped_all_weights)\n",
    "\n",
    "        self.init_pop = self.all_weights\n",
    "\n",
    "    # ================ Initial Weights Part Ends ================ #\n",
    "\n",
    "\n",
    "    def Predict(self, chromo):\n",
    "        # X, Y and pop are used\n",
    "        self.fitness = []\n",
    "        total_error = 0\n",
    "        m_arr = []\n",
    "        k1 = 0\n",
    "        for i in range(len(self.dimension) - 1):\n",
    "            p = self.dimension[i]\n",
    "            q = self.dimension[i + 1]\n",
    "            k2 = k1 + p * q\n",
    "            m_temp = chromo[k1:k2]\n",
    "            m_arr.append(np.reshape(m_temp, (p, q)))\n",
    "            k1 = k2\n",
    "\n",
    "        y_predicted = []\n",
    "        for x, y in zip(self.X, self.Y):\n",
    "\n",
    "            yo = x\n",
    "\n",
    "            for mCount in range(len(m_arr)):\n",
    "                yo = np.dot(yo, m_arr[mCount])\n",
    "                yo = self.sigmoid(yo)\n",
    "            \n",
    "            # converting to sklearn acceptable form\n",
    "            max_yo = max(yo)\n",
    "            for y_vals in range(len(yo)):\n",
    "                if(yo[y_vals] == max_yo):\n",
    "                    yo[y_vals] = 1\n",
    "                else:\n",
    "                    yo[y_vals] = 0\n",
    "            y_predicted.append(yo)\n",
    "        return (y_predicted, self.Y)\n",
    "\n",
    "    def main(self):\n",
    "        Y_PREDICT, Y_ACTUAL = self.Predict(self.init_pop)\n",
    "        Y_PREDICT = np.array(Y_PREDICT)\n",
    "        Y_ACTUAL = np.array(Y_ACTUAL)\n",
    "        \n",
    "        n_classes = 3\n",
    "        \n",
    "        label_binarizer = LabelBinarizer()\n",
    "        label_binarizer.fit(range(n_classes))\n",
    "        Y_PREDICT = label_binarizer.inverse_transform(np.array(Y_PREDICT))\n",
    "        Y_ACTUAL = label_binarizer.inverse_transform(np.array(Y_ACTUAL))\n",
    "        \n",
    "        # find error\n",
    "        \n",
    "        print(\"\\n Actual / Expected\", Y_ACTUAL)\n",
    "        print(\"\\n Predictions\", Y_PREDICT)\n",
    "        print(\"\\n\\nConfusion Matrix\")\n",
    "        print(confusion_matrix(Y_ACTUAL, Y_PREDICT))\n",
    "        \n",
    "        print(\"\\n\\nClassification Report\")\n",
    "        target_names = ['class 0', 'class 1', 'class 2']\n",
    "        print(classification_report(Y_ACTUAL, Y_PREDICT, target_names=target_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d36914a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inputting data :  0.008975505828857422\n",
      "============ Calling FFA to get best weights ===============\n",
      "--------------GENERATION 0-----------\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "--------------GENERATION 1-----------\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "--------------GENERATION 2-----------\n",
      "hihi\n",
      "hi\n",
      "\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "--------------GENERATION 3-----------\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "--------------GENERATION 4-----------\n",
      "--------------GENERATION 5-----------\n",
      "--------------GENERATION 6-----------\n",
      "--------------GENERATION 7-----------\n",
      "--------------GENERATION 8-----------\n",
      "--------------GENERATION 9-----------\n",
      "--------------GENERATION 10-----------\n",
      "--------------GENERATION 11-----------\n",
      "--------------GENERATION 12-----------\n",
      "--------------GENERATION 13-----------\n",
      "--------------GENERATION 14-----------\n",
      "--------------GENERATION 15-----------\n",
      "--------------GENERATION 16-----------\n",
      "--------------GENERATION 17-----------\n",
      "--------------GENERATION 18-----------\n",
      "--------------GENERATION 19-----------\n",
      "--------------GENERATION 20-----------\n",
      "--------------GENERATION 21-----------\n",
      "--------------GENERATION 22-----------\n",
      "--------------GENERATION 23-----------\n",
      "--------------GENERATION 24-----------\n",
      "--------------GENERATION 25-----------\n",
      "--------------GENERATION 26-----------\n",
      "--------------GENERATION 27-----------\n",
      "--------------GENERATION 28-----------\n",
      "--------------GENERATION 29-----------\n",
      "--------------GENERATION 30-----------\n",
      "--------------GENERATION 31-----------\n",
      "--------------GENERATION 32-----------\n",
      "--------------GENERATION 33-----------\n",
      "--------------GENERATION 34-----------\n",
      "--------------GENERATION 35-----------\n",
      "--------------GENERATION 36-----------\n",
      "--------------GENERATION 37-----------\n",
      "--------------GENERATION 38-----------\n",
      "--------------GENERATION 39-----------\n",
      "--------------GENERATION 40-----------\n",
      "--------------GENERATION 41-----------\n",
      "--------------GENERATION 42-----------\n",
      "--------------GENERATION 43-----------\n",
      "--------------GENERATION 44-----------\n",
      "--------------GENERATION 45-----------\n",
      "--------------GENERATION 46-----------\n",
      "--------------GENERATION 47-----------\n",
      "--------------GENERATION 48-----------\n",
      "--------------GENERATION 49-----------\n",
      "--------------GENERATION 50-----------\n",
      "--------------GENERATION 51-----------\n",
      "--------------GENERATION 52-----------\n",
      "--------------GENERATION 53-----------\n",
      "--------------GENERATION 54-----------\n",
      "--------------GENERATION 55-----------\n",
      "--------------GENERATION 56-----------\n",
      "--------------GENERATION 57-----------\n",
      "--------------GENERATION 58-----------\n",
      "--------------GENERATION 59-----------\n",
      "--------------GENERATION 60-----------\n",
      "--------------GENERATION 61-----------\n",
      "--------------GENERATION 62-----------\n",
      "--------------GENERATION 63-----------\n",
      "--------------GENERATION 64-----------\n",
      "--------------GENERATION 65-----------\n",
      "--------------GENERATION 66-----------\n",
      "--------------GENERATION 67-----------\n",
      "--------------GENERATION 68-----------\n",
      "--------------GENERATION 69-----------\n",
      "--------------GENERATION 70-----------\n",
      "--------------GENERATION 71-----------\n",
      "--------------GENERATION 72-----------\n",
      "--------------GENERATION 73-----------\n",
      "--------------GENERATION 74-----------\n",
      "--------------GENERATION 75-----------\n",
      "--------------GENERATION 76-----------\n",
      "--------------GENERATION 77-----------\n",
      "--------------GENERATION 78-----------\n",
      "--------------GENERATION 79-----------\n",
      "--------------GENERATION 80-----------\n",
      "--------------GENERATION 81-----------\n",
      "--------------GENERATION 82-----------\n",
      "--------------GENERATION 83-----------\n",
      "--------------GENERATION 84-----------\n",
      "--------------GENERATION 85-----------\n",
      "--------------GENERATION 86-----------\n",
      "--------------GENERATION 87-----------\n",
      "--------------GENERATION 88-----------\n",
      "--------------GENERATION 89-----------\n",
      "--------------GENERATION 90-----------\n",
      "--------------GENERATION 91-----------\n",
      "--------------GENERATION 92-----------\n",
      "--------------GENERATION 93-----------\n",
      "--------------GENERATION 94-----------\n",
      "--------------GENERATION 95-----------\n",
      "--------------GENERATION 96-----------\n",
      "--------------GENERATION 97-----------\n",
      "--------------GENERATION 98-----------\n",
      "--------------GENERATION 99-----------\n",
      "Fitness :  62.117153142110624\n",
      "Time taken :  85.11415314674377\n",
      "\n",
      " Fitness :  -62.117153142110624 \n",
      " Best Weights :  [-1, -1, 0, -2, 0, -18, 0, -2, -4, -15, 3, 2, 13, -7, 2, 10, 14, 12, -1, 0, -2, 1, 0, -6, -1, -1, 9, 3, 0, -4, -2, 5, -8, -2, -12, 0, 6, 0, 20, -1, -4, 0, -7, 9, -13, -6, -8, -1, 6, 20, -26, 0, -2, -25, 6, 14, -5, -4, -2, -4, -1, 5, 15, -16, 4, 1, 0, -4, -12, -10, -18, -3, -10, -4, 17, -6, 4, 9, -6, -10, 1, 13, -10, 1, 3, -8, -5, -1, 3, 2, -11, -20, 0, 9, 6, 20, 0, 4, 7, 8, 17, -4, 17, -15, 2, -20, -5, -17, -2, 1, 6, 5, -6, 0, 9, 0, -10, 9, -9, 9, 0, 0, 0, -10, 0, 16, 0, -14, 17, 0, 8, 0, -12, 2, -15, 7, 0, 6, 3, 0, 23, 0, 16, 14, 20, 2, 0, -4, 2, 3, -4, 0, 5, 2, 2, 15, 7, -5, -4, 12, -14, 4, -22, 0, -8, -9, -3, 0, 0, -8, 4, 1, -9, 14, 4, -5, -10, -12, 15, 0, -3, 7, -12, 10, -17, 8, 13, -7, 0, -11, -10, -7, -10, -1, -1, 21, 0, -11, -1, -6, 0, -11, 3, -1, 20, 11, 0, 11, 3, -17, 0, -11, 20, 0, -13, 0, -4, 20, 0, 10, -21, -1, -3, 2, 1, 2, -5, 7, -8, -4, -5, -7, -11, 9, -5, -3, -9, -2, 3, -19, 9, -3, -11, -7, 5, 2, 4, 3, 4, 10, 3, 3, -10, 0, -2, 4, 0, 20, 3, 0, -20, -2, 0, 10, 0, -6, 8, -6, -2, -12, -3, 8, 4, 0, 21, -9, -3, 0, 2, 11, 0, -13, 0, -10, -8, 4, -10, -1, 8, 0, 2, 10, -11, 4, -4, 1, -7, -9, 16, 5, -2, 12, -7, -5, -6, 0, 4, -6, 3, 2, -9, -2, 0, -6, 5, -10, -10, -18, 1, -6, 15, -23, 11, -13, -12, 10, 2, -14, -6, 23, -4, 10, -4, 4, -8, -3, 2, 0, -2, 0, 2, -15, -18, -10, -8, 2, -8, 3, -10, -11, 4, 14, 11, 18, -14, 2, -14, -7, 5, 8, 0, -15, -9, -10, 0, 0, -5, -9, 29, 1, 11, -8, 0, 11, -13, -21, 14, 9, -4, -9, 7, -2, -12, 0, -19, -11, 3, 3, -13, 9, -8, 0, -1, 0, 16, 7, 0, -16, 21, -34, -9, 0, -4, 15, 12, 5, -20, -12, 8, -8, 14, -3, -8, 17, 19, 0, 3, 0, 10, -2, -15, 0, -6, 18, 14, 8, -5, 16, 9, 14, -16, 0, -3, 5, 7, 15, -10, 0, 10, -14, -11, 6, -34, -6, 0, 16, 0, 4, -4, 0, 6, -10, -17, -1, 7, -3, -2, 3, -15, -4, -11, 3, 0, 7, -7, 1, 0, 0, 0, -2, 2, -9, 0, -3, -10, 25, -11, -10, 11, -9, -5, 15, -10, 1, 0, -11, -7, -4, -7, -14, -10, 0, 9, 5, 7, -17, 0, -8, -2, -10, -4, 6, 0, -15, 1, 5, -8, 0, -16, 10, 18, -1, -8, -15, 0, 1, 0, 15, -12, 1, 4, 6, -1, 10, -1, 0, -8, 0, -15, -4, -12, 12, 6, -17, 12, -9, 0, 2, -5, -15, -23, 8, -8, 0, -12, 0, -3, -11, 0, 2, 1, -4, -4, 10, 3, -4, 1, 19, -12, 0, 14, 11, 3, 10, -11, 1, 5, 11, -14, -5, 25, 20, -9, -6, 12, 0, -13, -14, -10, -15, -8, -3, -5, -2, -14, 19, 5, -5, 0, -1, -6, -13, -6, 19, 0, -13, -20, -10, -1, -5, 0, -7, -12, 1, 0, 2, -7, -15, -15, 4, 2, 14, -12, 14, 7, 8, 0, 3, -6, 4, 3, -1, 5, 7, 4, 0, -10, 4, 0, 2, 17, 6, -11, 10, -6, 3, 4, 27, 0, 12, 0, -17, 13, -6, -9, 0, 0, 5, -16, 6, 14, -4, -6, 2, -13, 17, 4, 8, 2, -4, -1, -16, -12, 18, 14, -16, 21, 2, -4, -6, -8, 2, 6, 4, 0, -10, 0, 16, -7, 16, 13, 8, 0, -8, 10, -7, -20, 3, 1, -5, 3, 0, -14, 17, 5, 0, 12, 1, -3, 5, -7, 12, -9, -8, 7, -14, 0, -20, -3, -1, -8, -3, 7, -9, -17, -6, 16, 6, 3, -14, 6, 15, 9, 19, -8, -5, 0, -10, 7, -2, 4, 0, 15, 0, -13, 7, -10, 7, 4, 15, 0, -8, 17, 10, -7, 3, 15, 4, -2, -1, -14, -15, 9, 25, -4, 0, 0, -2, 0, 7, -4, 10, -1, 1, 0, 3, -22, -14, 13, 8, 7, 2, -3, 22, 9, 10, 11, 11, -1, 2, -4, 2, -8, -1, 18, -12, -8, -19, -5, -14, 1, 6, -6, 14, -4, -11, 1, -13, 0, 2, -15, 18, 5, 0, 3, 10, 17, -32, 0, 0, -8, -9, 4, 15, -4, -5, -10, 0, 3, -7, -6, -2, -6, 11, 12, -6, -17, -6, 7, 0, 12, 6, 15, -8, 6, 3, -9, -2, 0, -3, 8, 3, -13, -6, -25, 0, -10, -5, 0, -2, -13, 6, -9, 7, 5, 2, 1, 2, 7, -2, 0, 0, -12, -5, -14, 4, -13, -3, 0, 1, 0, 7, 4, -2, -5, 3, 4, 20, 4, -6, -14, 12, -15, 0, -15, 0, 9, 4, 5, 1, -9, -10, -17, 18, 0, 21, 0, -3, -7, 5, 2, -11, -14, 13, -5, 5, -6, -8, -5, -12, 0, 0, 12, 0, -8, -18, 0, -11, -10, -14, 1, 8, 2, -10, -4, -40, -1, 1, 8, 10, 8, 14, 1, -4, 4, 7, -5, 11, -26, -14, 2, 9, -19, -4, -26, -4, -4, -7, 13, 5, -9, 13, 6, 3, 0, 12, 16, 0, 5, -4, -18, 3, -4, 12, -16, -9, 2, -3, -7, 6, -10, 14, -6, 20, 0, -3, 0, -3, 5, -6, -9, 2, 5, -4, -21, -2, 1, 8, 3, -10, 19, 13, 6, 0, 11, 15, 6, 8, 0, -12, 10, -1, -3, 3, 2, 0, 2, -11, -20, -5, 6, -1, -4, -2, 0, 7, 0, -5, -12, -4, -12, 14, -7, 5, 16, 9, 0, 3, 2, -12, -7, -4, 10, 3, 0, -12, 0, -3, 2, 0, 1, -5, 6, -6, 12, 4, -4, -10, 3, 0, -7, 13, -19, 12, -8, 3, -14, 13, -5, 4, 3, -5, 17, -6, -19, 5, 1, -10, 2, -8, -5, 16, -3, -24, 12, 0, -14, 4, 0, -16, 0, -5, 0, 18, 4, 11, 8, -6, 9, 17, -1, 0, 21, -3, -15, 8, 10, -5, -2, 3, 15, -11, 9, 14, 4, 2, 14, -3, -13, 0, 15, 7, 18, -5, -12, -8, -8, 12, -4, 9, -10, 20, 18, -18, -5, -3, 16, -11, -13, 0, 11, -3, -11, 0, -10, 0, 1, -21, 12, -8, 0, 8, 0, -1, -17, 15, 14, 20, 5, -19, -16, -14, 10, 9, 5, 6, -1, -12, -4, -2, 12, -3, 28, 1, -1, 14, -31, -11, 2, 11, 0, 2, -1, -1, 9, 0, -9, -14, -11, 2, -3, -26, 4, -7, 0, 0, -4, 0, -18, 4, 3, -9, 3, 8, 3, -5, -20, -5, 7, -8, 3, 0, 17, -17, 1, 4, 10, 1, -4, 9, 31, -1, 17, 7, 15, 8, 11, 7, 2, -10, 3, 15, -6, -7, 0, -1, -1, 5, 11, -5, 22, 5, -23, 7, -18, 3, 10, -8, 2, -8, -4, -20, -3, -9, 0, -5, -10, 0, 10, 19, -6, -10, -3, -17, -3, -17, 11, -5, -5, -10, -17, 3, 13, -3, 0, 8, 6, 5, 0, 0, -13, 4, 16, -4, 4, -2, 0, 0, 1, 5, 12, -1, 1, -16, 11, 0, -1, 0, -3, 2, -4, -10, 12, 0, -5, 4, 0, -6, 5, 6, 0, -5, 18, -2, -2, -4, 8, 9, 0, 8, 2, -3, 5, -7, -14, 0, -6, -2, 2, -20, 0, 2, 11, 8, 0, 2, -19, -3, 18, 10, -1, -18, 5, -6, 8, 4, -5, 0, 14, 0, -22, -19, 4, -2, -6, -4, 7, 12, -6, 4, -18, -22, 8, -5, 8, -1, -6, 10, -3, 3, -6, 2, -7, -9, -6, -6, -11, -6, -7, -8, -13, -11, 4, -3, 23, -12, -1, 12, 15, 0, -5, -6, 10, 7, -4, 9, 2, 2, 9, -14, 3, -4, 20, -9, 9, 6, -16, -6, 0, 3, -8, -5, 0, -1, -9, 5, -8, 7, -5, 15, -9, 0, -15, 7, 1, 9, 12, -10, -10, -2, 0, -9, -14, 14, -3, -8, 0, -7, 0, -9] \n",
      " Dimensions :  [4, 100, 10, 3]\n",
      "Time Taken :  85.14547634124756\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "i = InputData(fileName=\"../ANN/iris\")\n",
    "input_val, output_val = i.main()\n",
    "end_time = time.time()\n",
    "print(\"Time for inputting data : \", end_time - start_time)\n",
    "        \n",
    "print(\"============ Calling FFA to get best weights ===============\")\n",
    "\n",
    "start_time = time.time()\n",
    "a = ffaAnn(initialPopSize=100, m=10, dimensions = [100,10], input_values=input_val, output_values_expected=output_val, iterations = 100)\n",
    "\n",
    "fit, b, weights, dim = a.main()\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Time taken : \", end_time - start_time)\n",
    "\n",
    "print(\"\\n Fitness : \", fit, \"\\n Best Weights : \", weights, \"\\n Dimensions : \", dim)\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "x=b[:]\n",
    "z=[i for i in range(0,100)]\n",
    "plt.plot(z,x)\n",
    "\n",
    "plt.title(\"Firefly Algorithm\")\n",
    "plt.ylabel(\"Fitness\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "end_time = time.time()\n",
    "print(\"Time Taken : \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2830eb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============= MLP Program Begins ============\n",
      "Training\n",
      "\n",
      " Actual / Expected [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      " Predictions [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 2\n",
      " 2 0 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 1 2\n",
      " 2 2 1 2 2 2 2 2 2]\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "[[40  0  0]\n",
      " [ 1 34  5]\n",
      " [ 0  3 37]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.98      1.00      0.99        40\n",
      "     class 1       0.92      0.85      0.88        40\n",
      "     class 2       0.88      0.93      0.90        40\n",
      "\n",
      "    accuracy                           0.93       120\n",
      "   macro avg       0.93      0.93      0.92       120\n",
      "weighted avg       0.93      0.93      0.92       120\n",
      "\n",
      "Time taken =  0.013961315155029297\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n============= MLP Program Begins ============\")\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Training\")\n",
    "m = MultiLayerPerceptron(fileName=\"../ANN/iris_train\", dimensions=dim, all_weights=weights)\n",
    "m.main()\n",
    "end_time = time.time()\n",
    "print(\"Time taken = \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3c970b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n",
      "\n",
      " Actual / Expected [0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      " Predictions [0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 0 2 2 2 2 2 2 2 2]\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "[[10  0  0]\n",
      " [ 0 10  0]\n",
      " [ 1  0  9]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.91      1.00      0.95        10\n",
      "     class 1       1.00      1.00      1.00        10\n",
      "     class 2       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "Time taken =  0.00897526741027832\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(\"Testing\")\n",
    "m = MultiLayerPerceptron(fileName=\"../ANN/iris_test\", dimensions=dim, all_weights=weights)\n",
    "m.main()\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Time taken = \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245fd1af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
